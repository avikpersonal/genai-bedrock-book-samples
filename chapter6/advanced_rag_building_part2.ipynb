{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed3a60a-5001-46ff-ae70-61d861a00c53",
   "metadata": {
    "tags": []
   },
   "source": [
    "# File Name: advanced_rag_building_part2.ipynb\n",
    "### Location: Chapter 6\n",
    "### Purpose: \n",
    "#####       1. Example of Sentence Window Retrieval RAG pattern.\n",
    "#####       2. Example of Reranker RAG pattern.\n",
    "#####       3. Example of FLARE RAG pattern.\n",
    "#####       4. Example of MultiStep Query Engine RAG pattern.\n",
    "\n",
    "##### Dependency: Not Applicable\n",
    "# <ins>-----------------------------------------------------------------------------------</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99464e3-0679-4a67-bb38-8d96892eefa5",
   "metadata": {},
   "source": [
    "# <ins>Amazon SageMaker Classic</ins>\n",
    "#### Those who are new to Amazon SageMaker Classic. Follow the link for the details. https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91003ca1-0026-46eb-bd09-81861135cb22",
   "metadata": {},
   "source": [
    "# <ins>Environment setup of Kernel</ins>\n",
    "##### Fill \"Image\" as \"Data Science\"\n",
    "##### Fill \"Kernel\" as \"Python 3\"\n",
    "##### Fill \"Instance type\" as \"ml-t3-medium\"\n",
    "##### Fill \"Start-up script\" as \"No Scripts\"\n",
    "##### Click \"Select\"\n",
    "\n",
    "###### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-create-open.html for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865b7f5-755a-498c-bff6-7647281b15b9",
   "metadata": {},
   "source": [
    "# <ins>Mandatory installation on the kernel through pip</ins>\n",
    "\n",
    "##### This lab will work with below software version. But, if you are trying with latest version of boto3, awscli, and botocore. This code may fail. You might need to change the corresponding api. \n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell. \n",
    "\n",
    "# CAUTION: The below pip installation can take more than 30 mins for the first time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade0112-a4be-4ce8-be92-8cbf28b2acfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall -q \\\n",
    "    \"boto3>=1.34.84\" \\\n",
    "    \"langchain>=0.2.16\" \\\n",
    "    \"langchain_community>=0.2.17\" \\\n",
    "    \"awscli>=1.32.84\" \\\n",
    "    \"botocore>=1.34.84\" \\\n",
    "    \"PyPDF2\" \\\n",
    "    \"pypdf\" \\\n",
    "    \"llama-index\" \\\n",
    "    \"llama-index-llms-bedrock\" \\\n",
    "    \"llama-index-embeddings-bedrock\" \\\n",
    "    \"llama-index-embeddings-huggingface\" \\\n",
    "    \"llama-index-llms-langchain\" \\\n",
    "    \"langchain-chroma>=0.1.2\" \\\n",
    "    \"ipywidgets>=7.6.5\" \\\n",
    "    \"jupyterlab\" \\\n",
    "    \"jupyter\" \\\n",
    "    \"tqdm\" \\\n",
    "    \"iprogress>=0.4\" \\\n",
    "    \"llama-index-embeddings-langchain\" \\\n",
    "    \"ipynb\" \\\n",
    "    \"langchain-aws>=0.1.7\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e3b85-4e64-42ef-9355-ed2a61c68b23",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins>\n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ee934-e99c-49ce-9e94-27ffc5baabe1",
   "metadata": {},
   "source": [
    "# <ins>Restart the kernel</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf41bf-d2d2-46fa-b1d4-1dd19c45a069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5af76d-b2a9-4740-b8bf-134932804719",
   "metadata": {},
   "source": [
    "# <ins>Python package import</ins>\n",
    "\n",
    "##### boto3 offers various clients for Amazon Bedrock to execute various actions.\n",
    "##### botocore is a low-level interface to AWS tools, while boto3 is built on top of botocore and provides additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7268959-6ef0-4cbd-bd53-3b31369b4513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "import warnings\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_aws.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Import necessary modules\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e298d-8904-4717-8e36-48fa549f7d52",
   "metadata": {},
   "source": [
    "# Define prompt, Amazon Bedrock Foundation model, and Amazon Bedrock embed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5b7c17-8de7-4651-b65d-9cbf651025fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define prompt\n",
    "prompt = \"What is Amazon doing and cashflow?\"\n",
    "\n",
    "# List of Bedrock models with names and model codes\n",
    "bedrock_model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# List of Bedrock embed models with names and model codes\n",
    "bedrock_embed_model_id = \"amazon.titan-embed-text-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2ac559-b058-4d02-9c27-a7a9aadfdf1a",
   "metadata": {},
   "source": [
    "## Define important environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd794cbe-7de5-48ec-af12-a69f33b50aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try-except block to handle potential errors\n",
    "try:\n",
    "    # Create a new Boto3 session to interact with AWS services\n",
    "    # This session is responsible for managing credentials and region configuration\n",
    "    boto3_session = boto3.session.Session()\n",
    "\n",
    "    # Retrieve the current AWS region from the session (e.g., 'us-east-1', 'us-west-2')\n",
    "    aws_region_name = boto3_session.region_name\n",
    "    \n",
    "    # Initialize Bedrock and Bedrock Runtime clients using Boto3\n",
    "    # These clients will allow interactions with Bedrock-related AWS services\n",
    "    boto3_bedrock_client = boto3.client('bedrock', region_name=aws_region_name)\n",
    "    boto3_bedrock_runtime_client = boto3.client('bedrock-runtime', region_name=aws_region_name)\n",
    "\n",
    "    # Store all relevant variables in a dictionary for easier access and management\n",
    "    variables_store = {\n",
    "        \"aws_region_name\": aws_region_name,                          # AWS region name\n",
    "        \"boto3_bedrock_client\": boto3_bedrock_client,                # Bedrock client instance\n",
    "        \"boto3_bedrock_runtime_client\": boto3_bedrock_runtime_client,  # Bedrock Runtime client instance\n",
    "        \"boto3_session\": boto3_session                               # Current Boto3 session object\n",
    "    }\n",
    "\n",
    "    # Print all stored variables for debugging and verification\n",
    "    for var_name, value in variables_store.items():\n",
    "        print(f\"{var_name}: {value}\")\n",
    "\n",
    "# Handle any exceptions that occur during the execution\n",
    "except Exception as e:\n",
    "    # Print the error message if an unexpected error occurs\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56862239-5a8d-40f9-bed8-689a02ba5d4d",
   "metadata": {},
   "source": [
    "# Modular Python setup for initializing language and embeddings models with error handling using Bedrock client.\n",
    "\n",
    "### 1. initialize_language_model: This function initializes a language model using the Bedrock client and model ID. If successful, it returns the model; otherwise, it returns None.\n",
    "\n",
    "### 2. initialize_embeddings_model: Similar to the first function, this initializes the embeddings model using the Bedrock client and embeddings model ID, with error handling in case of failure.\n",
    "\n",
    "### 3. setup_models: This is the main function that calls the previous two functions to initialize both the language model and embeddings model. It handles errors, ensuring that if either initialization fails, the process is halted, and appropriate messages are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965068c9-2b97-4003-bef7-5040936fa190",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing necessary packages from langchain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Function to initialize the language model with the Bedrock client\n",
    "def initialize_language_model(client, model_id):\n",
    "    \"\"\"\n",
    "    Initializes the language model using the provided Bedrock client and model ID.\n",
    "    \n",
    "    Args:\n",
    "        client: The Bedrock client for model invocation.\n",
    "        model_id (str): The ID of the language model to be initialized.\n",
    "    \n",
    "    Returns:\n",
    "        ChatBedrock: The initialized language model or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the language model\n",
    "        llm = ChatBedrock(model_id=model_id, client=client)\n",
    "        print(\"Successfully initialized the language model.\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        # Handle any errors during initialization\n",
    "        print(f\"Error initializing the language model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to initialize the Bedrock Embeddings model\n",
    "def initialize_embeddings_model(client, embed_model_id):\n",
    "    \"\"\"\n",
    "    Initializes the embeddings model using the provided Bedrock client and embeddings model ID.\n",
    "    \n",
    "    Args:\n",
    "        client: The Bedrock client for model invocation.\n",
    "        embed_model_id (str): The ID of the embeddings model to be initialized.\n",
    "    \n",
    "    Returns:\n",
    "        BedrockEmbeddings: The initialized embeddings model or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the embeddings model\n",
    "        embeddings_model = BedrockEmbeddings(client=client, model_id=embed_model_id)\n",
    "        print(\"Successfully initialized the Bedrock Embeddings model.\")\n",
    "        return embeddings_model\n",
    "    except Exception as e:\n",
    "        # Handle any errors during initialization\n",
    "        print(f\"Error initializing the Bedrock Embeddings model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main function to set up both models with error handling\n",
    "def setup_models(bedrock_client, bedrock_model_id, embed_model_id):\n",
    "    \"\"\"\n",
    "    Sets up the language model and embeddings model, handling errors during the setup.\n",
    "    \n",
    "    Args:\n",
    "        bedrock_client: The Bedrock client used to interact with the models.\n",
    "        bedrock_model_id (str): The ID of the language model.\n",
    "        embed_model_id (str): The ID of the embeddings model.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (llm, embeddings_model) initialized models or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the language model\n",
    "        llm = initialize_language_model(bedrock_client, bedrock_model_id)\n",
    "        if not llm:\n",
    "            # Return None if language model initialization failed\n",
    "            print(\"Failed to initialize language model, exiting setup.\")\n",
    "            return None, None\n",
    "\n",
    "        # Initialize the embeddings model\n",
    "        embeddings_model = initialize_embeddings_model(bedrock_client, embed_model_id)\n",
    "        if not embeddings_model:\n",
    "            # Return None if embeddings model initialization failed\n",
    "            print(\"Failed to initialize embeddings model, exiting setup.\")\n",
    "            return llm, None\n",
    "\n",
    "        print(\"Both models initialized successfully.\")\n",
    "        return llm, embeddings_model\n",
    "    except Exception as e:\n",
    "        # Handle any unexpected errors during the setup process\n",
    "        print(f\"Unexpected error in setup process: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage: setting up both models\n",
    "llm, embeddings_model = setup_models(boto3_bedrock_runtime_client, bedrock_model_id, bedrock_embed_model_id)\n",
    "\n",
    "if llm and embeddings_model:\n",
    "    # Indicate that both models are ready for use\n",
    "    print(\"Language model and embeddings model are ready for use.\")\n",
    "else:\n",
    "    # Indicate that there was an issue with initialization\n",
    "    print(\"One or both models failed to initialize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7b3047-1726-4dc1-9025-a4fb51cb93f0",
   "metadata": {},
   "source": [
    "# Important python package for Llama index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036805ab-b743-4529-9899-ef55142b8641",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.query_engine import FLAREInstructQueryEngine\n",
    "from llama_index.llms.bedrock import Bedrock\n",
    "\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    get_response_synthesizer,\n",
    "    Settings,\n",
    ")\n",
    "\n",
    "from llama_index.core.indices.query.query_transform.base import (\n",
    "    StepDecomposeQueryTransform,\n",
    ")\n",
    "\n",
    "from llama_index.core.query_engine import MultiStepQueryEngine\n",
    "\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embeddings_model\n",
    "Settings.chunk_size = 2056"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1b877-deab-4961-a249-7663e812fbb8",
   "metadata": {},
   "source": [
    "### Ignore warning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71587271-9f02-46dc-9eaf-5a5291132f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ea2067-a0d1-47bf-828e-d675d155e199",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Find out data directory\n",
    "\n",
    "#### 1. Retrieves the current working directory and prints it.\n",
    "#### 2. Builds a path that navigates up one directory and appends 'data/rag_use_cases' to the path, then prints this resulting path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5156036-ec42-48fa-b13a-59ffd316cf94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get the current working directory\n",
    "    current_directory = os.getcwd()\n",
    "    \n",
    "    # Print the current working directory\n",
    "    print(f\"Current working directory: {current_directory}\")\n",
    "    \n",
    "    # Attempt to navigate up one directory and then to 'data/rag_use_cases'\n",
    "    data_directory = os.path.join(os.path.dirname(current_directory), 'data/rag_use_cases')\n",
    "    \n",
    "    # Print the resulting path\n",
    "    print(f\"Data directory path: {data_directory}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    # Handle the case where the directory path does not exist\n",
    "    print(f\"Error: The specified path does not exist - {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # General exception handler for any other errors\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88925ed9-968c-4671-abf2-f5a92e393e23",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Disclaimer\n",
    "##### Make Sure that data_directory is pointing to the right path and data files are present. Otherwise, you need to change the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2cf6f-265b-4cbd-becf-0def0815a238",
   "metadata": {},
   "source": [
    "# Define prompt, Amazon Bedrock Foundation model, and Amazon Bedrock embed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750244c4-6399-430b-b5ea-84ef10837108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define prompt\n",
    "prompt = \"What is Amazon doing and cashflow?\"\n",
    "\n",
    "# List of Bedrock models with names and model codes\n",
    "bedrock_model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# List of Bedrock embed models with names and model codes\n",
    "bedrock_embed_model_id = \"amazon.titan-embed-text-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a1e824-1734-4ca4-8b40-b5ef3f5b65e4",
   "metadata": {},
   "source": [
    "## Define important environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21c86fa-3cbd-468a-962b-8e4ef1d7fcf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try-except block to handle potential errors\n",
    "try:\n",
    "    # Create a new Boto3 session to interact with AWS services\n",
    "    # This session is responsible for managing credentials and region configuration\n",
    "    boto3_session = boto3.session.Session()\n",
    "\n",
    "    # Retrieve the current AWS region from the session (e.g., 'us-east-1', 'us-west-2')\n",
    "    aws_region_name = boto3_session.region_name\n",
    "    \n",
    "    # Initialize Bedrock and Bedrock Runtime clients using Boto3\n",
    "    # These clients will allow interactions with Bedrock-related AWS services\n",
    "    boto3_bedrock_client = boto3.client('bedrock', region_name=aws_region_name)\n",
    "    boto3_bedrock_runtime_client = boto3.client('bedrock-runtime', region_name=aws_region_name)\n",
    "\n",
    "    # Store all relevant variables in a dictionary for easier access and management\n",
    "    variables_store = {\n",
    "        \"aws_region_name\": aws_region_name,                          # AWS region name\n",
    "        \"boto3_bedrock_client\": boto3_bedrock_client,                # Bedrock client instance\n",
    "        \"boto3_bedrock_runtime_client\": boto3_bedrock_runtime_client,  # Bedrock Runtime client instance\n",
    "        \"boto3_session\": boto3_session                               # Current Boto3 session object\n",
    "    }\n",
    "\n",
    "    # Print all stored variables for debugging and verification\n",
    "    for var_name, value in variables_store.items():\n",
    "        print(f\"{var_name}: {value}\")\n",
    "\n",
    "# Handle any exceptions that occur during the execution\n",
    "except Exception as e:\n",
    "    # Print the error message if an unexpected error occurs\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb1560-7324-42d1-9f86-480b7142998a",
   "metadata": {},
   "source": [
    "# Prepare dataset \n",
    "\n",
    "##### This function, load_documents_from_directory, attempts to load documents from a specified directory path using SimpleDirectoryReader. It returns a list of documents if successful, printing the count of documents loaded. The function includes exception handling for cases such as missing directories (FileNotFoundError), insufficient permissions (PermissionError), and other general errors, with each case providing a clear error message. In the example usage, it checks if documents were loaded successfully, printing a confirmation message if they are ready for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9632866b-2fba-4526-96bb-a003db1dd8ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to load documents from a directory\n",
    "def load_documents_from_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Loads documents from a specified directory path.\n",
    "    \n",
    "    Parameters:\n",
    "    - directory_path (str): The path to the directory containing document files.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of loaded documents if successful; otherwise, None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to load documents from the specified directory\n",
    "        documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "        \n",
    "        # Confirm successful document loading\n",
    "        print(f\"Successfully loaded {len(documents)} documents from {directory_path}.\")\n",
    "        print()  # Add extra line breaks for clarity in output\n",
    "        return documents\n",
    "\n",
    "    # Handle any exceptions during loading process\n",
    "    except FileNotFoundError:\n",
    "        # Handle the case where directory or files are not found\n",
    "        print(f\"Error: Directory '{directory_path}' not found.\")\n",
    "        return None\n",
    "    \n",
    "    except PermissionError:\n",
    "        # Handle cases where permissions are insufficient to read files\n",
    "        print(f\"Error: Insufficient permissions to read from '{directory_path}'.\")\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        # General error handling for unexpected issues\n",
    "        print(f\"Error loading documents from {directory_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "\n",
    "llamaindex_documents = load_documents_from_directory(data_directory)\n",
    "\n",
    "# Check if documents were loaded successfully\n",
    "if llamaindex_documents:\n",
    "    print(\"Documents loaded and ready for processing.\")\n",
    "else:\n",
    "    print(\"Document loading failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9c6994-97f5-4bfd-a3e7-f8ba12560293",
   "metadata": {},
   "source": [
    "# 1. Sentence Window Retrieval patterns\n",
    "\n",
    "< Details >\n",
    "\n",
    "#### Refer:\n",
    "https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/sentence_window_retriever/sentence_window.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2992b31f-1a60-4f52-ba05-242b575d9604",
   "metadata": {},
   "source": [
    "## Create a VectorStoreIndex from the previously loaded documents. A VectorStoreIndex is used to store and index document embeddings, enabling efficient similarity searches and queries.\n",
    "### It uses the VectorStoreIndex.from_documents() method to create the index, incorporating both the document data and an embedding model for the indexing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da643779-a3f1-4b41-9f0d-4a61ad934db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to create a VectorStoreIndex from the loaded documents\n",
    "try:\n",
    "    llama_index = VectorStoreIndex.from_documents(\n",
    "        llamaindex_documents,\n",
    "        embed_model=embeddings_model,\n",
    "        llm_predictor=llm,  # Use the language model directly for predictions\n",
    "        show_progress=True    # Display progress during index creation\n",
    "    )\n",
    "    print(\"Successfully created the VectorStoreIndex.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating VectorStoreIndex: {e}\")\n",
    "    llama_index = None  # Set to None in case of an error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472dc32d-dbce-4ca7-920b-d034210352cf",
   "metadata": {},
   "source": [
    "# Creating and executing a query engine based on the successful creation of a VectorStoreIndex\n",
    "\n",
    "### 1. similarity_top_k=2: Specifies that the query engine should retrieve the top 2 most similar documents from the index based on the query.\n",
    "### 2. node_postprocessors: A list of postprocessors applied to the nodes in the query engine. In this case, the MetadataReplacementPostProcessor is used to modify the metadata associated with the documents, specifically replacing the metadata key with 'window'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889efd68-a645-408e-8ef7-49bf9b7427df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the index was successfully created, proceed to create a query engine\n",
    "if llama_index:\n",
    "    try:\n",
    "        query_engine = llama_index.as_query_engine(\n",
    "            similarity_top_k=2,  # Retrieve the top 2 similar documents\n",
    "            node_postprocessors=[\n",
    "                MetadataReplacementPostProcessor(target_metadata_key=\"window\")  # Replace metadata key with 'window'\n",
    "            ],\n",
    "        )\n",
    "        print(\"Successfully created the query engine.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating query engine: {e}\")\n",
    "        query_engine = None  # Set to None in case of an error\n",
    "\n",
    "    # If the query engine was successfully created, execute the query\n",
    "    if query_engine:\n",
    "        try:\n",
    "            window_response = query_engine.query(prompt)  # Execute the query\n",
    "            print(\"Query executed successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing query: {e}\")\n",
    "            window_response = None  # Set to None in case of an error\n",
    "\n",
    "        # Print the response if it was successful\n",
    "        if window_response:\n",
    "            print(\"Response:\", window_response)\n",
    "        else:\n",
    "            print(\"No response received from the query.\")\n",
    "    else:\n",
    "        print(\"Query engine was not created; cannot execute query.\")\n",
    "else:\n",
    "    print(\"VectorStoreIndex was not created; cannot create query engine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38aadd0-5de9-49c4-af1e-d088b00c4a4e",
   "metadata": {},
   "source": [
    "# 2. Reranker patterns\n",
    "\n",
    "< Details >\n",
    "\n",
    "#### Refer:\n",
    "https://python.langchain.com/docs/templates/rag-pinecone-rerank/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd50e74-e2cd-4614-bb18-5d985f784cc3",
   "metadata": {},
   "source": [
    "# Process to Initialize Retriever, Create Temporary Index, Configure Query Engine, and Execute Query with Synthesized Response\n",
    "\n",
    "### Approach to querying an indexed document store, with various functions focused on retrieving, configuring, and processing data. It incorporates error handling and logs to ensure that any issues encountered during the process are clearly communicated. Below is a detailed breakdown of each function and its role in the overall process.\n",
    "\n",
    "### 1. initialize_retriever Function: Initializes the retriever, which is responsible for retrieving the most relevant nodes based on the user’s query (prompt).\n",
    "### 2. create_temp_index Function: Creates a temporary VectorStoreIndex from a list of nodes.\n",
    "### 3. configure_retriever Function: Configures the retriever for querying the llama_index based on the provided query mode and top-k similarity threshold.\n",
    "### 4. initialize_response_synthesizer Function: Initializes a response synthesizer to generate responses based on the retrieved information.\n",
    "### 5. query_engine_with_retriever Function: Assembles the query engine using the retriever and the response synthesizer, and then executes the query based on the user's prompt.\n",
    "### 6. reranker Function (Main Execution Process): The main function that ties all the components together and executes the complete process of retrieving, indexing, and generating a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252290bf-3c39-4844-9793-bcbcc3d3b8c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to initialize the retriever and retrieve nodes based on a prompt\n",
    "def initialize_retriever(llama_index, prompt, top_k=50):\n",
    "    \"\"\"\n",
    "    Initializes the retriever with a specified similarity threshold and retrieves nodes based on the prompt.\n",
    "\n",
    "    Parameters:\n",
    "    - llama_index: The index used for retrieving nodes.\n",
    "    - prompt (str): The user query.\n",
    "    - top_k (int): The number of top documents to retrieve for similarity matching.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of nodes retrieved from the index, or None if retrieval fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize retriever and retrieve nodes based on similarity\n",
    "        retriever = llama_index.as_retriever(similarity_top_k=top_k)\n",
    "        nodes = retriever.retrieve(prompt)\n",
    "        print(f\"Successfully retrieved {len(nodes)} nodes.\")\n",
    "        return [node.node for node in nodes]\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing retriever or retrieving nodes: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to create a temporary index from a list of nodes\n",
    "def create_temp_index(node_list):\n",
    "    \"\"\"\n",
    "    Creates a VectorStoreIndex from a list of nodes.\n",
    "\n",
    "    Parameters:\n",
    "    - node_list (list): The list of nodes for the index.\n",
    "    \n",
    "    Returns:\n",
    "    - VectorStoreIndex: The created index, or None if index creation fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize VectorStoreIndex with node list and show progress\n",
    "        temp_index = VectorStoreIndex(node_list, show_progress=True)\n",
    "        print(\"Temporary VectorStoreIndex created successfully.\")\n",
    "        return temp_index\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating VectorStoreIndex: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to set up and configure the retriever with query mode\n",
    "def configure_retriever(llama_index, top_k=10, query_mode=\"mmr\"):\n",
    "    \"\"\"\n",
    "    Configures the retriever with the specified query mode and similarity threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - llama_index: The index used for configuring the retriever.\n",
    "    - top_k (int): The number of top documents for similarity search.\n",
    "    - query_mode (str): Mode for querying the vector store.\n",
    "\n",
    "    Returns:\n",
    "    - VectorIndexRetriever: Configured retriever or None if configuration fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        retriever = VectorIndexRetriever(\n",
    "            index=llama_index,\n",
    "            similarity_top_k=top_k,\n",
    "            vector_store_query_mode=query_mode,\n",
    "        )\n",
    "        print(\"Retriever configured successfully.\")\n",
    "        return retriever\n",
    "    except Exception as e:\n",
    "        print(f\"Error configuring the retriever: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to initialize the response synthesizer\n",
    "def initialize_response_synthesizer(mode=\"tree_summarize\"):\n",
    "    \"\"\"\n",
    "    Initializes the response synthesizer with the specified mode.\n",
    "\n",
    "    Parameters:\n",
    "    - mode (str): The mode used for synthesizing responses.\n",
    "\n",
    "    Returns:\n",
    "    - ResponseSynthesizer: Configured response synthesizer or None if initialization fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response_synthesizer = get_response_synthesizer(response_mode=mode)\n",
    "        print(\"Response synthesizer initialized successfully.\")\n",
    "        return response_synthesizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing response synthesizer: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to assemble the query engine and query a prompt\n",
    "def query_engine_with_retriever(retriever, response_synthesizer, prompt):\n",
    "    \"\"\"\n",
    "    Assembles the query engine with the retriever and synthesizer and executes a query.\n",
    "\n",
    "    Parameters:\n",
    "    - retriever: The retriever configured with the index.\n",
    "    - response_synthesizer: The synthesizer configured for response generation.\n",
    "    - prompt (str): The query or prompt to send to the query engine.\n",
    "\n",
    "    Returns:\n",
    "    - str: The response from the query engine or None if querying fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Assemble query engine\n",
    "        query_engine = RetrieverQueryEngine(\n",
    "            retriever=retriever,\n",
    "            response_synthesizer=response_synthesizer,\n",
    "        )\n",
    "        \n",
    "        # Query with the assembled engine\n",
    "        response = query_engine.query(prompt)\n",
    "        print(\"Query executed successfully.\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main execution process\n",
    "def reranker(prompt):\n",
    "    try:\n",
    "        # Load and retrieve nodes\n",
    "        nodes = initialize_retriever(llama_index, prompt)\n",
    "        if not nodes:\n",
    "            print(\"No nodes retrieved, exiting.\")\n",
    "            return\n",
    "\n",
    "        # Create temporary index from nodes\n",
    "        temp_index = create_temp_index(nodes)\n",
    "        if not temp_index:\n",
    "            print(\"Temporary index creation failed, exiting.\")\n",
    "            return\n",
    "\n",
    "        # Configure retriever\n",
    "        retriever = configure_retriever(llama_index)\n",
    "        if not retriever:\n",
    "            print(\"Retriever configuration failed, exiting.\")\n",
    "            return\n",
    "\n",
    "        # Initialize response synthesizer\n",
    "        response_synthesizer = initialize_response_synthesizer()\n",
    "        if not response_synthesizer:\n",
    "            print(\"Response synthesizer initialization failed, exiting.\")\n",
    "            return\n",
    "\n",
    "        # Run query\n",
    "        response = query_engine_with_retriever(retriever, response_synthesizer, prompt)\n",
    "        if response:\n",
    "            print(\"Final response:\", response)\n",
    "        else:\n",
    "            print(\"No response received from the query engine.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Execute the main process with a prompt\n",
    "reranker(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ab06d-e001-4d2c-abba-605f984dda63",
   "metadata": {},
   "source": [
    "# 3. FLARE patterns\n",
    "\n",
    "< Details >\n",
    "\n",
    "#### Refer:\n",
    "https://docs.llamaindex.ai/en/stable/examples/query_engine/flare_query_engine/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2438ee32-ae78-4916-9bf1-af948c04ceca",
   "metadata": {},
   "source": [
    "# Initializing and Executing LlamaIndex and FLARE Query Engines with Error Handling\n",
    "\n",
    "### Approach to querying an indexed document store, with various functions focused on retrieving, configuring, and processing data. It incorporates error handling and logs to ensure that any issues encountered during the process are clearly communicated. Below is a detailed breakdown of each function and its role in the overall process.\n",
    "\n",
    "### 1. initialize_retriever Function: Initializes the retriever, which is responsible for retrieving the most relevant nodes based on the user’s query (prompt).\n",
    "### 2. create_temp_index Function: Creates a temporary VectorStoreIndex from a list of nodes.\n",
    "### 3. configure_retriever Function: Configures the retriever for querying the llama_index based on the provided query mode and top-k similarity threshold.\n",
    "### 4. initialize_response_synthesizer Function: Initializes a response synthesizer to generate responses based on the retrieved information.\n",
    "### 5. query_engine_with_retriever Function: Assembles the query engine using the retriever and the response synthesizer, and then executes the query based on the user's prompt.\n",
    "### 6. reranker Function (Main Execution Process): The main function that ties all the components together and executes the complete process of retrieving, indexing, and generating a response.\n",
    "\n",
    "### Function run_query_engines that initializes and runs two types of query engines — LlamaIndex and FLARE — using a provided query. It includes robust error handling to manage any issues that may arise during engine initialization or query execution. Below is a detailed breakdown of the function and its flow:\n",
    "\n",
    "### 1. Initialize the LlamaIndex query engine with the given index.\n",
    "### 2. Initialize the FLARE query engine using LlamaIndex as its base.\n",
    "### 3. Execute the provided query using the FLARE query engine and return the results.\n",
    "### 4. Provide error handling throughout the initialization and query execution stages to ensure the process is robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3ff051-8dca-4068-8aa5-bd4e130b2f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to initialize and run the query engines with error handling\n",
    "def run_query_engines(llama_index, query):\n",
    "    \"\"\"\n",
    "    Initializes and runs the LlamaIndex and FLARE query engines with the specified query.\n",
    "    \n",
    "    Parameters:\n",
    "    - llama_index: The LlamaIndex object containing the indexed documents.\n",
    "    - query (str): The query string for retrieving relevant documents.\n",
    "    \n",
    "    Returns:\n",
    "    - response: The response generated from the FLARE query engine, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the Llama query engine with specified similarity_top_k\n",
    "        index_query_engine = llama_index.as_query_engine(similarity_top_k=2)\n",
    "        print(\"Llama query engine initialized successfully.\")\n",
    "\n",
    "        # Initialize the FLARE query engine with the Llama query engine as the base\n",
    "        flare_query_engine = FLAREInstructQueryEngine(\n",
    "            query_engine=index_query_engine,\n",
    "            max_iterations=7,\n",
    "            verbose=True\n",
    "        )\n",
    "        print(\"FLARE query engine initialized successfully.\")\n",
    "        \n",
    "        # Execute the query using the FLARE query engine\n",
    "        response = flare_query_engine.query(query)\n",
    "        print(\"Query executed successfully.\")\n",
    "        \n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions during the setup or query process\n",
    "        print(f\"An error occurred while executing the query: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "response = run_query_engines(llama_index, prompt)\n",
    "\n",
    "# Print the response if available\n",
    "if response:\n",
    "    print(\"Query response:\", response)\n",
    "else:\n",
    "    print(\"Failed to retrieve a response.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1a042e-55d2-406b-b3b3-71babd8eacce",
   "metadata": {},
   "source": [
    "# 4. MultiStep Query Engine patterns\n",
    "\n",
    "< Details >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaed1e5-0a16-4e86-8ba8-56db16bb7d60",
   "metadata": {},
   "source": [
    "# Initializing and Executing Multi-Step Query Engine with Bedrock LLM Model\n",
    "\n",
    "### To initialize a Bedrock LLM model, configure a query engine, and execute a multi-step query using step decomposition. The process involves several stages, with comprehensive error handling to ensure smooth execution. Below is a detailed breakdown of each part of the code:\n",
    "\n",
    "### 1. Initializing the Bedrock LLM Model: The first step is to initialize a Bedrock LLM model using a specified model ID (bedrock_model_id).\n",
    "### 2. Setting Up Step Decomposition Transform: To handle complex queries by breaking them into smaller, more manageable steps, a step decomposition transform is created.\n",
    "### 3. Defining the Index Summary: An index summary is defined to provide a concise description of the content within the index. In this case, it is set to \"Cash Flow\", which will be used as part of the query process.\n",
    "### 4. Configuring the Primary Query Engine: The query engine is configured by transforming the existing VectorStoreIndex into a query engine that integrates with the Bedrock LLM model.\n",
    "### 5. Setting Up the Multi-Step Query Engine: A multi-step query engine is created to allow for advanced query processing that includes the step decomposition transform and the defined index summary.\n",
    "### 6. Executing the Query: Once the multi-step query engine is set up, the next step is to execute a query using the engine. The query is provided as the prompt variable, which is passed into the query_engine.query(prompt) method.\n",
    "\n",
    "### 9. Considerations:\n",
    "##### Verbose Logging: The verbose=True flag in StepDecomposeQueryTransform ensures that detailed logs are generated, which can help in debugging the decomposition process.\n",
    "##### Complex Queries: The use of a multi-step query engine enables the handling of more complex queries by breaking them down into smaller steps.\n",
    "##### Model Integration: The integration of the Bedrock LLM into the process allows the query engine to leverage powerful language model capabilities for better response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e2876a-cd18-49f4-a01d-e6561e6bd5f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize Bedrock LLM model with the specified model ID\n",
    "    llama_llm = Bedrock(model=bedrock_model_id)\n",
    "    print(\"Bedrock LLM model initialized successfully.\")\n",
    "\n",
    "    # Set up a step decomposition transform for query processing\n",
    "    step_decompose_transform = StepDecomposeQueryTransform(llm=llama_llm, verbose=True)\n",
    "    print(\"Step decomposition transform initialized successfully.\")\n",
    "\n",
    "    # Define the index summary, a concise description of the index's content\n",
    "    index_summary = \"Cash Flow\"\n",
    "\n",
    "    # Configure the primary query engine with Bedrock LLM\n",
    "    query_engine = llama_index.as_query_engine(llm=llama_llm)\n",
    "    print(\"Primary query engine created successfully.\")\n",
    "\n",
    "    # Set up a multi-step query engine with decomposition capabilities\n",
    "    query_engine = MultiStepQueryEngine(\n",
    "        query_engine=query_engine,\n",
    "        query_transform=step_decompose_transform,\n",
    "        index_summary=index_summary,\n",
    "    )\n",
    "    print(\"Multi-step query engine created successfully.\")\n",
    "\n",
    "    # Execute the query using the multi-step query engine\n",
    "    response = query_engine.query(prompt)\n",
    "    print(\"Query executed successfully. Response received.\")\n",
    "    print(response)\n",
    "\n",
    "except Exception as e:\n",
    "    # Catch any errors that occur during initialization or querying\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51745d43-360d-42a1-8617-a7a3a2769ab8",
   "metadata": {},
   "source": [
    "# End of NoteBook \n",
    "\n",
    "## Please ensure that you close the kernel after using this notebook to avoid any potential charges to your account.\n",
    "\n",
    "## Process: Go to \"Kernel\" at top option. Choose \"Shut Down Kernel\". \n",
    "##### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/studio-ui.html"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
