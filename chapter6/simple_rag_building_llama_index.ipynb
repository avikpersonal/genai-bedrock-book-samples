{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed3a60a-5001-46ff-ae70-61d861a00c53",
   "metadata": {
    "tags": []
   },
   "source": [
    "# File Name: simple_rag_building_llama_index.ipynb\n",
    "### Location: Chapter 6\n",
    "### Purpose: \n",
    "#####       1. Create a Llama index\n",
    "#####       2. Ingest and build the index\n",
    "#####       3. Generating responses with RAG\n",
    "##### Dependency: Not Applicable\n",
    "# <ins>-----------------------------------------------------------------------------------</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99464e3-0679-4a67-bb38-8d96892eefa5",
   "metadata": {},
   "source": [
    "# <ins>Amazon SageMaker Classic</ins>\n",
    "#### Those who are new to Amazon SageMaker Classic. Follow the link for the details. https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91003ca1-0026-46eb-bd09-81861135cb22",
   "metadata": {},
   "source": [
    "# <ins>Environment setup of Kernel</ins>\n",
    "##### Fill \"Image\" as \"Data Science\"\n",
    "##### Fill \"Kernel\" as \"Python 3\"\n",
    "##### Fill \"Instance type\" as \"ml-t3-medium\"\n",
    "##### Fill \"Start-up script\" as \"No Scripts\"\n",
    "##### Click \"Select\"\n",
    "\n",
    "###### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-create-open.html for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865b7f5-755a-498c-bff6-7647281b15b9",
   "metadata": {},
   "source": [
    "# <ins>Mandatory installation on the kernel through pip</ins>\n",
    "\n",
    "##### This lab will work with below software version. But, if you are trying with latest version of boto3, awscli, and botocore. This code may fail. You might need to change the corresponding api. \n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade0112-a4be-4ce8-be92-8cbf28b2acfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall -q \\\n",
    "    \"boto3>=1.34.84\" \\\n",
    "    \"langchain>=0.2.16\" \\\n",
    "    \"langchain_community>=0.2.17\" \\\n",
    "    \"awscli>=1.32.84\" \\\n",
    "    \"botocore>=1.34.84\" \\\n",
    "    \"PyPDF2\" \\\n",
    "    \"pypdf\" \\\n",
    "    \"llama-index\" \\\n",
    "    \"llama-index-llms-bedrock\" \\\n",
    "    \"llama-index-embeddings-bedrock\" \\\n",
    "    \"llama-index-embeddings-huggingface\" \\\n",
    "    \"llama-index-llms-langchain\" \\\n",
    "    \"llama-index-embeddings-langchain\" \\\n",
    "    \"langchain-aws>=0.1.7\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e3b85-4e64-42ef-9355-ed2a61c68b23",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins>\n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ee934-e99c-49ce-9e94-27ffc5baabe1",
   "metadata": {},
   "source": [
    "# <ins>Restart the kernel</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf41bf-d2d2-46fa-b1d4-1dd19c45a069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5af76d-b2a9-4740-b8bf-134932804719",
   "metadata": {},
   "source": [
    "# <ins>Python package import</ins>\n",
    "\n",
    "##### boto3 offers various clients for Amazon Bedrock to execute various actions.\n",
    "##### botocore is a low-level interface to AWS tools, while boto3 is built on top of botocore and provides additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7268959-6ef0-4cbd-bd53-3b31369b4513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "import warnings\n",
    "import time\n",
    "from langchain_aws.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain_aws import ChatBedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1b877-deab-4961-a249-7663e812fbb8",
   "metadata": {},
   "source": [
    "### Ignore warning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71587271-9f02-46dc-9eaf-5a5291132f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4550613-7b53-4c15-9933-19fc78f7196c",
   "metadata": {},
   "source": [
    "# Find out data directory\n",
    "\n",
    "#### 1. Retrieves the current working directory and prints it.\n",
    "##### 2. Builds a path that navigates up one directory and appends 'data/rag_use_cases' to the path, then prints this resulting path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ee296-131b-4540-9e34-d869fc70f054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "    # Get the current working directory\n",
    "    current_directory = os.getcwd()\n",
    "    \n",
    "    # Print the current working directory\n",
    "    print(f\"Current working directory: {current_directory}\")\n",
    "    \n",
    "    # Construct the path to 'data/rag_use_cases' inside the current directory\n",
    "    data_directory = os.path.join(current_directory, 'data', 'rag_use_cases')\n",
    "    \n",
    "    # Print the resulting path\n",
    "    print(f\"Data directory path: {data_directory}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    # Handle the case where the directory path does not exist\n",
    "    print(f\"Error: The specified path does not exist - {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # General exception handler for any other errors\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50af1077-3400-45eb-9fa7-492a259b9d8e",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "##### Make Sure that data_directory is pointing to the right path and data files are present. Otherwise, you need to change the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb26c0-6ea3-4153-9813-4f0a69aa9242",
   "metadata": {},
   "source": [
    "# Define prompt, Amazon Bedrock Foundation model, and Amazon Bedrock embed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83242d79-e461-4aad-9ca0-9ac928b461d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define prompt\n",
    "prompt = \"What is Amazon doing and cashflow?\"\n",
    "\n",
    "# List of Bedrock models with names and model codes\n",
    "bedrock_model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# List of Bedrock embed models with names and model codes\n",
    "bedrock_embed_model_id = \"amazon.titan-embed-text-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a707d91-23c2-4786-bd2a-6287ca51ef82",
   "metadata": {},
   "source": [
    "## Define important environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4247b5-5fe7-4ec8-9f25-bff782734606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Try-except block to handle potential errors\n",
    "try:\n",
    "    # Create a new Boto3 session to interact with AWS services\n",
    "    # This session is responsible for managing credentials and region configuration\n",
    "    boto3_session = boto3.session.Session()\n",
    "\n",
    "    # Retrieve the current AWS region from the session (e.g., 'us-east-1', 'us-west-2')\n",
    "    aws_region_name = boto3_session.region_name\n",
    "    \n",
    "    # Initialize Bedrock and Bedrock Runtime clients using Boto3\n",
    "    # These clients will allow interactions with Bedrock-related AWS services\n",
    "    boto3_bedrock_client = boto3.client('bedrock', region_name=aws_region_name)\n",
    "    boto3_bedrock_runtime_client = boto3.client('bedrock-runtime', region_name=aws_region_name)\n",
    "\n",
    "    # Store all relevant variables in a dictionary for easier access and management\n",
    "    variables_store = {\n",
    "        \"aws_region_name\": aws_region_name,                          # AWS region name\n",
    "        \"boto3_bedrock_client\": boto3_bedrock_client,                # Bedrock client instance\n",
    "        \"boto3_bedrock_runtime_client\": boto3_bedrock_runtime_client,  # Bedrock Runtime client instance\n",
    "        \"boto3_session\": boto3_session                               # Current Boto3 session object\n",
    "    }\n",
    "\n",
    "    # Print all stored variables for debugging and verification\n",
    "    for var_name, value in variables_store.items():\n",
    "        print(f\"{var_name}: {value}\")\n",
    "\n",
    "# Handle any exceptions that occur during the execution\n",
    "except Exception as e:\n",
    "    # Print the error message if an unexpected error occurs\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84716f64-dd11-4852-b3b9-bb550a0da1c6",
   "metadata": {},
   "source": [
    "# Important package to consider for Llama Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd0ff9-5732-471d-a5b4-8028987b9bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary modules from LlamaIndex for VectorStore management and AWS Bedrock interaction\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.bedrock import Bedrock\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding, Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674cb33d-c1f3-4b22-806a-cc13983dcc33",
   "metadata": {},
   "source": [
    "# Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d854a5-e696-4eb4-8cd1-e7ba726fe29f",
   "metadata": {},
   "source": [
    "#  Embedding Model Initialization and LLaMA Settings Configuration\n",
    "\n",
    "### 1. initialize_bedrock_embeddings Function:\n",
    "##### Initializes the Bedrock Embeddings model using the provided Bedrock client and model ID.\n",
    "##### Includes error handling to manage failures during initialization.\n",
    "\n",
    "### 2. configure_llama_settings Function:\n",
    "##### Configures global settings for LLaMA by assigning the LLM, embeddings model, and chunk size.\n",
    "##### Error handling ensures that any issues during configuration are logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1b3eff-e15a-4251-a4ea-999ad017894b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Function to initialize the Bedrock embeddings model\n",
    "def initialize_bedrock_embeddings(bedrock_client, model_id):\n",
    "    \"\"\"\n",
    "    Initializes the Bedrock Embeddings model using the provided Bedrock client and model ID.\n",
    "    \n",
    "    Args:\n",
    "        bedrock_client: The initialized Bedrock client.\n",
    "        model_id: The model ID for the embeddings model.\n",
    "    \n",
    "    Returns:\n",
    "        embeddings_model: The initialized Bedrock Embeddings model or None if initialization fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to initialize the embeddings model\n",
    "        embeddings_model = BedrockEmbeddings(client=bedrock_client, model_id=model_id)\n",
    "        print(\"Successfully initialized Bedrock Embeddings model.\")\n",
    "        return embeddings_model\n",
    "    except Exception as e:\n",
    "        # Handle errors in model initialization\n",
    "        print(f\"Error initializing Bedrock Embeddings model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to set up core settings for LLM and embeddings model\n",
    "def configure_llama_settings(llm, embed_model, chunk_size=512):\n",
    "    \"\"\"\n",
    "    Configures settings for LLM and the embeddings model, including chunk size.\n",
    "    \n",
    "    Args:\n",
    "        llm: The initialized language model.\n",
    "        embed_model: The initialized embeddings model.\n",
    "        chunk_size: The chunk size for processing documents (default is 512).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configure the LLaMA settings globally\n",
    "        Settings.llm = llm\n",
    "        Settings.embed_model = embed_model\n",
    "        Settings.chunk_size = chunk_size\n",
    "        print(f\"Settings configured: LLM={llm}, Embeddings Model={embed_model}, Chunk Size={chunk_size}\")\n",
    "    except Exception as e:\n",
    "        # Handle errors in configuration\n",
    "        print(f\"Error configuring LLaMA settings: {e}\")\n",
    "\n",
    "# Example usage with modular try-catch blocks for setup\n",
    "try:\n",
    "    # Initialize Bedrock embeddings model\n",
    "    embed_model = initialize_bedrock_embeddings(boto3_bedrock_runtime_client, bedrock_embed_model_id)\n",
    "    if not embed_model:\n",
    "        raise Exception(\"Bedrock embeddings model initialization failed.\")\n",
    "\n",
    "    # Initialize the LLM (replace with your LLM instance as required)\n",
    "    llm = ChatBedrock(client=boto3_bedrock_runtime_client, model_id=bedrock_model_id)\n",
    "\n",
    "    # Configure settings for LLaMA with the LLM and embeddings model\n",
    "    configure_llama_settings(llm, embed_model, chunk_size=512)\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle unexpected errors during the setup process\n",
    "    print(f\"An unexpected error occurred during setup: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed3c25-0a43-4139-a815-bd84c519dd11",
   "metadata": {},
   "source": [
    "# Loading Documents, Creating a Vector Store Index with llama index, Executing a Query with RAG Strategy\n",
    "\n",
    "### 1. load_documents_from_directory:\n",
    "##### Loads documents from a specified directory using the SimpleDirectoryReader.\n",
    "##### It returns the documents if successful or None if an error occurs, with error handling included.\n",
    "\n",
    "### 2. create_vector_store_index:\n",
    "##### Creates a VectorStoreIndex from the loaded documents.\n",
    "##### It returns the created index or None if there is an error.\n",
    "\n",
    "### 3. execute_query:\n",
    "##### Executes a query using the QueryEngine created from the index and returns the response.\n",
    "##### Includes error handling for issues during query execution.\n",
    "\n",
    "### 4. load_data_retrieve:\n",
    "##### Main function that orchestrates the document loading, index creation, and query execution process.\n",
    "##### If any step fails (loading documents, creating the index, or executing the query), it exits with appropriate messages.\n",
    "##### If the process succeeds, it prints the final query response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81613f85-ccd2-41fd-a292-d703053f1d24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Function to load documents from a directory\n",
    "def load_documents_from_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Loads documents from the specified directory using SimpleDirectoryReader.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing documents.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of loaded documents or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load documents from the specified directory\n",
    "        documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "        print(f\"Successfully loaded {len(documents)} documents from {directory_path}.\")\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        # Handle errors during document loading\n",
    "        print(f\"Error loading documents from {directory_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to create a VectorStore index from the loaded documents\n",
    "def create_vector_store_index(documents):\n",
    "    \"\"\"\n",
    "    Creates a VectorStoreIndex from the provided documents.\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of documents to be indexed.\n",
    "    \n",
    "    Returns:\n",
    "        VectorStoreIndex: The created index or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a VectorStoreIndex from the provided documents\n",
    "        index = VectorStoreIndex.from_documents(documents)\n",
    "        print(\"VectorStore index created successfully.\")\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        # Handle errors during index creation\n",
    "        print(f\"Error creating VectorStore index: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a988e18c-2c8b-4d50-b791-2644c1115262",
   "metadata": {},
   "source": [
    "# Section 2\n",
    "\n",
    "## Generating responses with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe05ed-cde7-4b8b-ab91-d493f825cd7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Function to execute a query using the query engine\n",
    "def execute_query(query_engine, query):\n",
    "    \"\"\"\n",
    "    Executes a query on the query engine and returns the response.\n",
    "    \n",
    "    Args:\n",
    "        query_engine (QueryEngine): The query engine to use for querying.\n",
    "        query (str): The query string to execute.\n",
    "    \n",
    "    Returns:\n",
    "        str: The query response or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Query the index and return the response\n",
    "        response = query_engine.query(query)\n",
    "        print(f\"Query executed successfully. Response: {response}\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        # Handle errors during query execution\n",
    "        print(f\"Error executing query '{query}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Main function to run the process\n",
    "def load_data_retrieve():\n",
    "    \"\"\"\n",
    "    Main function to load documents, create the index, and execute a query.\n",
    "    Handles the entire data retrieval process.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load documents\n",
    "        documents = load_documents_from_directory(data_directory)\n",
    "\n",
    "        if not documents:\n",
    "            print(\"No documents loaded, exiting.\")\n",
    "            return\n",
    "\n",
    "        # Create the index from loaded documents\n",
    "        index = create_vector_store_index(documents)\n",
    "\n",
    "        if not index:\n",
    "            print(\"Index creation failed, exiting.\")\n",
    "            return\n",
    "\n",
    "        # Convert index to a query engine\n",
    "        query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "        # Execute query\n",
    "        response = execute_query(query_engine, prompt)\n",
    "\n",
    "        # Print the final response\n",
    "        if response:\n",
    "            print(\"Final response:\", response)\n",
    "    except Exception as e:\n",
    "        # Handle any unexpected errors in the main process\n",
    "        print(f\"An unexpected error occurred in the main process: {e}\")\n",
    "\n",
    "# Execute the main section\n",
    "load_data_retrieve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1827aa2b-a5be-467e-916f-55b7d19857c4",
   "metadata": {},
   "source": [
    "# You shouod try to generate responses without RAG and compare the response with RAG result. \n",
    "\n",
    "# Generating responses from Amazon Bedrock foundation model without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d121d-0be6-4dfe-b181-362b0dfcaf5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Initialize the language model\n",
    "llm = ChatBedrock(\n",
    "    model_id=bedrock_model_id,\n",
    "    client=boto3_bedrock_runtime_client\n",
    ")\n",
    "\n",
    "# Define the system prompt\n",
    "system_prompt = (\n",
    "    \"You are an assistant designed for answering questions. \"\n",
    "    \"If you are unsure of the answer, please indicate that you \"\n",
    "    \"do not know. Limit your response to a maximum of three sentences, \"\n",
    "    \"ensuring that your answer is concise.\"\n",
    ")\n",
    "\n",
    "# Function to generate a response to a question\n",
    "def generate_response(question):\n",
    "    # Combine the system prompt and the user question\n",
    "    input_text = f\"{system_prompt}\\n\\n{question}\"\n",
    "    \n",
    "    # Invoke the Bedrock model with the input text as a string\n",
    "    response = llm.invoke(input_text)\n",
    "    \n",
    "    # Extract the content of the response\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "response = generate_response(prompt)\n",
    "\n",
    "# Print the result\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c73b6e-3769-4686-8cdd-972999137344",
   "metadata": {},
   "source": [
    "# Must Read\n",
    "\n",
    "######  Please have a look of the result of both the cases. Case 1 is generating more context aware information compare to Case 2.\n",
    "\n",
    "###### Case1: Prompt enhancement with RAG and generating responses from Amazon Bedrock foundation model\n",
    "###### Case2: Generating responses from Amazon Bedrock foundation model without RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51745d43-360d-42a1-8617-a7a3a2769ab8",
   "metadata": {},
   "source": [
    "# End of NoteBook \n",
    "\n",
    "## Please ensure that you close the kernel after using this notebook to avoid any potential charges to your account.\n",
    "\n",
    "## Process: Go to \"Kernel\" at top option. Choose \"Shut Down Kernel\". \n",
    "##### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/studio-ui.html"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
