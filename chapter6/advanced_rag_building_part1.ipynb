{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed3a60a-5001-46ff-ae70-61d861a00c53",
   "metadata": {
    "tags": []
   },
   "source": [
    "# File Name: advanced_rag_building_part1.ipynb\n",
    "### Location: Chapter 6\n",
    "### Purpose: \n",
    "#####       1. Example of simple RAG pattern.\n",
    "#####       2. Example of HyDe RAG pattern.\n",
    "#####       3. Example of multi query RAG pattern.\n",
    "#####       4. Example of LLM Augmented Retrieval RAG pattern.\n",
    "##### Dependency: Not Applicable\n",
    "# <ins>-----------------------------------------------------------------------------------</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99464e3-0679-4a67-bb38-8d96892eefa5",
   "metadata": {},
   "source": [
    "# <ins>Amazon SageMaker Classic</ins>\n",
    "#### Those who are new to Amazon SageMaker Classic. Follow the link for the details. https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91003ca1-0026-46eb-bd09-81861135cb22",
   "metadata": {},
   "source": [
    "# <ins>Environment setup of Kernel</ins>\n",
    "##### Fill \"Image\" as \"Data Science\"\n",
    "##### Fill \"Kernel\" as \"Python 3\"\n",
    "##### Fill \"Instance type\" as \"ml-t3-medium\"\n",
    "##### Fill \"Start-up script\" as \"No Scripts\"\n",
    "##### Click \"Select\"\n",
    "\n",
    "###### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-create-open.html for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865b7f5-755a-498c-bff6-7647281b15b9",
   "metadata": {},
   "source": [
    "# <ins>Mandatory installation on the kernel through pip</ins>\n",
    "\n",
    "##### This lab will work with below software version. But, if you are trying with latest version of boto3, awscli, and botocore. This code may fail. You might need to change the corresponding api. \n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade0112-a4be-4ce8-be92-8cbf28b2acfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall -q \\\n",
    "    \"boto3>=1.34.84\" \\\n",
    "    \"langchain>=0.2.16\" \\\n",
    "    \"langchain_community>=0.2.17\" \\\n",
    "    \"awscli>=1.32.84\" \\\n",
    "    \"botocore>=1.34.84\" \\\n",
    "    \"PyPDF2\" \\\n",
    "    \"pypdf\" \\\n",
    "    \"llama-index\" \\\n",
    "    \"llama-index-llms-bedrock\" \\\n",
    "    \"llama-index-embeddings-bedrock\" \\\n",
    "    \"llama-index-embeddings-huggingface\" \\\n",
    "    \"llama-index-llms-langchain\" \\\n",
    "    \"langchain-chroma>=0.1.2\" \\\n",
    "    \"ipywidgets>=7.6.5\" \\\n",
    "    \"jupyterlab\" \\\n",
    "    \"jupyter\" \\\n",
    "    \"tqdm\" \\\n",
    "    \"iprogress>=0.4\" \\\n",
    "    \"ipynb\" \\\n",
    "    \"langchain-aws>=0.1.7\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e3b85-4e64-42ef-9355-ed2a61c68b23",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins>\n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ee934-e99c-49ce-9e94-27ffc5baabe1",
   "metadata": {},
   "source": [
    "# <ins>Restart the kernel</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf41bf-d2d2-46fa-b1d4-1dd19c45a069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5af76d-b2a9-4740-b8bf-134932804719",
   "metadata": {},
   "source": [
    "# <ins>Python package import</ins>\n",
    "\n",
    "##### boto3 offers various clients for Amazon Bedrock to execute various actions.\n",
    "##### botocore is a low-level interface to AWS tools, while boto3 is built on top of botocore and provides additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7268959-6ef0-4cbd-bd53-3b31369b4513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "import warnings\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_aws.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Import necessary modules\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1b877-deab-4961-a249-7663e812fbb8",
   "metadata": {},
   "source": [
    "### Ignore warning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71587271-9f02-46dc-9eaf-5a5291132f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ea2067-a0d1-47bf-828e-d675d155e199",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Find out data directory\n",
    "\n",
    "#### 1. Retrieves the current working directory and prints it.\n",
    "#### 2. Builds a path that navigates up one directory and appends 'data/rag_use_cases' to the path, then prints this resulting path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5156036-ec42-48fa-b13a-59ffd316cf94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get the current working directory\n",
    "    current_directory = os.getcwd()\n",
    "    \n",
    "    # Print the current working directory\n",
    "    print(f\"Current working directory: {current_directory}\")\n",
    "    \n",
    "    # Attempt to navigate up one directory and then to 'data/rag_use_cases'\n",
    "    data_directory = os.path.join(os.path.dirname(current_directory), 'data/rag_use_cases')\n",
    "    \n",
    "    # Print the resulting path\n",
    "    print(f\"Data directory path: {data_directory}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    # Handle the case where the directory path does not exist\n",
    "    print(f\"Error: The specified path does not exist - {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # General exception handler for any other errors\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88925ed9-968c-4671-abf2-f5a92e393e23",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Disclaimer\n",
    "##### Make Sure that data_directory is pointing to the right path and data files are present. Otherwise, you need to change the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2cf6f-265b-4cbd-becf-0def0815a238",
   "metadata": {},
   "source": [
    "# Define prompt, Amazon Bedrock Foundation model, and Amazon Bedrock embed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750244c4-6399-430b-b5ea-84ef10837108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define prompt\n",
    "prompt = \"What is Amazon doing and cashflow?\"\n",
    "\n",
    "# List of Bedrock models with names and model codes\n",
    "bedrock_model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# List of Bedrock embed models with names and model codes\n",
    "bedrock_embed_model_id = \"amazon.titan-embed-text-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a1e824-1734-4ca4-8b40-b5ef3f5b65e4",
   "metadata": {},
   "source": [
    "## Define important environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21c86fa-3cbd-468a-962b-8e4ef1d7fcf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try-except block to handle potential errors\n",
    "try:\n",
    "    # Create a new Boto3 session to interact with AWS services\n",
    "    # This session is responsible for managing credentials and region configuration\n",
    "    boto3_session = boto3.session.Session()\n",
    "\n",
    "    # Retrieve the current AWS region from the session (e.g., 'us-east-1', 'us-west-2')\n",
    "    aws_region_name = boto3_session.region_name\n",
    "    \n",
    "    # Initialize Bedrock and Bedrock Runtime clients using Boto3\n",
    "    # These clients will allow interactions with Bedrock-related AWS services\n",
    "    boto3_bedrock_client = boto3.client('bedrock', region_name=aws_region_name)\n",
    "    boto3_bedrock_runtime_client = boto3.client('bedrock-runtime', region_name=aws_region_name)\n",
    "\n",
    "    # Store all relevant variables in a dictionary for easier access and management\n",
    "    variables_store = {\n",
    "        \"aws_region_name\": aws_region_name,                          # AWS region name\n",
    "        \"boto3_bedrock_client\": boto3_bedrock_client,                # Bedrock client instance\n",
    "        \"boto3_bedrock_runtime_client\": boto3_bedrock_runtime_client,  # Bedrock Runtime client instance\n",
    "        \"boto3_session\": boto3_session                               # Current Boto3 session object\n",
    "    }\n",
    "\n",
    "    # Print all stored variables for debugging and verification\n",
    "    for var_name, value in variables_store.items():\n",
    "        print(f\"{var_name}: {value}\")\n",
    "\n",
    "# Handle any exceptions that occur during the execution\n",
    "except Exception as e:\n",
    "    # Print the error message if an unexpected error occurs\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb1560-7324-42d1-9f86-480b7142998a",
   "metadata": {},
   "source": [
    "# Prepare dataset \n",
    "\n",
    "##### 1.Parameter Inputs: Accepts data_directory (path to the PDF folder) and documents (list to store PDF data).\n",
    "##### 2.File Loading Loop: Iterates through all files in the directory, checking if each one has a .pdf extension.\n",
    "##### 3.PDF Loading with Error Handling: For each PDF, it uses PyPDFLoader to load the content, appending it to documents. If an error occurs, it prints an error message for that specific file, allowing the process to continue.\n",
    "##### 4.Document Check and Output: If any documents are loaded, it prints the content of the first page from the first document; otherwise, it notifies that no PDFs were found.\n",
    "##### 5.Function Return: Returns the updated documents list with loaded PDF content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9632866b-2fba-4526-96bb-a003db1dd8ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pdf_documents(data_directory, documents):\n",
    "    \"\"\"\n",
    "    Load PDF documents from the specified directory and append their content to the documents list.\n",
    "\n",
    "    Parameters:\n",
    "    - data_directory (str): The directory path containing the PDF files.\n",
    "    - documents (list): A list to store the loaded PDF data.\n",
    "\n",
    "    Returns:\n",
    "    - list: The updated documents list with the loaded PDF content.\n",
    "    \"\"\"\n",
    "    # Loop through all files in the specified directory\n",
    "    for filename in os.listdir(data_directory):\n",
    "        if filename.endswith('.pdf'):  # Check if the file is a PDF\n",
    "            file_path = os.path.join(data_directory, filename)\n",
    "            try:\n",
    "                # Initialize the PDF loader for the current file\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                \n",
    "                # Load the PDF data\n",
    "                data = loader.load()\n",
    "                \n",
    "                # Extend the documents list with the loaded data\n",
    "                documents.extend(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")  # Handle exceptions during loading\n",
    "\n",
    "    # Display the content of the first page of the first document if available\n",
    "    if documents:\n",
    "        print(documents[0].page_content)  # Printing page content of the first document\n",
    "    else:\n",
    "        print(\"No PDF files found in the folder.\")\n",
    "\n",
    "    return documents\n",
    "\n",
    "# Usage\n",
    "documents = []\n",
    "documents = load_pdf_documents(data_directory, documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4893883a-f12f-4e41-b27c-397e7f9cf410",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modular Python setup for initializing language and embeddings models with error handling using Bedrock client.\n",
    "\n",
    "### 1. initialize_language_model: This function initializes a language model using the Bedrock client and model ID. If successful, it returns the model; otherwise, it returns None.\n",
    "\n",
    "### 2. initialize_embeddings_model: Similar to the first function, this initializes the embeddings model using the Bedrock client and embeddings model ID, with error handling in case of failure.\n",
    "\n",
    "### 3. setup_models: This is the main function that calls the previous two functions to initialize both the language model and embeddings model. It handles errors, ensuring that if either initialization fails, the process is halted, and appropriate messages are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5705e94-0d88-4fb0-a176-365b38816ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing necessary packages from langchain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Function to initialize the language model with the Bedrock client\n",
    "def initialize_language_model(client, model_id):\n",
    "    \"\"\"\n",
    "    Initializes the language model using the provided Bedrock client and model ID.\n",
    "    \n",
    "    Args:\n",
    "        client: The Bedrock client for model invocation.\n",
    "        model_id (str): The ID of the language model to be initialized.\n",
    "    \n",
    "    Returns:\n",
    "        ChatBedrock: The initialized language model or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the language model\n",
    "        llm = ChatBedrock(model_id=model_id, client=client)\n",
    "        print(\"Successfully initialized the language model.\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        # Handle any errors during initialization\n",
    "        print(f\"Error initializing the language model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to initialize the Bedrock Embeddings model\n",
    "def initialize_embeddings_model(client, embed_model_id):\n",
    "    \"\"\"\n",
    "    Initializes the embeddings model using the provided Bedrock client and embeddings model ID.\n",
    "    \n",
    "    Args:\n",
    "        client: The Bedrock client for model invocation.\n",
    "        embed_model_id (str): The ID of the embeddings model to be initialized.\n",
    "    \n",
    "    Returns:\n",
    "        BedrockEmbeddings: The initialized embeddings model or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the embeddings model\n",
    "        embeddings_model = BedrockEmbeddings(client=client, model_id=embed_model_id)\n",
    "        print(\"Successfully initialized the Bedrock Embeddings model.\")\n",
    "        return embeddings_model\n",
    "    except Exception as e:\n",
    "        # Handle any errors during initialization\n",
    "        print(f\"Error initializing the Bedrock Embeddings model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main function to set up both models with error handling\n",
    "def setup_models(bedrock_client, bedrock_model_id, embed_model_id):\n",
    "    \"\"\"\n",
    "    Sets up the language model and embeddings model, handling errors during the setup.\n",
    "    \n",
    "    Args:\n",
    "        bedrock_client: The Bedrock client used to interact with the models.\n",
    "        bedrock_model_id (str): The ID of the language model.\n",
    "        embed_model_id (str): The ID of the embeddings model.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (llm, embeddings_model) initialized models or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the language model\n",
    "        llm = initialize_language_model(bedrock_client, bedrock_model_id)\n",
    "        if not llm:\n",
    "            # Return None if language model initialization failed\n",
    "            print(\"Failed to initialize language model, exiting setup.\")\n",
    "            return None, None\n",
    "\n",
    "        # Initialize the embeddings model\n",
    "        embeddings_model = initialize_embeddings_model(bedrock_client, embed_model_id)\n",
    "        if not embeddings_model:\n",
    "            # Return None if embeddings model initialization failed\n",
    "            print(\"Failed to initialize embeddings model, exiting setup.\")\n",
    "            return llm, None\n",
    "\n",
    "        print(\"Both models initialized successfully.\")\n",
    "        return llm, embeddings_model\n",
    "    except Exception as e:\n",
    "        # Handle any unexpected errors during the setup process\n",
    "        print(f\"Unexpected error in setup process: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage: setting up both models\n",
    "llm, embeddings_model = setup_models(boto3_bedrock_runtime_client, bedrock_model_id, bedrock_embed_model_id)\n",
    "\n",
    "if llm and embeddings_model:\n",
    "    # Indicate that both models are ready for use\n",
    "    print(\"Language model and embeddings model are ready for use.\")\n",
    "else:\n",
    "    # Indicate that there was an issue with initialization\n",
    "    print(\"One or both models failed to initialize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa42f66-24f8-45cf-9012-62f0932022ef",
   "metadata": {},
   "source": [
    "# Advanced RAG Patterns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd972a-78ee-4ad0-8adc-e6d394845033",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Simple Langchain Rag pattern\n",
    "\n",
    "< Details >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb22f7-f0fb-4cbb-99bb-7dac5752f241",
   "metadata": {},
   "source": [
    "# Modular document processing with error handling: splitting documents, creating embeddings, and storing in Chroma vector store.\n",
    "\n",
    "### 1. split_documents:\n",
    "##### Splits the provided documents into smaller chunks based on the specified chunk size and overlap.\n",
    "##### If splitting fails, it returns None and prints an error message.\n",
    "\n",
    "### 2. create_vectorstore:\n",
    "##### Creates a vector store using Chroma, which stores the embeddings of the split documents.\n",
    "##### If an error occurs during the vector store creation, it returns None and prints an error message.\n",
    "\n",
    "### 3. create_ingest_vector_store:\n",
    "##### Manages the overall process by calling split_documents and create_vectorstore in sequence.\n",
    "##### If any step fails (splitting or vector store creation), it prints an error message and halts further execution.\n",
    "##### Returns the created vector store if both steps are successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b90292-953c-4bb8-8460-be3be6de5477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to split the documents into chunks\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Splits the provided documents into smaller chunks.\n",
    "\n",
    "    Args:\n",
    "        documents (list): List of documents to be split.\n",
    "        chunk_size (int): The maximum size of each chunk (default is 1000).\n",
    "        chunk_overlap (int): The overlap between consecutive chunks (default is 200).\n",
    "\n",
    "    Returns:\n",
    "        list: List of document chunks or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize a recursive character text splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        \n",
    "        # Split documents into smaller chunks\n",
    "        splits = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # Print the number of chunks created\n",
    "        print(f\"Number of splits: {len(splits)}\")\n",
    "        return splits\n",
    "    except Exception as e:\n",
    "        # Handle errors that occur during document splitting\n",
    "        print(f\"Error while splitting documents: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to create embeddings and store vectors using Chroma\n",
    "def create_vectorstore(splits, embeddings_model):\n",
    "    \"\"\"\n",
    "    Creates a vector store from the document splits using an embeddings model.\n",
    "\n",
    "    Args:\n",
    "        splits (list): List of document chunks to be indexed.\n",
    "        embeddings_model: The model used for embedding the documents.\n",
    "\n",
    "    Returns:\n",
    "        Chroma: The created vector store or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a vector store using the splits and embeddings model\n",
    "        vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings_model)\n",
    "        \n",
    "        # Print success message\n",
    "        print(\"Vector store created successfully.\")\n",
    "        return vectorstore\n",
    "    except Exception as e:\n",
    "        # Handle errors during vector store creation\n",
    "        print(f\"Error while creating vector store: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main execution function to modularize the flow\n",
    "def create_ingest_vector_store(documents, embeddings_model):\n",
    "    \"\"\"\n",
    "    Main function to manage the ingestion and creation of vector store.\n",
    "\n",
    "    Args:\n",
    "        documents (list): List of documents to process.\n",
    "        embeddings_model: The model used for embedding documents into vectors.\n",
    "\n",
    "    Returns:\n",
    "        Chroma: The vector store created from the documents or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Split documents into chunks\n",
    "        splits = split_documents(documents)\n",
    "        \n",
    "        # Check if document splitting was successful\n",
    "        if not splits:\n",
    "            raise Exception(\"Document splitting failed\")\n",
    "\n",
    "        # Step 2: Create a vector store using the embeddings model\n",
    "        vectorstore = create_vectorstore(splits, embeddings_model)\n",
    "        \n",
    "        # Check if vector store creation was successful\n",
    "        if not vectorstore:\n",
    "            raise Exception(\"Vector store creation failed\")\n",
    "\n",
    "        # If all steps succeed\n",
    "        print(\"Process completed successfully.\")\n",
    "        return vectorstore\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Handle any errors during the overall process\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "vectorstore_chroma = create_ingest_vector_store(documents, embeddings_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd4cdf9-ebd9-4ca3-aaad-1a358b2e02b3",
   "metadata": {},
   "source": [
    "# Modular functions for creating a prompt template, setting up a retrieval QA system, executing a query, and managing the entire process with error handling.\n",
    "\n",
    "### 1. create_prompt_template(): Defines the assistant's prompt template for generating responses, with error handling for template creation.\n",
    "### 2. setup_retrieval_qa(): Configures the RetrievalQA chain using the LLM, vector store, and prompt template, including error handling for setup failures.\n",
    "### 3. execute_query(): Executes a query on the retrieval QA chain and returns the response, with error handling for execution failures.\n",
    "### 4. main_retrieval_qa(): Coordinates the entire process: creating the prompt, setting up the retrieval QA system, and executing the query, with error handling at each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a6087-2bf7-4a79-9e39-de08725d2666",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the prompt template for the assistant\n",
    "def create_prompt_template():\n",
    "    try:\n",
    "        # Template instructing the assistant on response generation\n",
    "        prompt_template = \"\"\"\n",
    "            Human: Please use the following context to provide a clear and concise answer to the question below. \n",
    "            If the answer is unknown, simply state that you don't know, without attempting to guess.\n",
    "            \n",
    "            <context>\n",
    "            {context}\n",
    "            </context>\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            Assistant:\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(\n",
    "            template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        print(\"Prompt template created successfully.\")\n",
    "        return prompt\n",
    "    except Exception as e:\n",
    "        # Handle errors during prompt template creation\n",
    "        print(f\"Error creating prompt template: {e}\")\n",
    "        return None\n",
    "\n",
    "# Configure the retrieval-based question-answering chain\n",
    "def setup_retrieval_qa(llm, vectorstore_chroma, prompt):\n",
    "    try:\n",
    "        # Set up the RetrievalQA chain using the LLM, vector store, and prompt\n",
    "        retrievalqa_res = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=vectorstore_chroma.as_retriever(\n",
    "                search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "            ),\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": prompt}\n",
    "        )\n",
    "        print(\"RetrievalQA chain set up successfully.\")\n",
    "        return retrievalqa_res\n",
    "    except Exception as e:\n",
    "        # Handle errors during the setup of the RetrievalQA chain\n",
    "        print(f\"Error setting up RetrievalQA: {e}\")\n",
    "        return None\n",
    "\n",
    "# Execute the query on the retrieval QA chain\n",
    "def execute_query(retrievalqa_res, prompt):\n",
    "    try:\n",
    "        # Run the query with the provided prompt\n",
    "        response = retrievalqa_res({\"query\": prompt})\n",
    "        print(\"Query executed successfully.\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        # Handle errors during query execution\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main process for setting up and running the RetrievalQA chain\n",
    "def main_retrieval_qa(llm, vectorstore_chroma, prompt_text):\n",
    "    try:\n",
    "        # Step 1: Create the prompt template\n",
    "        prompt = create_prompt_template()\n",
    "        if not prompt:\n",
    "            print(\"Failed to create prompt template. Exiting.\")\n",
    "            return\n",
    "\n",
    "        # Step 2: Set up the RetrievalQA chain\n",
    "        retrievalqa_res = setup_retrieval_qa(llm, vectorstore_chroma, prompt)\n",
    "        if not retrievalqa_res:\n",
    "            print(\"Failed to set up RetrievalQA chain. Exiting.\")\n",
    "            return\n",
    "\n",
    "        # Step 3: Execute the query with the provided prompt text\n",
    "        response = execute_query(retrievalqa_res, prompt_text)\n",
    "        if response:\n",
    "            print(\"Final Response:\", response)\n",
    "        else:\n",
    "            print(\"No response returned.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any unexpected errors during the main process\n",
    "        print(f\"An unexpected error occurred in the main process: {e}\")\n",
    "\n",
    "# Example usage\n",
    "main_retrieval_qa(llm, vectorstore_chroma, prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e075157-bd02-4fbd-996f-3f20f9d2eb04",
   "metadata": {},
   "source": [
    "# 2. HYDE patterns\n",
    "\n",
    "< Details >\n",
    "\n",
    "#### Refer:\n",
    "https://arxiv.org/pdf/2212.10496.pdf https://github.com/langchainai/langchain/blob/master/cookbook/hypothetical_document_embeddings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff618ee-138d-44d7-989f-75184aa2190e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python important package \n",
    "from langchain.chains import HypotheticalDocumentEmbedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c7772-944b-4c97-b751-332c73c4d992",
   "metadata": {},
   "source": [
    "# Modular function to define a prompt template and replace placeholders with context and question values, with error handling for key errors and other exceptions.\n",
    "\n",
    "### 1. Prompt Template Definition:\n",
    "#### The hyde_prompt_template is a string that contains placeholders for the context and question. It instructs the assistant to provide concise answers or state \"I don't know\" if the answer is not available.\n",
    "\n",
    "### 2. Function to Format the Prompt:\n",
    "#### The get_refined_prompt_template() function takes in the prompt template, context, and question as inputs.\n",
    "#### It uses the format() method to replace the placeholders ({context} and {question}) in the template with the provided context and question.\n",
    "#### The function returns the formatted prompt or None in case of an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec4cb6e-8db0-4466-ac3a-5bd0255707da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the prompt template with placeholders for context and question\n",
    "hyde_prompt_template = \"\"\"\n",
    "    Human: Use the following pieces of context to provide a concise answer to the question at the end. \n",
    "    If you don't know the answer, just say that you don't know; don't try to make up an answer.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Assistant:\n",
    "\"\"\"\n",
    "\n",
    "def get_refined_prompt_template(prompt_template, context, question):\n",
    "    \"\"\"\n",
    "    Replaces placeholders in the provided prompt template with the given context and question.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt_template (str): The template string containing placeholders for context and question.\n",
    "    - context (str): The context to be inserted into the template.\n",
    "    - question (str): The question to be inserted into the template.\n",
    "    \n",
    "    Returns:\n",
    "    - str or None: The prompt with context and question inserted, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Substitute the placeholders with context and question values\n",
    "        formatted_prompt = prompt_template.format(context=context, question=question)\n",
    "        return formatted_prompt\n",
    "    except KeyError as e:\n",
    "        print(f\"Key error formatting prompt template: Missing placeholder - {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error formatting prompt template: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409c9c7-95a1-4e90-bb4b-ee3f478978e4",
   "metadata": {},
   "source": [
    "# The code defines functions to format a payload, invoke the Bedrock model, and generate a response based on user and assistant prompts with error handling at each step.\n",
    "\n",
    "### 1. format_bedrock_payload:\n",
    "##### This function formats the JSON payload for invoking the Bedrock model. It includes parameters like:\n",
    "##### user_prompt: The user's input.\n",
    "##### assistant_prompt: The assistant's previous response or context.\n",
    "##### max_tokens, temperature, and top_p: Model configuration settings.\n",
    "##### It constructs the payload in the required format and returns it as a JSON string. Errors are caught and logged.\n",
    "\n",
    "### 2. invoke_bedrock_model:\n",
    "##### This function uses the boto3 client to invoke the Bedrock model with the formatted payload.\n",
    "##### It makes an API call to the model and retrieves the response.\n",
    "##### The function extracts the text content from the response and returns it. If an error occurs, it logs the error and returns None.\n",
    "\n",
    "### 3. call_bedrock:\n",
    "##### This function orchestrates the process by:\n",
    "##### Calling format_bedrock_payload to create the payload.\n",
    "##### Passing the payload to invoke_bedrock_model for model invocation.\n",
    "##### Returning the generated response from the model.\n",
    "##### It checks for successful payload creation and model invocation, handling errors gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266119cd-4fdc-4b75-b529-eaf939c70958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_bedrock_payload(user_prompt, assistant_prompt, max_tokens=512, temperature=0.5, top_p=1.0):\n",
    "    \"\"\"\n",
    "    Formats the payload for Bedrock model invocation.\n",
    "\n",
    "    Parameters:\n",
    "    - user_prompt (str): The user's input.\n",
    "    - assistant_prompt (str): The assistant's previous response or context.\n",
    "    - max_tokens (int): Maximum number of tokens in the response.\n",
    "    - temperature (float): Sampling temperature.\n",
    "    - top_p (float): Nucleus sampling parameter.\n",
    "\n",
    "    Returns:\n",
    "    - str: JSON payload for Bedrock model invocation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": 'user', \"content\": [{'type': 'text', 'text': user_prompt}]},\n",
    "            {\"role\": 'assistant', \"content\": [{'type': 'text', 'text': assistant_prompt}]}\n",
    "        ]\n",
    "        \n",
    "        payload = json.dumps({\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p\n",
    "        })\n",
    "        \n",
    "        return payload\n",
    "    except Exception as e:\n",
    "        print(f\"Error formatting Bedrock payload: {e}\")\n",
    "        return None\n",
    "\n",
    "def invoke_bedrock_model(boto3_client, payload, model_id, accept='application/json', content_type='application/json'):\n",
    "    \"\"\"\n",
    "    Invokes the Bedrock model with the given payload.\n",
    "\n",
    "    Parameters:\n",
    "    - boto3_client (boto3 client): Bedrock runtime client.\n",
    "    - payload (str): JSON payload for the model.\n",
    "    - model_id (str): Model ID for Bedrock.\n",
    "    - accept (str): Accept header.\n",
    "    - content_type (str): Content-Type header.\n",
    "\n",
    "    Returns:\n",
    "    - str: Text response from the model, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = boto3_client.invoke_model(\n",
    "            body=payload,\n",
    "            modelId=model_id,\n",
    "            accept=accept,\n",
    "            contentType=content_type\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        response_text = response_body.get('content')[0]['text']\n",
    "        \n",
    "        return response_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking Bedrock model: {e}\")\n",
    "        return None\n",
    "\n",
    "def call_bedrock(user_prompt, assistant_prompt, model_id=bedrock_model_id):\n",
    "    \"\"\"\n",
    "    Generates a response from the Bedrock model based on the user and assistant prompts.\n",
    "\n",
    "    Parameters:\n",
    "    - user_prompt (str): The user's input prompt.\n",
    "    - assistant_prompt (str): The assistant's response or initial prompt.\n",
    "    - model_id (str): The Bedrock model ID.\n",
    "\n",
    "    Returns:\n",
    "    - str: The response generated by the Bedrock model, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    # Step 1: Format the payload\n",
    "    payload = format_bedrock_payload(user_prompt, assistant_prompt)\n",
    "    if payload is None:\n",
    "        print(\"Failed to create payload.\")\n",
    "        return None\n",
    "\n",
    "    # Step 2: Invoke the model with the formatted payload\n",
    "    response_text = invoke_bedrock_model(boto3_bedrock_runtime_client, payload, model_id)\n",
    "    \n",
    "    # Step 3: Check and return the model's response\n",
    "    if response_text:\n",
    "        print(\"Response received successfully.\")\n",
    "    else:\n",
    "        print(\"No response received from the model.\")\n",
    "    \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594bdaf-960a-43bb-8c51-8350c9efbd8e",
   "metadata": {},
   "source": [
    "# A function to embed a user query, search for relevant documents, create a refined prompt, and retrieve a response from the Bedrock model, with error handling for key and general exceptions.\n",
    "\n",
    "### 1. Initialize Embeddings Model:\n",
    "##### Uses HypotheticalDocumentEmbedder to create an embedding model (hyde_embeddings) from the provided LLM and embeddings model, specifically for web search context.\n",
    "\n",
    "### 2. Embed User Query:\n",
    "##### Embeds the userâ€™s input (prompt) into a vector representation using embed_query.\n",
    "\n",
    "### 3. Document Similarity Search:\n",
    "##### Performs a similarity search in the vectorstore_chroma to find the top 5 relevant documents based on the embedded query vector.\n",
    "\n",
    "### 4. Context Creation:\n",
    "##### Extracts the content from the relevant documents and constructs a context string to provide more information for the model's response.\n",
    "\n",
    "### 5. Generate Refined Prompt:\n",
    "##### Calls get_refined_prompt_template to create a refined prompt incorporating the context and the original user prompt.\n",
    "\n",
    "### 6. Call Bedrock Model:\n",
    "##### Uses the call_bedrock function to send the refined prompt to the Bedrock model and retrieve a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919cdcf-d23b-45ee-b018-79e924ad4205",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to embed a query using HypotheticalDocumentEmbedder and search for relevant documents\n",
    "def generate_response_from_bedrock(prompt, embeddings_model, vectorstore_chroma, llm, bedrock_model_id):\n",
    "    try:\n",
    "        # Step 1: Initialize the HypotheticalDocumentEmbedder\n",
    "        hyde_embeddings = HypotheticalDocumentEmbedder.from_llm(llm, embeddings_model, \"web_search\")\n",
    "        print(\"Embeddings model initialized successfully.\")\n",
    "        \n",
    "        # Step 2: Embed the user query\n",
    "        result = hyde_embeddings.embed_query(prompt)\n",
    "        print(\"Query embedded successfully.\")\n",
    "        \n",
    "        # Step 3: Perform similarity search using the vectorstore\n",
    "        relevant_docs = vectorstore_chroma.similarity_search_by_vector(result, k=5)\n",
    "        print(f\"Found {len(relevant_docs)} relevant documents.\")\n",
    "        \n",
    "        # Step 4: Create the context from the relevant documents\n",
    "        context = \"\"\n",
    "        for doc in relevant_docs:\n",
    "            context = f\"{context} \\n {doc.page_content}\"\n",
    "        \n",
    "        # Step 5: Get the refined prompt with the context\n",
    "        prompt_template = get_refined_prompt_template(hyde_prompt_template, context, prompt)\n",
    "        if not prompt_template:\n",
    "            print(\"Failed to create prompt template. Exiting.\")\n",
    "            return None\n",
    "        print(\"Refined prompt template created successfully.\")\n",
    "        \n",
    "        # Step 6: Call the Bedrock model to get the response\n",
    "        response = call_bedrock(prompt_template, \"Assistant:\", bedrock_model_id)\n",
    "        \n",
    "        if response:\n",
    "            print(\"Response received successfully.\")\n",
    "        else:\n",
    "            print(\"No response received from Bedrock model.\")\n",
    "        \n",
    "        return ( response, prompt_template )\n",
    "\n",
    "    except KeyError as e:\n",
    "        # Handle errors related to missing keys\n",
    "        print(f\"Key error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Handle any other unexpected errors\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "( response, prompt_template ) = generate_response_from_bedrock(prompt, embeddings_model, vectorstore_chroma, llm, bedrock_model_id)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c81be-6627-49d5-8d8b-8612289bc333",
   "metadata": {},
   "source": [
    "# End-to-end process for generating responses from Bedrock using HyDE embeddings, document retrieval, and prompt generation.\n",
    "\n",
    "### 1. Initialize HyDE Embeddings using the provided LLM and embeddings model.\n",
    "### 2. Generate an Embedding for the input prompt using HyDE.\n",
    "### 3. Search for Relevant Documents in the vector store based on the embedding.\n",
    "### 4. Compile Context from the retrieved documents.\n",
    "### 5. Generate a Prompt with the compiled context and question.\n",
    "### 6. Call Bedrock with the generated prompt and obtain a response.\n",
    "### 7. The process includes error handling for each step to ensure robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3688973-935b-490a-80ff-73b3b257ef69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assume necessary imports for HypotheticalDocumentEmbedder, vectorstore_chroma, etc., are already handled\n",
    "\n",
    "def initialize_hyde_embeddings(llm, embeddings_model, task=\"web_search\"):\n",
    "    \"\"\"\n",
    "    Initializes the Hypothetical Document Embedder (HyDE) with the given LLM and embeddings model.\n",
    "    \n",
    "    Parameters:\n",
    "    - llm: Language model to use for HyDE.\n",
    "    - embeddings_model: Embeddings model for HyDE.\n",
    "    - task (str): Task type for the embedder, default is 'web_search'.\n",
    "    \n",
    "    Returns:\n",
    "    - HypotheticalDocumentEmbedder instance or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        hyde_embeddings = HypotheticalDocumentEmbedder.from_llm(llm, embeddings_model, task)\n",
    "        print(\"HyDE embeddings model initialized successfully.\")\n",
    "        return hyde_embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing HyDE embeddings: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_embedding(hyde_embeddings, prompt):\n",
    "    \"\"\"\n",
    "    Generates an embedding for a given prompt using the initialized HyDE embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "    - hyde_embeddings: Initialized HyDE embeddings model.\n",
    "    - prompt (str): The prompt for which to generate embeddings.\n",
    "    \n",
    "    Returns:\n",
    "    - Embedding vector or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = hyde_embeddings.embed_query(prompt)\n",
    "        print(\"Embedding generated successfully.\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for prompt: {e}\")\n",
    "        return None\n",
    "\n",
    "def search_relevant_documents(vectorstore, embedding_vector, top_k=5):\n",
    "    \"\"\"\n",
    "    Searches for relevant documents in the vector store based on the embedding vector.\n",
    "    \n",
    "    Parameters:\n",
    "    - vectorstore: The vector store to search in.\n",
    "    - embedding_vector: The embedding vector used for similarity search.\n",
    "    - top_k (int): Number of top documents to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "    - List of retrieved documents or an empty list if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        relevant_docs = vectorstore.similarity_search_by_vector(embedding_vector, k=top_k)\n",
    "        print(f\"Retrieved {len(relevant_docs)} relevant documents.\")\n",
    "        return relevant_docs\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving documents: {e}\")\n",
    "        return []\n",
    "\n",
    "def compile_context_from_docs(relevant_docs):\n",
    "    \"\"\"\n",
    "    Compiles a context string from the retrieved documents.\n",
    "    \n",
    "    Parameters:\n",
    "    - relevant_docs (list): List of documents with page content.\n",
    "    \n",
    "    Returns:\n",
    "    - Compiled context string.\n",
    "    \"\"\"\n",
    "    context = \"\"\n",
    "    for doc in relevant_docs:\n",
    "        context += f\"\\n {doc.page_content}\"\n",
    "    print(\"Context compiled from documents.\")\n",
    "    return context\n",
    "\n",
    "def generate_prompt(prompt_template, context, question):\n",
    "    \"\"\"\n",
    "    Generates a complete prompt by formatting the template with the context and question.\n",
    "    \n",
    "    Parameters:\n",
    "    - prompt_template (str): The template string.\n",
    "    - context (str): The compiled context.\n",
    "    - question (str): The question for the assistant.\n",
    "    \n",
    "    Returns:\n",
    "    - Formatted prompt or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return get_refined_prompt_template(prompt_template, context, question)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating prompt: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main function to execute the process\n",
    "def main_process(prompt, llm, embeddings_model, vectorstore, prompt_template, bedrock_model_id):\n",
    "    \"\"\"\n",
    "    Executes the end-to-end process of initializing HyDE, generating embeddings, \n",
    "    retrieving documents, compiling context, generating prompt, and calling Bedrock.\n",
    "    \n",
    "    Parameters:\n",
    "    - prompt (str): User's initial query.\n",
    "    - llm: Language model for HyDE.\n",
    "    - embeddings_model: Embeddings model for HyDE.\n",
    "    - vectorstore: Vector store for document retrieval.\n",
    "    - prompt_template (str): Template for prompt generation.\n",
    "    - bedrock_model_id (str): Model ID for Bedrock.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Initialize HyDE embeddings\n",
    "        hyde_embeddings = initialize_hyde_embeddings(llm, embeddings_model)\n",
    "        if not hyde_embeddings:\n",
    "            return\n",
    "\n",
    "        # Step 2: Generate embedding for the prompt\n",
    "        embedding_vector = generate_embedding(hyde_embeddings, prompt)\n",
    "        if not embedding_vector:\n",
    "            return\n",
    "\n",
    "        # Step 3: Retrieve relevant documents\n",
    "        relevant_docs = search_relevant_documents(vectorstore, embedding_vector)\n",
    "        if not relevant_docs:\n",
    "            print(\"No relevant documents found.\")\n",
    "            return\n",
    "\n",
    "        # Step 4: Compile context from retrieved documents\n",
    "        context = compile_context_from_docs(relevant_docs)\n",
    "\n",
    "        # Step 5: Generate final prompt\n",
    "        full_prompt = get_refined_prompt_template(prompt_template, context, prompt)\n",
    "        if not full_prompt:\n",
    "            return\n",
    "\n",
    "        # Step 6: Call Bedrock and get response\n",
    "        response = call_bedrock(full_prompt, \"Assistant:\", bedrock_model_id)\n",
    "\n",
    "        # Step 7: Print response\n",
    "        if response:\n",
    "            print(\"Final Response:\\n\", response)\n",
    "        else:\n",
    "            print(\"No response received from Bedrock.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred in the main process: {e}\")\n",
    "\n",
    "# Run main process with given parameters\n",
    "main_process(\n",
    "    prompt,\n",
    "    llm,  # Assuming llm instance is provided\n",
    "    embeddings_model,  # Assuming embeddings model instance is provided\n",
    "    vectorstore_chroma,  # Assuming vector store instance is provided\n",
    "    prompt_template,\n",
    "    bedrock_model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574b36dc-3fe6-4fa5-9fd0-48c44032dc4e",
   "metadata": {},
   "source": [
    "# 3. Multi Query Retrieval\n",
    "\n",
    "< Details >\n",
    "\n",
    "#### Refer:\n",
    "https://arxiv.org/abs/2402.03367"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a732ef-3e2a-4f9c-b6ec-1a1b2cd9cb3b",
   "metadata": {},
   "source": [
    "### The multi_query_prompt_template is designed to generate five distinct variations of a user's question to improve document retrieval accuracy in vector databases. By framing the question from multiple perspectives, this approach aims to overcome the limitations of similarity-based matching. The mq_prompt_template then guides the assistant to use the provided context to answer a question directly, ensuring concise, accurate responses or acknowledging when an answer is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824881d8-db4a-4f2c-a72e-226eaf282e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "multi_query_prompt_template = \"\"\"\n",
    "    You are an AI language model assistant. Your task is to create five unique variations of the userâ€™s question to \n",
    "    help retrieve relevant documents from a vector database. By presenting multiple perspectives on the userâ€™s question, \n",
    "    you aim to enhance the search process and address some limitations of distance-based similarity matching. \n",
    "    Please provide each alternative question separated by newlines only.\n",
    "\n",
    "    For example, these alternative questions:\n",
    "\n",
    "    'What is Amazon's current cash flow status?'\n",
    "    'Can you provide an overview of Amazon's cash flow performance?'\n",
    "    'How has Amazon's cash flow trended in recent quarters?'\n",
    "    'What factors are impacting Amazon's cash flow?'\n",
    "    'What is Amazonâ€™s approach to managing cash flow?'\n",
    "\n",
    "    Not:\n",
    "\n",
    "    'What are Amazon's main business segments and services?'\n",
    "    'How has Amazon's approach to innovation impacted its market position?'\n",
    "    'What are some recent developments in Amazonâ€™s technology and infrastructure?'\n",
    "    'How does Amazon prioritize sustainability within its operations?'\n",
    "    'What are Amazon's key strategies for customer satisfaction and loyalty?'\n",
    "\n",
    "    Original question: {question}\"\"\"\n",
    "\n",
    "mq_prompt_template = \"\"\"\n",
    "\n",
    "        Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "        <context>\n",
    "        {context}\n",
    "        </context\n",
    "\n",
    "        Question: {question}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc81a7fe-6f2b-41bd-acc4-e9768bd7feb7",
   "metadata": {},
   "source": [
    "# Functions to generate related queries and retrieve embeddings from AWS Bedrock, with error handling included.\n",
    "\n",
    "### 1. multiple_related_queries_generator(question): Generates multiple related queries based on an input question by formatting a prompt, invoking a model on AWS Bedrock, and extracting relevant queries from the response.\n",
    "\n",
    "### 2. get_embeddings(text): Retrieves an embedding vector for a given text input by preparing a payload, invoking the embedding model on AWS Bedrock, and extracting the embedding from the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d2e8ae-85f5-4f46-894b-2c3bd980bdaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multiple_related_queries_generator(question):\n",
    "    \"\"\"\n",
    "    Generates multiple related queries based on the provided question by leveraging an AI model.\n",
    "\n",
    "    Parameters:\n",
    "    - question (str): The original user question for which related queries are generated.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of generated queries relevant to the input question.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        queries_generated = []\n",
    "        \n",
    "        # Generate prompt template based on the original question\n",
    "        template = get_refined_prompt_template(multi_query_prompt_template, \"\", question)\n",
    "        \n",
    "        # Call the model to generate alternative queries\n",
    "        response_text = call_bedrock(\n",
    "            user_prompt=template,  # User's query template\n",
    "            assistant_prompt=\"Below are the generated Questions separated by \\n:\",  # Assistant's response prompt\n",
    "            model_id=bedrock_model_id\n",
    "        )\n",
    "\n",
    "        # Split the response text into separate queries, ignoring blank lines\n",
    "        queries_generated = [line for line in response_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        # Filter out short or irrelevant lines\n",
    "        queries = [q for q in queries_generated if len(q) > 5]\n",
    "\n",
    "        return queries\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating related queries: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_embeddings(text):\n",
    "    \"\"\"\n",
    "    Retrieves the embedding for a given text using Bedrock's embedding model.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text to generate embeddings for.\n",
    "\n",
    "    Returns:\n",
    "    - list or None: The embedding vector for the input text, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare the input payload for the model\n",
    "        body = json.dumps({\"inputText\": text})\n",
    "        accept = 'application/json' \n",
    "        content_type = 'application/json'\n",
    "\n",
    "        # Invoke the embedding model\n",
    "        response = boto3_bedrock_runtime_client.invoke_model(\n",
    "            body=body,\n",
    "            modelId=bedrock_embed_model_id,\n",
    "            accept=accept,\n",
    "            contentType=content_type\n",
    "        )\n",
    "        \n",
    "        # Process the response and retrieve the embedding\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        embedding = response_body.get('embedding')\n",
    "        \n",
    "        if embedding:\n",
    "            return embedding\n",
    "        else:\n",
    "            print(\"No embedding found in the response.\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for text: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e0380-cdae-4c1c-acb8-b5daa03979dc",
   "metadata": {},
   "source": [
    "# Generating Related Queries and Retrieving Relevant Documents for Bedrock Model Responses\n",
    "\n",
    "### This function generates related queries based on an initial prompt, retrieves relevant documents for each query using embeddings, compiles the document content, and formats it into a refined prompt for the Bedrock model to produce a response. The process involves calling an embedding function for each query, conducting a similarity search in a vector store, and appending any found documents to the relevant_documents string. Finally, it passes the compiled information to the Bedrock model, ensuring error handling at each step to manage any issues with query generation, document retrieval, or model invocation.\n",
    "\n",
    "### 1. Generate Related Queries:\n",
    "##### It begins by calling multiple_related_queries_generator(prompt), which generates several related queries based on the initial prompt. These queries will be used to search for relevant documents. An empty string relevant_documents is initialized to store document content gathered for each query.\n",
    "\n",
    "### 2. Retrieve Relevant Documents for Each Query:\n",
    "##### For each generated query, the code tries to:\n",
    "##### Retrieve an embedding for the query using get_embeddings(query).\n",
    "##### Use this embedding to search for relevant documents in the vectorstore_chroma vector store via similarity_search_by_vector(), limiting to the top result (k=1).\n",
    "##### If documents are found, the code appends the content of each document to the relevant_documents string, organizing them as \"Document 1\", \"Document 2,\" etc. If no results are found for a query, a message is printed.\n",
    "##### Each document retrieval process includes its own try-except block to handle and report any errors.\n",
    "\n",
    "### 3. Compile Documents and Generate Final Prompt:\n",
    "##### After collecting document content from all queries, the code uses get_refined_prompt_template() to generate a new prompt template (question_answer_template). This template is based on the original prompt, the gathered relevant_documents content, and the specified mq_prompt_template.\n",
    "\n",
    "### 4. Call Bedrock Model for Response:\n",
    "##### The refined prompt template is passed to the Bedrock model via call_bedrock() to obtain the final response.\n",
    "##### This function invocation specifies both the user prompt (question_answer_template) and an assistant response prefix (â€œAssistant:â€), along with the model ID (bedrock_model_id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db58e1b-5f0e-431e-bed6-30ef2e589fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to generate multiple related queries\n",
    "try:\n",
    "    # Generate multiple related queries based on the prompt\n",
    "    queries = multiple_related_queries_generator(prompt)\n",
    "    relevant_documents = \"\"  # Initialize empty string to store retrieved documents\n",
    "\n",
    "    # Iterate over each generated query and retrieve relevant documents\n",
    "    for i, query in enumerate(queries):\n",
    "        try:\n",
    "            # Retrieve embedding for each query and perform similarity search\n",
    "            embedding = get_embeddings(query)\n",
    "            search_results = vectorstore_chroma.similarity_search_by_vector(embedding, k=1)\n",
    "            \n",
    "            # Check if search results are not empty and add to relevant documents\n",
    "            if search_results:\n",
    "                relevant_documents += f\"\\n Document {i + 1}:\\n\" + str(search_results[0].page_content)\n",
    "            else:\n",
    "                print(f\"No relevant documents found for query {i + 1}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving document for query {i + 1}: {e}\")\n",
    "\n",
    "    # Generate refined prompt template with the gathered documents and original prompt\n",
    "    question_answer_template = get_refined_prompt_template(\n",
    "        mq_prompt_template, relevant_documents, prompt\n",
    "    )\n",
    "\n",
    "    # Call Bedrock model with the generated prompt and retrieve response\n",
    "    response = call_bedrock(\n",
    "        question_answer_template, # User prompt\n",
    "        \"Assistant:\", # Assistant's response prompt\n",
    "        bedrock_model_id\n",
    "    )\n",
    "\n",
    "    # Print the response\n",
    "    print(response)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred in the query generation and retrieval process: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c11fa0-0e60-42b0-aab9-6bc4d7c7672b",
   "metadata": {},
   "source": [
    "# 4. LLM Augmented Retrieval patterns\n",
    "\n",
    "< Details >\n",
    "\n",
    "#### Refer:\n",
    "https://arxiv.org/pdf/2404.05825.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a03db0-b583-492f-8250-04c3dbf80ae7",
   "metadata": {},
   "source": [
    "# Function to Create and Manage LLM Prompt Templates with Error Handling\n",
    "\n",
    "### Function to create a prompt template with error handling and demonstrates how to use it for generating a prompt with context and question variables. Below is a detailed breakdown of the flow:\n",
    "\n",
    "### 1. Function Definition: create_llm_aug_prompt_template: The function create_llm_aug_prompt_template is designed to create a PromptTemplate using a template string and input variables.\n",
    "### 2. Creating the PromptTemplate: The function tries to create a PromptTemplate by passing the template string and input_vars list to the PromptTemplate constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6fad54-b252-485a-a548-ddeefe4f05d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to create the prompt template with error handling\n",
    "def create_llm_aug_prompt_template(template, input_vars):\n",
    "    \"\"\"\n",
    "    Creates a prompt template by substituting variables for context and question.\n",
    "    \n",
    "    Parameters:\n",
    "    - template (str): The template string containing placeholders for context and question.\n",
    "    - input_vars (list): A list of input variable names required by the template.\n",
    "    \n",
    "    Returns:\n",
    "    - PromptTemplate or None: Returns the PromptTemplate if successful, None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to create the prompt template with the provided input variables\n",
    "        prompt_template = PromptTemplate(template=template, input_variables=input_vars)\n",
    "        print(\"Prompt template created successfully.\")\n",
    "        return prompt_template\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle errors during prompt template creation\n",
    "        print(f\"Error creating prompt template: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Define the template string and input variables\n",
    "llm_aug_prompt_template = \"\"\"\n",
    "    Human: Use the following pieces of context to provide a concise answer to the question at the end. \n",
    "    If you don't know the answer, just say that you don't know and don't try to make up an answer.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Assistant:\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt template using the function\n",
    "PROMPT = create_llm_aug_prompt_template(llm_aug_prompt_template, [\"context\", \"question\"])\n",
    "\n",
    "# Verify and use the created prompt template if successful\n",
    "if PROMPT:\n",
    "    print(\"Prompt template is ready for use.\")\n",
    "else:\n",
    "    print(\"Prompt template creation failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1945e5c-2907-480e-ad62-607124e992c5",
   "metadata": {},
   "source": [
    "### set up a RetrievalQA system that combines a language model (LLM) with a retriever to perform question answering using relevant documents. Here's a breakdown of the steps and components involved:\n",
    "\n",
    "### 1. RetrievalQA Instance Creation: The RetrievalQA.from_chain_type() method is used to create a RetrievalQA instance. This method combines a language model (LLM) with a retriever (in this case, using a Chroma Vector Store)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93098081-35b4-47e5-b927-3bef099f8339",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create a RetrievalQA instance from the specified chain type and retriever\n",
    "    retrievalqa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,  # Language model to be used\n",
    "        chain_type=\"stuff\",  # Chain type indicating how to process input/output\n",
    "        retriever=vectorstore_chroma.as_retriever(  # Set up the retriever to use similarity search\n",
    "            search_type=\"similarity\",  # Define the search type for retrieving relevant documents\n",
    "            search_kwargs={\"k\": 3}  # Set the number of top similar documents to retrieve\n",
    "        ),\n",
    "        return_source_documents=True,  # Ensure source documents are returned along with the response\n",
    "        chain_type_kwargs={\"prompt\": PROMPT}  # Pass the prompt template for question-answer generation\n",
    "    )\n",
    "\n",
    "    # Execute the retrieval QA process with the provided query (prompt)\n",
    "    response = retrievalqa({\"query\": prompt})\n",
    "\n",
    "    # Print the response from the retrieval QA system\n",
    "    print(response)\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle any errors that may occur during the RetrievalQA process\n",
    "    print(f\"Error during RetrievalQA process: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51745d43-360d-42a1-8617-a7a3a2769ab8",
   "metadata": {},
   "source": [
    "# End of NoteBook \n",
    "\n",
    "## Please ensure that you close the kernel after using this notebook to avoid any potential charges to your account.\n",
    "\n",
    "## Process: Go to \"Kernel\" at top option. Choose \"Shut Down Kernel\". \n",
    "##### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/studio-ui.html"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
