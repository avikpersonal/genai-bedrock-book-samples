{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed3a60a-5001-46ff-ae70-61d861a00c53",
   "metadata": {
    "tags": []
   },
   "source": [
    "# File Name: simple_rag_building_langchain.ipynb\n",
    "### Location: Chapter 6\n",
    "### Purpose: \n",
    "#####       1. Create an open-source Chroma vector store.  \n",
    "#####       2. Ingest data into the Vector DB.  \n",
    "#####       3. Retrieve data with langchain framework \n",
    "##### Dependency: Not Applicable\n",
    "# <ins>-----------------------------------------------------------------------------------</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99464e3-0679-4a67-bb38-8d96892eefa5",
   "metadata": {},
   "source": [
    "# <ins>Amazon SageMaker Classic</ins>\n",
    "#### Those who are new to Amazon SageMaker Classic. Follow the link for the details. https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91003ca1-0026-46eb-bd09-81861135cb22",
   "metadata": {},
   "source": [
    "# <ins>Environment setup of Kernel</ins>\n",
    "##### Fill \"Image\" as \"Data Science\"\n",
    "##### Fill \"Kernel\" as \"Python 3\"\n",
    "##### Fill \"Instance type\" as \"ml-t3-medium\"\n",
    "##### Fill \"Start-up script\" as \"No Scripts\"\n",
    "##### Click \"Select\"\n",
    "\n",
    "###### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-create-open.html for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865b7f5-755a-498c-bff6-7647281b15b9",
   "metadata": {},
   "source": [
    "# <ins>Mandatory installation on the kernel through pip</ins>\n",
    "\n",
    "##### This lab will work with below software version. But, if you are trying with latest version of boto3, awscli, and botocore. This code may fail. You might need to change the corresponding api. \n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade0112-a4be-4ce8-be92-8cbf28b2acfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall -q \\\n",
    "    \"boto3>=1.34.84\" \\\n",
    "    \"langchain>=0.2.16\" \\\n",
    "    \"langchain_community>=0.2.17\" \\\n",
    "    \"awscli>=1.32.84\" \\\n",
    "    \"botocore>=1.34.84\" \\\n",
    "    \"PyPDF2\" \\\n",
    "    \"pypdf\" \\\n",
    "    \"langchain-chroma>=0.1.2\" \\\n",
    "    \"langchain-aws>=0.1.7\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e3b85-4e64-42ef-9355-ed2a61c68b23",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins>\n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ee934-e99c-49ce-9e94-27ffc5baabe1",
   "metadata": {},
   "source": [
    "# <ins>Restart the kernel</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf41bf-d2d2-46fa-b1d4-1dd19c45a069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5af76d-b2a9-4740-b8bf-134932804719",
   "metadata": {},
   "source": [
    "# <ins>Python package import</ins>\n",
    "\n",
    "##### boto3 offers various clients for Amazon Bedrock to execute various actions.\n",
    "##### botocore is a low-level interface to AWS tools, while boto3 is built on top of botocore and provides additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7268959-6ef0-4cbd-bd53-3b31369b4513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "import warnings\n",
    "import time\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_aws.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1b877-deab-4961-a249-7663e812fbb8",
   "metadata": {},
   "source": [
    "### Ignore warning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71587271-9f02-46dc-9eaf-5a5291132f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4550613-7b53-4c15-9933-19fc78f7196c",
   "metadata": {},
   "source": [
    "# Find out data directory\n",
    "\n",
    "#### 1. Retrieves the current working directory and prints it.\n",
    "##### 2. Builds a path that navigates up one directory and appends 'data/rag_use_cases' to the path, then prints this resulting path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ee296-131b-4540-9e34-d869fc70f054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get the current working directory\n",
    "    current_directory = os.getcwd()\n",
    "    \n",
    "    # Print the current working directory\n",
    "    print(f\"Current working directory: {current_directory}\")\n",
    "    \n",
    "    # Attempt to navigate up one directory and then to 'data/rag_use_cases'\n",
    "    data_directory = os.path.join(os.path.dirname(current_directory), 'data/rag_use_cases')\n",
    "    \n",
    "    # Print the resulting path\n",
    "    print(f\"Data directory path: {data_directory}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    # Handle the case where the directory path does not exist\n",
    "    print(f\"Error: The specified path does not exist - {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # General exception handler for any other errors\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50af1077-3400-45eb-9fa7-492a259b9d8e",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "##### Make Sure that data_directory is pointing to the right path and data files are present. Otherwise, you need to change the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e914ef4e-03c1-488e-af83-2c07d60cfd00",
   "metadata": {},
   "source": [
    "# Prepare dataset \n",
    "\n",
    "##### 1.Parameter Inputs: Accepts data_directory (path to the PDF folder) and documents (list to store PDF data).\n",
    "##### 2.File Loading Loop: Iterates through all files in the directory, checking if each one has a .pdf extension.\n",
    "##### 3.PDF Loading with Error Handling: For each PDF, it uses PyPDFLoader to load the content, appending it to documents. If an error occurs, it prints an error message for that specific file, allowing the process to continue.\n",
    "##### 4.Document Check and Output: If any documents are loaded, it prints the content of the first page from the first document; otherwise, it notifies that no PDFs were found.\n",
    "##### 5.Function Return: Returns the updated documents list with loaded PDF content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5248cf-1677-45fe-94ac-e861d184af0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pdf_documents(data_directory, documents):\n",
    "    \"\"\"\n",
    "    Load PDF documents from the specified directory and append their content to the documents list.\n",
    "\n",
    "    Parameters:\n",
    "    - data_directory (str): The directory path containing the PDF files.\n",
    "    - documents (list): A list to store the loaded PDF data.\n",
    "\n",
    "    Returns:\n",
    "    - list: The updated documents list with the loaded PDF content.\n",
    "    \"\"\"\n",
    "    # Loop through all files in the specified directory\n",
    "    for filename in os.listdir(data_directory):\n",
    "        if filename.endswith('.pdf'):  # Check if the file is a PDF\n",
    "            file_path = os.path.join(data_directory, filename)\n",
    "            try:\n",
    "                # Initialize the PDF loader for the current file\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                \n",
    "                # Load the PDF data\n",
    "                data = loader.load()\n",
    "                \n",
    "                # Extend the documents list with the loaded data\n",
    "                documents.extend(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")  # Handle exceptions during loading\n",
    "\n",
    "    # Display the content of the first page of the first document if available\n",
    "    if documents:\n",
    "        print(documents[0].page_content)  # Printing page content of the first document\n",
    "    else:\n",
    "        print(\"No PDF files found in the folder.\")\n",
    "\n",
    "    return documents\n",
    "\n",
    "# Usage\n",
    "documents = []\n",
    "documents = load_pdf_documents(data_directory, documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb26c0-6ea3-4153-9813-4f0a69aa9242",
   "metadata": {},
   "source": [
    "# Define prompt, Amazon Bedrock Foundation model, and Amazon Bedrock embed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83242d79-e461-4aad-9ca0-9ac928b461d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define prompt\n",
    "prompt = \"What is Amazon doing and cashflow?\"\n",
    "\n",
    "# List of Bedrock models with names and model codes\n",
    "bedrock_model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# List of Bedrock embed models with names and model codes\n",
    "bedrock_embed_model_id = \"amazon.titan-embed-text-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a707d91-23c2-4786-bd2a-6287ca51ef82",
   "metadata": {},
   "source": [
    "## Define important environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4247b5-5fe7-4ec8-9f25-bff782734606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try-except block to handle potential errors\n",
    "try:\n",
    "    # Create a new Boto3 session to interact with AWS services\n",
    "    # This session is responsible for managing credentials and region configuration\n",
    "    boto3_session = boto3.session.Session()\n",
    "\n",
    "    # Retrieve the current AWS region from the session (e.g., 'us-east-1', 'us-west-2')\n",
    "    aws_region_name = boto3_session.region_name\n",
    "    \n",
    "    # Initialize Bedrock and Bedrock Runtime clients using Boto3\n",
    "    # These clients will allow interactions with Bedrock-related AWS services\n",
    "    boto3_bedrock_client = boto3.client('bedrock', region_name=aws_region_name)\n",
    "    boto3_bedrock_runtime_client = boto3.client('bedrock-runtime', region_name=aws_region_name)\n",
    "\n",
    "    # Store all relevant variables in a dictionary for easier access and management\n",
    "    variables_store = {\n",
    "        \"aws_region_name\": aws_region_name,                          # AWS region name\n",
    "        \"boto3_bedrock_client\": boto3_bedrock_client,                # Bedrock client instance\n",
    "        \"boto3_bedrock_runtime_client\": boto3_bedrock_runtime_client,  # Bedrock Runtime client instance\n",
    "        \"boto3_session\": boto3_session                               # Current Boto3 session object\n",
    "    }\n",
    "\n",
    "    # Print all stored variables for debugging and verification\n",
    "    for var_name, value in variables_store.items():\n",
    "        print(f\"{var_name}: {value}\")\n",
    "\n",
    "# Handle any exceptions that occur during the execution\n",
    "except Exception as e:\n",
    "    # Print the error message if an unexpected error occurs\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674cb33d-c1f3-4b22-806a-cc13983dcc33",
   "metadata": {},
   "source": [
    "# Section 1\n",
    "\n",
    "## Create Vector Store with Chroma and ingest data\n",
    "\n",
    "#### 1. Document Splitting:\n",
    "###### split_documents function divides large documents into smaller chunks.\n",
    "###### Uses RecursiveCharacterTextSplitter with specified chunk size and overlap.\n",
    "###### Returns the splits or logs an error if splitting fails.\n",
    "\n",
    "#### 2. Vector Store Creation:\n",
    "###### create_vectorstore function initializes an embedding model (BedrockEmbeddings) and creates a vector store (Chroma).\n",
    "###### Stores vector representations of document chunks.\n",
    "###### Returns the vector store or logs an error if it fails.\n",
    "\n",
    "#### 3. Main Execution Function:\n",
    "###### create_ingest_vector_store orchestrates the workflow.\n",
    "###### It first calls split_documents and, upon success, moves to create_vectorstore.\n",
    "###### Raises an exception if any step fails, logging the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c9bdd2-ee28-4a26-afd2-9ba957e421b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to split the documents into chunks\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    try:\n",
    "        # Initialize a recursive character text splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        # Split documents into smaller chunks\n",
    "        splits = text_splitter.split_documents(documents)\n",
    "        print(f\"Number of splits: {len(splits)}\")\n",
    "        return splits\n",
    "    except Exception as e:\n",
    "        # Handle document splitting errors\n",
    "        print(f\"Error while splitting documents: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to create embeddings and store vectors using Chroma\n",
    "def create_vectorstore(splits, bedrock_client, model_id=bedrock_embed_model_id):\n",
    "    try:\n",
    "        # Initialize the Bedrock Embeddings model using the Bedrock client\n",
    "        embeddings_model = BedrockEmbeddings(client=bedrock_client, model_id=model_id)\n",
    "        \n",
    "        # Create a vector store with the splits and embeddings model\n",
    "        vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings_model)\n",
    "        print(\"Vector store created successfully.\")\n",
    "        return vectorstore\n",
    "    except Exception as e:\n",
    "        # Handle errors during the vector store creation\n",
    "        print(f\"Error while creating vector store: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main execution function to modularize the flow\n",
    "def create_ingest_vector_store(documents):\n",
    "    try:\n",
    "\n",
    "        # Step 1: Split documents into chunks\n",
    "        splits = split_documents(documents)\n",
    "        if not splits:\n",
    "            raise Exception(\"Document splitting failed\")\n",
    "\n",
    "        # Step 2: Create a vector store using embeddings\n",
    "        vectorstore = create_vectorstore(splits, boto3_bedrock_runtime_client)\n",
    "        if not vectorstore:\n",
    "            raise Exception(\"Vector store creation failed\")\n",
    "\n",
    "        # If all steps succeed\n",
    "        print(\"Process completed successfully.\")\n",
    "        return vectorstore\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Handle any general errors in the flow\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage\n",
    "vectorstore = create_ingest_vector_store(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b12bed-366f-4b8d-bff4-6a699054587a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test the Vector DB\n",
    "\n",
    "#### 1. Document Retrieval Function:\n",
    "###### retrieve_documents(query, retriever) function takes a query and a retriever to find relevant documents.\n",
    "###### It attempts to retrieve documents and prints the number retrieved.\n",
    "###### If any documents are retrieved, it prints the content of the first one; otherwise, it displays a message indicating no documents were found.\n",
    "###### Errors are caught and printed if retrieval fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ae1d7-4ce9-4d36-bbf2-6e4809873ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to retrieve documents based on a query\n",
    "def retrieve_documents(query, retriever):\n",
    "    try:\n",
    "        # Attempt to retrieve documents based on the query\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        \n",
    "        # Print the number of retrieved documents\n",
    "        print(f\"Number of retrieved documents: {len(retrieved_docs)}\")\n",
    "        \n",
    "        # Print the content of the first retrieved document (if any)\n",
    "        if len(retrieved_docs) > 0:\n",
    "            print(retrieved_docs[0].page_content)\n",
    "        else:\n",
    "            print(\"No documents retrieved.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that arise during retrieval\n",
    "        print(f\"An error occurred while retrieving documents: {e}\")\n",
    "\n",
    "# Example usage of the function\n",
    "try:\n",
    "    # Assume `vectorstore` is your pre-defined vector store, converted to a retriever\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # Retrieve documents for two different queries\n",
    "    retrieve_documents(prompt, retriever)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74552c11-ee44-4037-b4b8-7114ecfe31bc",
   "metadata": {},
   "source": [
    "# Section 2\n",
    "## Prompt enhancement with RAG and generating responses from Amazon Bedrock foundation model\n",
    "\n",
    "#### 1. Prompt Creation (create_prompt):\n",
    "###### Defines a system prompt using ChatPromptTemplate with guidance for concise, three-sentence responses.\n",
    "###### Includes a {context} placeholder for contextual information and {input} for user queries.\n",
    "\n",
    "#### 2. RAG Chain Creation (create_rag_chain):\n",
    "###### Builds a RAG chain by combining a language model with retrieval.\n",
    "###### Creates a question-answering sub-chain (question_answer_chain) and combines it with the retriever to form rag_chain.\n",
    "\n",
    "#### 3. Main Execution Function (generate_responses_with_rag):\n",
    "###### Initializes the language model (ChatBedrock) with a specific model ID and client.\n",
    "###### Creates a prompt and sets up the retriever (assuming it's defined elsewhere).\n",
    "###### Invokes the RAG chain with a sample question and handles exceptions to print any errors during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f0554-8d2d-45c3-9d5d-f8e8bd877938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prompt():\n",
    "    \"\"\"Create the system prompt for the ChatPromptTemplate.\"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are an assistant designed for answering questions. \"\n",
    "        \"Utilize the following context to provide your response. \"\n",
    "        \"If you are unsure of the answer, please indicate that you \"\n",
    "        \"do not know. Limit your response to a maximum of three sentences, \"\n",
    "        \"ensuring that your answer is concise.\"\n",
    "        \"\\n\\n\"\n",
    "        \"{context}\"\n",
    "    )\n",
    "    return ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def create_rag_chain(llm, prompt, retriever):\n",
    "    \"\"\"Create the Retrieval-Augmented Generation (RAG) chain.\"\"\"\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "    return rag_chain\n",
    "\n",
    "def generate_responses_with_rag():\n",
    "    \"\"\"Main function to run the RAG workflow.\"\"\"\n",
    "\n",
    "    # Initialize the language model with the Bedrock client\n",
    "    llm = ChatBedrock(model_id=bedrock_model_id,\n",
    "                      client=boto3_bedrock_runtime_client)\n",
    "\n",
    "    # Create the prompt for the question-answering task\n",
    "    gen_prompt = create_prompt()\n",
    "\n",
    "    # Initialize the retriever (assumed to be defined elsewhere)\n",
    "    # retriever = AmazonKnowledgeBasesRetriever()  # Modify as needed\n",
    "\n",
    "    # Create the RAG chain\n",
    "    rag_chain = create_rag_chain(llm, gen_prompt, retriever)\n",
    "\n",
    "    # Invoke the RAG chain with a sample question\n",
    "    try:\n",
    "        results = rag_chain.invoke({\"input\": prompt})\n",
    "        print(results)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the RAG chain invocation: {e}\")\n",
    "\n",
    "generate_responses_with_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3e764b-6e86-4480-9445-3b21c9dc910a",
   "metadata": {},
   "source": [
    "# You shouod try to generate responses without RAG and compare the response with RAG result. \n",
    "\n",
    "# Generating responses from Amazon Bedrock foundation model without RAG\n",
    "\n",
    "### 1. Language Model Initialization:\n",
    "##### The llm is initialized with ChatBedrock, using bedrock_model_id and boto3_bedrock_runtime_client.\n",
    "\n",
    "### 2. System Prompt Definition:\n",
    "##### system_prompt is a concise instruction set for the assistant, enforcing brevity (three sentences max) and advising it to state if it doesn’t know the answer.\n",
    "\n",
    "### 3. Response Generation Function (generate_response):\n",
    "##### Combines system_prompt and the user’s question (question) to form a complete input.\n",
    "##### Uses llm.invoke(input_text) to obtain a response from the Bedrock model.\n",
    "##### Returns the model’s response content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d121d-0be6-4dfe-b181-362b0dfcaf5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "llm = ChatBedrock(\n",
    "    model_id=bedrock_model_id,\n",
    "    client=boto3_bedrock_runtime_client\n",
    ")\n",
    "\n",
    "# Define the system prompt\n",
    "system_prompt = (\n",
    "    \"You are an assistant designed for answering questions. \"\n",
    "    \"If you are unsure of the answer, please indicate that you \"\n",
    "    \"do not know. Limit your response to a maximum of three sentences, \"\n",
    "    \"ensuring that your answer is concise.\"\n",
    ")\n",
    "\n",
    "# Function to generate a response to a question\n",
    "def generate_response(question):\n",
    "    # Combine the system prompt and the user question\n",
    "    input_text = f\"{system_prompt}\\n\\n{question}\"\n",
    "    \n",
    "    # Invoke the Bedrock model with the input text as a string\n",
    "    response = llm.invoke(input_text)\n",
    "    \n",
    "    # Extract the content of the response\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "response = generate_response(prompt)\n",
    "\n",
    "# Print the result\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c73b6e-3769-4686-8cdd-972999137344",
   "metadata": {},
   "source": [
    "# Must Read\n",
    "\n",
    "######  Please have a look of the result of both the cases. Case 1 is generating more context aware information compare to Case 2.\n",
    "\n",
    "###### Case1: Prompt enhancement with RAG and generating responses from Amazon Bedrock foundation model\n",
    "###### Case2: Generating responses from Amazon Bedrock foundation model without RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51745d43-360d-42a1-8617-a7a3a2769ab8",
   "metadata": {},
   "source": [
    "# End of NoteBook \n",
    "\n",
    "## Please ensure that you close the kernel after using this notebook to avoid any potential charges to your account.\n",
    "\n",
    "## Process: Go to \"Kernel\" at top option. Choose \"Shut Down Kernel\". \n",
    "##### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/studio-ui.html"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
