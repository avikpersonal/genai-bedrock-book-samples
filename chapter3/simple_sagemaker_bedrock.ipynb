{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed3a60a-5001-46ff-ae70-61d861a00c53",
   "metadata": {
    "tags": []
   },
   "source": [
    "# File Name: simple_sagemaker_bedrock.ipynb\n",
    "### Location: Chapter 3\n",
    "### Purpose: \n",
    "#####             1. Understanding Amazon Bedrock client and Amazon Bedrock runtime client.\n",
    "#####             2. Understanding of list_foundation_models API.\n",
    "#####             3. Example of Amazon Titan LLM foundation model with and without parameters.\n",
    "#####             4. Example of Anthropic LLM foundation model with and without parameters.\n",
    "#####             5. Example of Amazon Titan Image foundation model with and without parameters.\n",
    "#####             6. Example of Amazon Titan LLM foundation model with streaming API with and with out parameters.\n",
    "##### Dependency: Not Applicable\n",
    "# <ins>-----------------------------------------------------------------------------------</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99464e3-0679-4a67-bb38-8d96892eefa5",
   "metadata": {},
   "source": [
    "# <ins>Amazon SageMaker Classic</ins>\n",
    "#### Those who are new to Amazon SageMaker Classic. Follow the link for the details. https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91003ca1-0026-46eb-bd09-81861135cb22",
   "metadata": {},
   "source": [
    "# <ins>Environment setup of Kernel</ins>\n",
    "##### Fill \"Image\" as \"Data Science\"\n",
    "##### Fill \"Kernel\" as \"Python 3\"\n",
    "##### Fill \"Instance type\" as \"ml-t3-medium\"\n",
    "##### Fill \"Start-up script\" as \"No Scripts\"\n",
    "##### Click \"Select\"\n",
    "\n",
    "###### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-create-open.html for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865b7f5-755a-498c-bff6-7647281b15b9",
   "metadata": {},
   "source": [
    "# <ins>Mandatory installation on the kernel through pip</ins>\n",
    "\n",
    "##### This lab will work with below software version. But, if you are trying with latest version of boto3, awscli, and botocore. This code may fail. You might need to change the corresponding api. \n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade0112-a4be-4ce8-be92-8cbf28b2acfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall -q \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e3b85-4e64-42ef-9355-ed2a61c68b23",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins>\n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ee934-e99c-49ce-9e94-27ffc5baabe1",
   "metadata": {},
   "source": [
    "# <ins>Restart the kernel</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf41bf-d2d2-46fa-b1d4-1dd19c45a069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5af76d-b2a9-4740-b8bf-134932804719",
   "metadata": {},
   "source": [
    "# <ins>Python package import</ins>\n",
    "\n",
    "##### boto3 offers various clients for Amazon Bedrock to execute various actions.\n",
    "##### botocore is a low-level interface to AWS tools, while boto3 is built on top of botocore and provides additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7268959-6ef0-4cbd-bd53-3b31369b4513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import botocore\n",
    "import boto3\n",
    "import warnings\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output, display, display_markdown, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1b877-deab-4961-a249-7663e812fbb8",
   "metadata": {},
   "source": [
    "### Ignore warning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71587271-9f02-46dc-9eaf-5a5291132f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac02fbe-2513-4898-992a-8a5b29ce4ee3",
   "metadata": {},
   "source": [
    "# <ins>Amazon Bedrock Runtime Client</ins>\n",
    "\n",
    "##### Purpose: used for making inference requests for models hosted in Amazon Bedrock. \n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Amazon_Bedrock_Runtime.html for details about Amazon Bedrock runtime client "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b83ea1-9c4e-4ad2-9f83-4dfb0721a166",
   "metadata": {},
   "source": [
    "## Define important environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362bc059-93ce-4812-851e-8e359fb4026b",
   "metadata": {},
   "source": [
    "# <ins>Amazon Bedrock Client</ins>\n",
    "\n",
    "##### Purpose: used for managing, training, and deploying models on Amazon Bedrock\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Amazon_Bedrock.html for details about Amazon Bedrock client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf36a8-87e3-462a-9a13-4a286861c327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try-except block to handle potential errors\n",
    "try:\n",
    "    # Create a new Boto3 session to interact with AWS services\n",
    "    boto3_session_name = boto3.session.Session()\n",
    "\n",
    "    # Retrieve the current AWS region from the session\n",
    "    aws_region_name = boto3_session_name.region_name\n",
    "    \n",
    "    # Create a new Boto3 bedrock client to interact with AWS services\n",
    "    boto3_bedrock_client = boto3.client('bedrock')\n",
    "    \n",
    "    # Create a new Boto3 bedrock runtime client to interact with AWS services\n",
    "    boto3_bedrock_runtime_client = boto3.client('bedrock-runtime', region_name = aws_region_name,)\n",
    "    \n",
    "    # Generate a random suffix number between 200 and 900\n",
    "    random_suffix = random.randrange(200, 900)\n",
    "    \n",
    "    # Store all variables in a dictionary\n",
    "    variables_store = {\n",
    "        \"boto3_session_name\": boto3_session_name,\n",
    "        \"aws_region_name\": aws_region_name,\n",
    "        \"boto3_bedrock_client\": boto3_bedrock_client,\n",
    "        \"random_suffix\": random_suffix,\n",
    "        \"boto3_bedrock_runtime_client\": boto3_bedrock_runtime_client\n",
    "    }\n",
    "\n",
    "    # Print all variables\n",
    "    for var_name, value in variables_store.items():\n",
    "        print(f\"{var_name}: {value}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04c796-eb62-4883-bb33-17c8301f7cdb",
   "metadata": {},
   "source": [
    "# <ins>Amazon Bedrock Runtime Client</ins>\n",
    "\n",
    "##### Purpose: used for making inference requests for models hosted in Amazon Bedrock. \n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Amazon_Bedrock_Runtime.html for details about Amazon Bedrock runtime client "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da9c2a-8a37-4ccd-be88-daf2399844f9",
   "metadata": {},
   "source": [
    "# <ins>Find out list of foundation models on Amazon Bedrock</ins>\n",
    "\n",
    "##### API name: list_foundation_models \n",
    "##### Documentation: https://docs.aws.amazon.com/bedrock/latest/APIReference/API_ListFoundationModels.html\n",
    "##### You can find out both foundation and customized model on Amazon Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91d6b96-d525-423f-911d-5c0740f2da4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Retrieve the list of foundation models available in Bedrock\n",
    "    model_summaries = boto3_bedrock_client.list_foundation_models().get('modelSummaries', [])\n",
    "    \n",
    "    # Check if model summaries are available and print them\n",
    "    if model_summaries:\n",
    "        print(\"Available Foundation Models:\")\n",
    "        for model in model_summaries:\n",
    "            print(f\"Model ID: {model.get('modelId')}, Model Name: {model.get('modelName')}\")\n",
    "    else:\n",
    "        print(\"No foundation models found.\")\n",
    "    \n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Handle errors if there is an issue with the request\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    print(f\"An error occurred: {error_code}\")\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(\"Access Denied: Please check your permissions for Bedrock services.\")\n",
    "    else:\n",
    "        print(\"An unexpected error occurred.\")\n",
    "        raise  # Reraise other exceptions for further debugging if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e728f5cd-0a3d-4488-8cb6-07e6cd5fd54e",
   "metadata": {},
   "source": [
    "# <ins>Example of Amazon Titan LLM foundation model</ins>\n",
    "\n",
    "##### This example is based on Titan Text G1 - Express v1 foundation model. \n",
    "##### Model ID: amazon.titan-text-express-v1\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-text.html\n",
    "\n",
    "##### API: invoke_model\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d61e1-fea0-43a0-ae8a-32601eceea39",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins> \n",
    "\n",
    "##### Make sure that amazon.titan-text-express-v1 is allowlisted on Amazon Bedrock model access. Refer Section 3.3 of Chapter 3 of the Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd78bc8-cd55-4638-8419-0fc5fda1621f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining model_id, prompt and other variables\n",
    "## You can try out different model id, your own prompt. \n",
    "model_id = \"amazon.titan-text-express-v1\"\n",
    "prompt = \"\"\"User: Generate a story for a kid about beauty of a rainbow within 100 words\n",
    "bot:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f7e2be-92a2-450a-b934-bd805dcc821d",
   "metadata": {},
   "source": [
    "##### <ins>Example with default inferance parameters</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8943f32-97bf-4bcb-83fd-9806b2be1011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Prepare the input payload for the model invocation\n",
    "    body = json.dumps({\"inputText\": prompt})\n",
    "    modelId = model_id\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    # Invoke the model with the specified parameters using boto3 client\n",
    "    response = boto3_bedrock_runtime_client.invoke_model(\n",
    "        body=body,\n",
    "        modelId=modelId,\n",
    "        accept=accept,\n",
    "        contentType=contentType\n",
    "    )\n",
    "    \n",
    "    # Parse the response body to extract the model's output\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    output_text = response_body.get(\"results\", [{}])[0].get(\"outputText\", \"No output text available\")\n",
    "\n",
    "    # Display the output text from the model\n",
    "    print(\"Model Output:\", output_text)\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Handle client errors and identify specific error types\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    \n",
    "    # Provide a specific message if access is denied\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\\x1b[0m\")\n",
    "    else:\n",
    "        # Print a generic error message for other issues\n",
    "        print(f\"An error occurred: {error}\")\n",
    "        raise  # Re-raise the exception for further handling if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8bab9-2b05-4747-b1d9-510c2fdede75",
   "metadata": {},
   "source": [
    "##### <ins>Example with  inferance parameters configuration</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef8536b-102a-4873-aa44-740669fd6c64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Construct the request payload with prompt and generation configuration\n",
    "    body = json.dumps({\n",
    "        \"inputText\": prompt,\n",
    "        \"textGenerationConfig\": {\n",
    "            \"topP\": 0.95,          # Controls the nucleus sampling probability (diversity of output)\n",
    "            \"temperature\": 0.2     # Controls the creativity of the model's response\n",
    "        }\n",
    "    })\n",
    "\n",
    "    # Define model and content parameters\n",
    "    modelId = model_id\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    # Invoke the model with the boto3 Bedrock client\n",
    "    response = boto3_bedrock_runtime_client.invoke_model(\n",
    "        body=body,\n",
    "        modelId=modelId,\n",
    "        accept=accept,\n",
    "        contentType=contentType\n",
    "    )\n",
    "    \n",
    "    # Parse the response body to retrieve the model output\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    output_text = response_body.get(\"results\", [{}])[0].get(\"outputText\", \"No output text available\")\n",
    "\n",
    "    # Print the generated output text\n",
    "    print(\"Model Output:\", output_text)\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Handle errors related to the ClientError exception\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    \n",
    "    # Specific handling for Access Denied errors with a styled output\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\\x1b[0m\")\n",
    "    else:\n",
    "        # Print a generic message for other ClientErrors and re-raise the error\n",
    "        print(f\"An error occurred: {error}\")\n",
    "        raise  # Reraise to allow further handling or debugging if necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb17e0-0bb3-45c5-9fd7-53bf0f9813cd",
   "metadata": {},
   "source": [
    "# <ins>Example of Anthropic LLM foundation model</ins>\n",
    "\n",
    "##### This example is based on Claude 3 Haiku foundation model. \n",
    "##### Model ID: anthropic.claude-3-haiku-20240307-v1:0\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html\n",
    "\n",
    "##### API: invoke_model\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fc9c3c-db0c-4b93-a14f-4bc504747674",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins> \n",
    "\n",
    "##### Make sure that anthropic.claude-3-haiku-20240307-v1:0 is allowlisted on Amazon Bedrock model access. Refer Section 3.3 of Chapter 3 of the Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab0db0-aa7f-4d22-8ac1-b601b3878489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining model_id, prompt and other variables\n",
    "## You can try out different model id, your own prompt. \n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "prompt = \"\"\"Human: Generate a story for a kid about beauty of a rainbow within 100 words\n",
    "\n",
    "Assitance:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7519271f-b78b-4353-a90c-57782eb6e52a",
   "metadata": {},
   "source": [
    "##### <ins>Example with default inferance parameters</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd9c19f-2d7d-41ab-b45f-31ab611171a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Construct the request payload with message info and model configuration\n",
    "    messages_info = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1000,  # Limit the response tokens to control output length\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt  # User-provided prompt for the model\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Convert the payload to JSON format\n",
    "    body = json.dumps(messages_info)\n",
    "    \n",
    "    # Define model parameters and content type\n",
    "    modelId = model_id\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    # Invoke the model using the boto3 Bedrock runtime client\n",
    "    response = boto3_bedrock_runtime_client.invoke_model(\n",
    "        body=body,\n",
    "        modelId=model_id,\n",
    "        accept=accept,\n",
    "        contentType=contentType\n",
    "    )\n",
    "\n",
    "    # Parse the response body to extract the model's response content\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    output_text = response_body.get(\"content\", [{}])\n",
    "\n",
    "    # Print the first part of the text response if available\n",
    "    if output_text and isinstance(output_text, list) and 'text' in output_text[0]:\n",
    "        print(\"Model Output:\", output_text[0]['text'])\n",
    "    else:\n",
    "        print(\"No valid text response received from the model.\")\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Handle exceptions specifically for ClientError\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    \n",
    "    # Specific handling for Access Denied errors\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\\x1b[0m\")\n",
    "    else:\n",
    "        # Print a generic message for other ClientErrors\n",
    "        print(f\"An error occurred: {error}\")\n",
    "        raise  # Reraise the error for further handling or debugging if necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1ac6cd-bd39-4e52-9af2-fb2ad6998531",
   "metadata": {},
   "source": [
    "##### <ins>Example with  inferance parameters configuration</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e0b46-2ad3-4c0f-8400-6ec2b78f9f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import botocore.exceptions\n",
    "\n",
    "try:\n",
    "    # Define the message payload with model configuration for generation\n",
    "    messages_info = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1000,         # Maximum number of tokens in the response\n",
    "        \"temperature\": 0.9,         # Controls response creativity; higher values increase randomness\n",
    "        \"top_p\": 0.8,               # Nucleus sampling probability for diverse responses\n",
    "        \"top_k\": 20,                # Limits the number of words to sample from at each step\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt  # User-provided prompt for the model\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Convert payload to JSON format for model invocation\n",
    "    body = json.dumps(messages_info)\n",
    "    \n",
    "    # Define model parameters and headers\n",
    "    modelId = model_id\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    # Call the Bedrock model using the boto3 runtime client\n",
    "    response = boto3_bedrock_runtime_client.invoke_model(\n",
    "        body=body,\n",
    "        modelId=model_id,\n",
    "        accept=accept,\n",
    "        contentType=contentType\n",
    "    )\n",
    "\n",
    "    # Parse the JSON response to retrieve the model output\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    output_text = response_body.get(\"content\", [{}])\n",
    "\n",
    "    # Print the response text if it's available and valid\n",
    "    if output_text and isinstance(output_text, list) and 'text' in output_text[0]:\n",
    "        print(\"Model Output:\", output_text[0]['text'])\n",
    "    else:\n",
    "        print(\"No valid text response received from the model.\")\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Error handling for ClientError, checking the specific error code\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    \n",
    "    # Specific handling if access is denied, with styled error output\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\\x1b[0m\")\n",
    "    else:\n",
    "        # Generic error message for other ClientErrors\n",
    "        print(f\"An error occurred: {error}\")\n",
    "        raise  # Re-raise the error for further handling if necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a66143e-c4e3-45a3-a350-c5a1cfcf6348",
   "metadata": {},
   "source": [
    "# <ins>Example of Amazon Titan Image foundation model</ins>\n",
    "\n",
    "##### This example is based on Amazon Titan image generator version 2 foundation model. \n",
    "##### Model ID: amazon.titan-image-generator-v2:0\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-image.html\n",
    "\n",
    "##### API: invoke_model\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe250909-fb5f-4c1e-b3bf-bc4b84a89498",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins> \n",
    "\n",
    "##### Make sure that amazon.titan-image-generator-v2:0 is allowlisted on Amazon Bedrock model access. Refer Section 3.3 of Chapter 3 of the Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f32404c-0b51-4948-aeaa-d1392717dc0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining model_id, prompt and other variables\n",
    "## You can try out different model id, your own prompt. \n",
    "model_id = \"amazon.titan-image-generator-v2:0\"\n",
    "prompt = \"\"\"An image of a cat with a big hat at a park\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e21f82-125c-422f-9698-33aa375bf8b7",
   "metadata": {},
   "source": [
    "##### <ins>Example with default inferance parameters</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a419ae-9d2f-4205-9263-a13320b1dde5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Define the request payload for text-to-image generation\n",
    "    body = json.dumps({\n",
    "        \"taskType\": \"TEXT_IMAGE\",            # Task type indicating text-to-image generation\n",
    "        \"textToImageParams\": {\n",
    "            \"text\": prompt                   # User-provided prompt for image generation\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # Define model parameters and headers\n",
    "    modelId = model_id\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    # Invoke the model using the boto3 Bedrock runtime client\n",
    "    response = boto3_bedrock_runtime_client.invoke_model(\n",
    "        body=body,\n",
    "        modelId=model_id,\n",
    "        accept=accept,\n",
    "        contentType=contentType\n",
    "    )\n",
    "\n",
    "    # Parse the JSON response to retrieve model's response content\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    # Check if the response contains an image or relevant information\n",
    "    image_data = response_body.get(\"image\", None)\n",
    "    if image_data:\n",
    "        print(\"Generated Image Data:\", image_data)\n",
    "    else:\n",
    "        print(\"No image data received in the response.\")\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Error handling for ClientError, with a check for access denial\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    \n",
    "    if error_code == 'AccessDeniedException':\n",
    "        # Highlight access denied error with a colored message\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\\x1b[0m\")\n",
    "    else:\n",
    "        # Generic error message for other ClientErrors\n",
    "        print(f\"An error occurred: {error}\")\n",
    "        raise  # Re-raise the error for further handling if necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b0c55-c669-47eb-ab59-5ab05159bcd0",
   "metadata": {},
   "source": [
    "#### <ins>The output of invoke model is a base64 encoded string of the image data. You need to convert this with below code to see the image.</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9cc9f1-99d7-407f-a438-2785ff70e64b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get the base64 encoded image from the response body\n",
    "    base64_image = response_body.get(\"images\")[0]\n",
    "    \n",
    "    if base64_image:\n",
    "        # Decode the base64 string into bytes\n",
    "        base64_bytes = base64_image.encode('ascii')\n",
    "        image_bytes = base64.b64decode(base64_bytes)\n",
    "        \n",
    "        # Open the image using PIL\n",
    "        image = Image.open(io.BytesIO(image_bytes))\n",
    "        \n",
    "        # Display the image\n",
    "        display(image)\n",
    "    else:\n",
    "        print(\"No image data found in the response.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while processing the image: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10245bce-66de-4fee-9b96-6f6853a4cbcd",
   "metadata": {},
   "source": [
    "##### <ins>Example with  inferance parameters configuration</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a85030-5e73-4edb-b416-a589811f2637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import botocore.exceptions\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# Define the number of images you want to generate\n",
    "numberOfImages = 3\n",
    "\n",
    "try:\n",
    "    # Prepare the request payload for text-to-image generation\n",
    "    body = json.dumps({\n",
    "        \"taskType\": \"TEXT_IMAGE\",  # Task type indicating text-to-image generation\n",
    "        \"textToImageParams\": {\n",
    "            \"text\": prompt  # User-provided prompt for image generation\n",
    "        },\n",
    "        \"imageGenerationConfig\": {\n",
    "            \"numberOfImages\": numberOfImages,  # Specify number of images to generate\n",
    "            \"height\": 1024,  # Height of the generated images\n",
    "            \"width\": 1024,   # Width of the generated images\n",
    "            \"cfgScale\": 8.0,  # CFG Scale for image generation\n",
    "            \"seed\": 0         # Seed value for randomization\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # Set headers and model ID\n",
    "    modelId = model_id\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    # Call the model to generate images\n",
    "    response = boto3_bedrock_runtime_client.invoke_model(\n",
    "        body=body, modelId=model_id, accept=accept, contentType=contentType\n",
    "    )\n",
    "\n",
    "    # Parse the JSON response\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    # Check if the response contains images\n",
    "    images = response_body.get(\"images\", [])\n",
    "    if images:\n",
    "        for idx, base64_image in enumerate(images[:numberOfImages]):\n",
    "            # Decode the base64 string to bytes\n",
    "            base64_bytes = base64_image.encode('ascii')\n",
    "            image_bytes = base64.b64decode(base64_bytes)\n",
    "            \n",
    "            # Open and display the image using PIL\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            print(f\"Displaying Image {idx + 1}\")\n",
    "            display(image)\n",
    "    else:\n",
    "        print(\"No image data found in the response.\")\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Handle client errors, specifically Access Denied\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\\x1b[0m\")\n",
    "    else:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "        raise  # Re-raise the error for further handling\n",
    "except Exception as e:\n",
    "    # Handle any other exceptions\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cae03e7-7586-43ab-8884-ee3a56804f54",
   "metadata": {},
   "source": [
    "# <ins>Example of Amazon Titan LLM foundation model with streaming API</ins>\n",
    "\n",
    "\n",
    "##### This example is based on Titan Text G1 - Express v1 foundation model. \n",
    "##### Model ID: amazon.titan-text-express-v1\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-text.html\n",
    "\n",
    "##### API: invoke_model_with_response_stream\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a302b99-fa5c-43be-983d-22e67d2fd3c7",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins> \n",
    "\n",
    "##### Make sure that amazon.titan-text-express-v1 is allowlisted on Amazon Bedrock model access. Refer Section 3.3 of Chapter 3 of the Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c5b67-8354-4746-bc2a-f697ee27764a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining model_id, prompt and other variables\n",
    "## You can try out different model id, your own prompt. \n",
    "model_id = \"amazon.titan-text-express-v1\"\n",
    "prompt = \"\"\"User: Generate a story for a kid about beauty of a rainbow within 1000 words\n",
    "bot:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060c611b-aea5-4108-beca-b9dd037d6b5c",
   "metadata": {},
   "source": [
    "##### <ins>Example with default inferance parameters</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc043b1d-0367-4ba2-af4e-7f2647aa2059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Prepare the body for the request\n",
    "    body = json.dumps({\"inputText\": prompt})\n",
    "    modelId = model_id\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    # Call the invoke_model_with_response_stream API to get the response\n",
    "    response = boto3_bedrock_runtime_client.invoke_model_with_response_stream(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    \n",
    "    # Get the response body stream\n",
    "    stream = response.get('body')\n",
    "\n",
    "    if stream:\n",
    "        # Process each event in the stream\n",
    "        for event in stream:\n",
    "            chunk = event.get('chunk')\n",
    "            if chunk:\n",
    "                # Decode and load the JSON chunk\n",
    "                chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "                \n",
    "                # Extract the output text from the chunk\n",
    "                text = chunk_obj.get('outputText')\n",
    "                if text:\n",
    "                    # Clear the previous output to avoid cluttering\n",
    "                    clear_output(wait=True)\n",
    "                    \n",
    "                    # Display the output text in markdown format\n",
    "                    display_markdown(Markdown(text))\n",
    "                else:\n",
    "                    print(\"No output text found in this chunk.\")\n",
    "    else:\n",
    "        print(\"No data stream available.\")\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Handle AWS client errors\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\")\n",
    "    else:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814b4e1a-c9df-470f-842d-3e9a82306f40",
   "metadata": {},
   "source": [
    "##### <ins>Example with  inferance parameters configuration</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed73fb0-1026-406a-841f-1dff2886c674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Prepare the request body with the desired configurations\n",
    "    body = json.dumps({\n",
    "        \"inputText\": prompt,\n",
    "        \"textGenerationConfig\": {\n",
    "            \"topP\": 0.95,\n",
    "            \"temperature\": 0.2\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    modelId = model_id\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    # Call the invoke_model_with_response_stream API to get the response stream\n",
    "    response = boto3_bedrock_runtime_client.invoke_model_with_response_stream(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    \n",
    "    # Get the response body stream\n",
    "    stream = response.get('body')\n",
    "\n",
    "    if stream:\n",
    "        # Process each event in the stream\n",
    "        for event in stream:\n",
    "            chunk = event.get('chunk')\n",
    "            if chunk:\n",
    "                # Decode and load the JSON chunk\n",
    "                chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "                \n",
    "                # Extract and print the output text\n",
    "                text = chunk_obj.get('outputText')\n",
    "                if text:\n",
    "                    clear_output(wait=True)  # Clear the previous output\n",
    "                    display_markdown(Markdown(text))  # Display the output in markdown format\n",
    "    else:\n",
    "        print(\"No data stream available.\")\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Handle AWS client errors\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\")\n",
    "    else:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51745d43-360d-42a1-8617-a7a3a2769ab8",
   "metadata": {},
   "source": [
    "# End of NoteBook \n",
    "\n",
    "## Please ensure that you close the kernel after using this notebook to avoid any potential charges to your account.\n",
    "\n",
    "## Process: Go to \"Kernel\" at top option. Choose \"Shut Down Kernel\". \n",
    "##### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/studio-ui.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d46a47-81e8-4ad7-b3c9-65a931230b86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
