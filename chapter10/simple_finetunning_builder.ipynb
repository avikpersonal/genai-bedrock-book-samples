{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed3a60a-5001-46ff-ae70-61d861a00c53",
   "metadata": {
    "tags": []
   },
   "source": [
    "# File Name: simple_finetunning_builder.ipynb\n",
    "### Location: Chapter 9\n",
    "### Purpose: \n",
    "#####             1. Need to fill up \n",
    "##### Dependency: simple-sageMaker-bedrock.ipynb at Chapter 3 should work properly. \n",
    "# <ins>-----------------------------------------------------------------------------------</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99464e3-0679-4a67-bb38-8d96892eefa5",
   "metadata": {},
   "source": [
    "# <ins>Amazon SageMaker Classic</ins>\n",
    "#### Those who are new to Amazon SageMaker Classic. Follow the link for the details. https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91003ca1-0026-46eb-bd09-81861135cb22",
   "metadata": {},
   "source": [
    "# <ins>Environment setup of Kernel</ins>\n",
    "##### Fill \"Image\" as \"Data Science\"\n",
    "##### Fill \"Kernel\" as \"Python 3\"\n",
    "##### Fill \"Instance type\" as \"ml-t3-medium\"\n",
    "##### Fill \"Start-up script\" as \"No Scripts\"\n",
    "##### Click \"Select\"\n",
    "\n",
    "###### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-create-open.html for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865b7f5-755a-498c-bff6-7647281b15b9",
   "metadata": {},
   "source": [
    "# <ins>Mandatory installation on the kernel through pip</ins>\n",
    "\n",
    "##### This lab will work with below software version. But, if you are trying with latest version of boto3, awscli, and botocore. This code may fail. You might need to change the corresponding api. \n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ade0112-a4be-4ce8-be92-8cbf28b2acfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (24.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.215.0 requires attrs<24,>=23.1.0, but you have attrs 24.2.0 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "sparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.6.0 which is incompatible.\n",
      "sphinx 7.2.6 requires docutils<0.21,>=0.18.1, but you have docutils 0.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 1.41 s, sys: 258 ms, total: 1.66 s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install --no-build-isolation --force-reinstall -q \\\n",
    "    \"boto3>=1.34.84\" \\\n",
    "    \"awscli>=1.32.84\" \\\n",
    "    \"botocore>=1.34.84\" \\\n",
    "    \"langchain\" \\\n",
    "    \"typing_extensions\" \\\n",
    "    \"pypdf\" \\\n",
    "    \"urllib3\" \\\n",
    "    \"jsonlines\" \\\n",
    "    \"datasets\" \\\n",
    "    \"pandas\" \\\n",
    "    \"matplotlib\" \\\n",
    "    \"ipywidgets>=7,<8\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e3b85-4e64-42ef-9355-ed2a61c68b23",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins>\n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ee934-e99c-49ce-9e94-27ffc5baabe1",
   "metadata": {},
   "source": [
    "# <ins>Restart the kernel</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aaf41bf-d2d2-46fa-b1d4-1dd19c45a069",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5af76d-b2a9-4740-b8bf-134932804719",
   "metadata": {},
   "source": [
    "# <ins>Python package import</ins>\n",
    "\n",
    "##### boto3 offers various clients for Amazon Bedrock to execute various actions.\n",
    "##### botocore is a low-level interface to AWS tools, while boto3 is built on top of botocore and provides additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7268959-6ef0-4cbd-bd53-3b31369b4513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3 \n",
    "import time\n",
    "import pprint\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import jsonlines\n",
    "import botocore\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1b877-deab-4961-a249-7663e812fbb8",
   "metadata": {},
   "source": [
    "### Ignore warning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71587271-9f02-46dc-9eaf-5a5291132f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b83ea1-9c4e-4ad2-9f83-4dfb0721a166",
   "metadata": {},
   "source": [
    "## Define important environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ccf36a8-87e3-462a-9a13-4a286861c327",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3_session_name: Session(region_name='us-east-1')\n",
      "aws_region_name: us-east-1\n",
      "boto3_bedrock_client: <botocore.client.Bedrock object at 0x7f6b8f84dc00>\n",
      "random_suffix: 406\n",
      "boto3_bedrock_runtime_client: <botocore.client.BedrockRuntime object at 0x7f6b8f84fa60>\n",
      "s3_suffix: us-east-1-027437746815\n",
      "s3_bucket_name: bedrock-kb-us-east-1-027437746815-406\n",
      "sts_client: <botocore.client.STS object at 0x7f6b8f73da50>\n",
      "aws_account_id: 027437746815\n",
      "iam_client: <botocore.client.IAM object at 0x7f6b8f5d11b0>\n",
      "s3_client: <botocore.client.S3 object at 0x7f6b8f44ca30>\n"
     ]
    }
   ],
   "source": [
    "# Try-except block to handle potential errors\n",
    "try:\n",
    "    # Create a new Boto3 session to interact with AWS services\n",
    "    boto3_session_name = boto3.session.Session()\n",
    "\n",
    "    # Retrieve the current AWS region from the session\n",
    "    aws_region_name = boto3_session_name.region_name\n",
    "    \n",
    "    # Create a new Boto3 bedrock client to interact with AWS services\n",
    "    boto3_bedrock_client = boto3.client('bedrock')\n",
    "    \n",
    "    # Create a new Boto3 bedrock runtime client to interact with AWS services\n",
    "    boto3_bedrock_runtime_client = boto3.client('bedrock-runtime')\n",
    "    \n",
    "    # Create an STS client to interact with AWS Security Token Service (STS)\n",
    "    sts_client = boto3.client('sts')\n",
    "    \n",
    "    # Create an STS client to interact with AWS Security Token Service (STS)\n",
    "    iam_client = boto3.client('iam')\n",
    "    \n",
    "    # Create an S3 client to interact with Amazon S3\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Get the AWS account ID of the caller\n",
    "    aws_account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "    \n",
    "    # Generate a random suffix number between 200 and 900\n",
    "    random_suffix = random.randrange(200, 900)\n",
    "    \n",
    "    # Generate a suffix using the region and account ID for the S3 bucket name\n",
    "    s3_suffix = f\"{aws_region_name}-{aws_account_id}\"\n",
    "\n",
    "    # Define the name of the S3 bucket (you can replace this with your actual bucket name)\n",
    "    s3_bucket_name = f'bedrock-kb-{s3_suffix}-{random_suffix}'\n",
    "    \n",
    "    # Store all variables in a dictionary\n",
    "    variables_store = {\n",
    "        \"boto3_session_name\": boto3_session_name,\n",
    "        \"aws_region_name\": aws_region_name,\n",
    "        \"boto3_bedrock_client\": boto3_bedrock_client,\n",
    "        \"random_suffix\": random_suffix,\n",
    "        \"boto3_bedrock_runtime_client\": boto3_bedrock_runtime_client,\n",
    "        \"s3_suffix\": s3_suffix,\n",
    "        \"s3_bucket_name\": s3_bucket_name,\n",
    "        \"sts_client\": sts_client,\n",
    "        \"aws_account_id\": aws_account_id,\n",
    "        \"iam_client\":iam_client,\n",
    "        \"s3_client\": s3_client\n",
    "    }\n",
    "\n",
    "    # Print all variables\n",
    "    for var_name, value in variables_store.items():\n",
    "        print(f\"{var_name}: {value}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf9fb8-552a-4719-8ab7-a1c19c1c4342",
   "metadata": {},
   "source": [
    "### ---------------\n",
    "##### The provided code snippet uses the AWS Boto3 library to manage an Amazon S3 bucket for a knowledge base data source. It begins by creating an S3 client and defines a bucket name, s3_bucket_name. A function, check_bucket_exists, checks whether the specified bucket exists by attempting to retrieve its metadata using the head_bucket method. If the bucket exists, a message is printed confirming its existence. If it does not exist (error code '404'), the function returns False. If the bucket is missing, the script proceeds to create it using the create_bucket method, ensuring the data source bucket is always available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3f72d3f-26d7-4bb1-b924-e6a821291b97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'bedrock-kb-us-east-1-027437746815-406' does not exist.\n",
      "Bucket 'bedrock-kb-us-east-1-027437746815-406' created successfully.\n",
      "CPU times: user 23.8 ms, sys: 3.41 ms, total: 27.2 ms\n",
      "Wall time: 200 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Check if s3 bucket exists, and if not create S3 bucket for knowledge base data source\n",
    "\n",
    "# Try-except block to handle potential errors\n",
    "try:\n",
    "\n",
    "    # Define the bucket name (you can replace this with your actual bucket name)\n",
    "    bucket_name = s3_bucket_name\n",
    "\n",
    "    # Check if the bucket exists\n",
    "    def check_bucket_exists(bucket_name):\n",
    "        try:\n",
    "            s3_client.head_bucket(Bucket=bucket_name)\n",
    "            print(f\"Bucket '{bucket_name}' already exists.\")\n",
    "            return True\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            error_code = e.response['Error']['Code']\n",
    "            if error_code == '404':\n",
    "                print(f\"Bucket '{bucket_name}' does not exist.\")\n",
    "                return False\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    # If the bucket doesn't exist, create it\n",
    "    if not check_bucket_exists(bucket_name):\n",
    "        # Create the S3 bucket\n",
    "        s3_client.create_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "\n",
    "except botocore.exceptions.BotoCoreError as boto_error:\n",
    "    print(f\"An error occurred with Boto3: {boto_error}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec480e3-ea1c-4880-8409-f552be5d6584",
   "metadata": {},
   "source": [
    "# Download and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "515479fe-ec8c-479e-a92a-a854ad05499a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /root/chapter9\n",
      "Data directory path: /root/chapter9/data/finetunning\n",
      "CPU times: user 658 μs, sys: 0 ns, total: 658 μs\n",
      "Wall time: 533 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Print the current working directory\n",
    "print(f\"Current working directory: {current_directory}\")\n",
    "\n",
    "# Construct the path to 'data/rag_use_cases' inside the current directory\n",
    "data_directory = os.path.join(current_directory, 'data', 'finetunning')\n",
    "\n",
    "# Print the resulting path\n",
    "print(f\"Data directory path: {data_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7224686a-4498-469e-94de-53cd8ece5202",
   "metadata": {},
   "source": [
    "# Understand dataset \n",
    "\n",
    "### Source of the datasets: https://huggingface.co/datasets/bitext/Bitext-retail-banking-llm-chatbot-training-dataset\n",
    "\n",
    "    Fields of the Dataset:\n",
    "    --------------------------\n",
    "    Each entry in the dataset comprises the following fields:\n",
    "\n",
    "    flags: tags\n",
    "    instruction: a user request from the Retail Banking domain\n",
    "    category: the high-level semantic category for the intent\n",
    "    intent: the specific intent corresponding to the user instruction\n",
    "    response: an example of an expected response from the virtual assistant\n",
    "    \n",
    "    Catagory of the dataset:\n",
    "    ----------------------------\n",
    "    ACCOUNT: check_recent_transactions, close_account, create_account\n",
    "    ATM: dispute_ATM_withdrawal, recover_swallowed_card\n",
    "    CARD: activate_card, activate_card_international_usage, block_card, cancel_card, check_card_annual_fee, check_current_balance_on_card\n",
    "    CONTACT: customer_service, human_agent\n",
    "    FEES: check_fees\n",
    "    FIND: find_ATM, find_branch\n",
    "    LOAN: apply_for_loan, apply_for_mortgage, cancel_loan, cancel_mortgage, check_loan_payments, check_mortgage_payments\n",
    "    PASSWORD: get_password, set_up_password\n",
    "    TRANSFER: cancel_transfer, make_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad854821-624d-4d63-8750-a492d01069d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retail_data_set = load_dataset(\"bitext/Bitext-retail-banking-llm-chatbot-training-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "415a9e15-dfee-454d-b808-727ea563e1a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instruction template\n",
    "instruction_template = '''The following is an instruction detailing a task, accompanied by an input that provides additional context. Compose a response that effectively fulfills the request.\n",
    "\n",
    "flags: {tags}\n",
    "instruction: {instruction}\n",
    "category: {category}\n",
    "intent: {intent}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628441b2-41ac-4fee-98cc-98f895afe37c",
   "metadata": {},
   "source": [
    "### The script processes a dataset, retail_data_set, to create a JSONL file containing formatted prompts and completions based on specific fields (tags, instruction, category, intent, and response). It ensures the output directory exists, dataset access, and file writing, and logs issues such as missing keys or processing failures for individual rows. Each row is converted into a JSON object with a dynamically generated prompt (using a predefined instruction template) and a completion, which are then written line by line to the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7b67ae5-7e3e-4a0f-af57-61b5ff1592de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file has been created at /root/chapter9/data/finetunning/retail_data.jsonl\n",
      "CPU times: user 1.63 s, sys: 41.2 ms, total: 1.67 s\n",
      "Wall time: 2.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define the directory and file path\n",
    "output_file_path = os.path.join(data_directory, \"retail_data.jsonl\")\n",
    "\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "try:\n",
    "    os.makedirs(data_directory, exist_ok=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error creating directory {data_directory}: {e}\")\n",
    "    raise\n",
    "\n",
    "# Assume `retail_data_set` is already loaded as a DatasetDict\n",
    "try:\n",
    "    train_dataset = retail_data_set['train']\n",
    "except KeyError as e:\n",
    "    print(f\"Error accessing the dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Open the file for writing\n",
    "try:\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "        # Loop through each row in the dataset\n",
    "        for index, row in enumerate(train_dataset):\n",
    "            try:\n",
    "                # Use the instruction template to create the prompt\n",
    "                prompt = instruction_template.format(\n",
    "                    tags=row['tags'],\n",
    "                    instruction=row['instruction'],\n",
    "                    category=row['category'],\n",
    "                    intent=row['intent']\n",
    "                )\n",
    "                \n",
    "                # Extract the completion from the 'response' field\n",
    "                completion = row['response']\n",
    "                \n",
    "                # Create the JSON object\n",
    "                json_object = {\n",
    "                    \"prompt\": prompt.strip(),  # Remove extra spaces if any\n",
    "                    \"completion\": completion.strip()  # Remove extra spaces if any\n",
    "                }\n",
    "                \n",
    "                # Write the JSON object to the JSONL file\n",
    "                jsonl_file.write(json.dumps(json_object) + '\\n')\n",
    "            except KeyError as e:\n",
    "                print(f\"Missing key in row {index}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {index}: {e}\")\n",
    "except IOError as e:\n",
    "    print(f\"Error writing to file {output_file_path}: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"JSONL file has been created at {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05924e70-b7c0-4325-afee-4eec795b286d",
   "metadata": {},
   "source": [
    "### The code snippet defines paths for training, validation, and test data files, and reads an original JSONL file into memory. It first ensures there are at least 1500 records in the dataset and raises an error if not. The data is shuffled to ensure randomness, and then split into three sets: 500 records each for training, validation, and testing. A function is defined to write the split data to separate JSONL files. Finally, the paths to the saved datasets are printed.\n",
    "\n",
    "### Disclaimer: You can split entire datasets into 3 parts and perform finetunning. But, here, you are using 500 records to each file to save the computation cost as this is just an example to showcaseing. \n",
    "### Needs to add the limitation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3df0786c-36e7-4bf2-a7e2-625954b10625",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data saved to /root/chapter9/data/finetunning/train_data.jsonl\n",
      "Validation data saved to /root/chapter9/data/finetunning/valid_data.jsonl\n",
      "Testing data saved to /root/chapter9/data/finetunning/test_data.jsonl\n",
      "CPU times: user 228 ms, sys: 95.8 ms, total: 324 ms\n",
      "Wall time: 743 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "# Paths to the original and output files\n",
    "\n",
    "train_file_name = \"train_data.jsonl\"\n",
    "validation_file_name = \"valid_data.jsonl\"\n",
    "test_file_name =  \"test_data.jsonl\"\n",
    "\n",
    "input_file_path = output_file_path\n",
    "train_file_path = os.path.join(data_directory, train_file_name)\n",
    "validation_file_path = os.path.join(data_directory, validation_file_name)\n",
    "test_file_path = os.path.join(data_directory, test_file_name)\n",
    "\n",
    "# Read the original JSONL file\n",
    "try:\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        data = [json.loads(line.strip()) for line in input_file.readlines()]\n",
    "except IOError as e:\n",
    "    print(f\"Error reading the input file {input_file_path}: {e}\")\n",
    "    raise\n",
    "\n",
    "# Ensure we have enough records\n",
    "if len(data) < 1500:\n",
    "    raise ValueError(\"The dataset does not contain enough records. Need at least 1500 records.\")\n",
    "\n",
    "# Shuffle the data to ensure randomness\n",
    "random.shuffle(data)\n",
    "\n",
    "# Split the data into train, validation, and test sets (500 records each)\n",
    "train_data = data[:500]\n",
    "valid_data = data[500:1000]\n",
    "test_data = data[1000:1500]\n",
    "\n",
    "# Function to write data to a JSONL file\n",
    "def write_jsonl(file_path, data):\n",
    "    try:\n",
    "        with open(file_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "            for record in data:\n",
    "                jsonl_file.write(json.dumps(record) + '\\n')\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Write the datasets to their respective files\n",
    "write_jsonl(train_file_path, train_data)\n",
    "write_jsonl(validation_file_path, valid_data)\n",
    "write_jsonl(test_file_path, test_data)\n",
    "\n",
    "print(f\"Training data saved to {train_file_path}\")\n",
    "print(f\"Validation data saved to {validation_file_path}\")\n",
    "print(f\"Testing data saved to {test_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd14e3b1-dd62-415d-b183-afd5a4ad9290",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "##### Make Sure that data_directory is pointing to the right path and data files are present. Otherwise, you need to change the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87370d19-4c49-42c9-8971-454ac5a11cde",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The code defines a function upload_to_s3 that uploads files to an S3 bucket with error handling for common issues such as missing files or upload failures. The function first checks if the file exists locally. If the file is found, it attempts to upload the file to the S3 bucket using s3_client.upload_file.  After uploading, the function returns the S3 URI for the uploaded file. If any file upload fails, the function returns None. The S3 URIs for the uploaded files (training, validation, and test datasets) are printed if all uploads are successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fc63c4d-8692-455f-8ed9-9f323932d82c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded train_data.jsonl to s3://bedrock-kb-us-east-1-027437746815-406/train_data.jsonl\n",
      "Successfully uploaded valid_data.jsonl to s3://bedrock-kb-us-east-1-027437746815-406/valid_data.jsonl\n",
      "Successfully uploaded test_data.jsonl to s3://bedrock-kb-us-east-1-027437746815-406/test_data.jsonl\n",
      "S3 URIs for the datasets:\n",
      "Train URI: s3://bedrock-kb-us-east-1-027437746815-406/train_data.jsonl\n",
      "Validation URI: s3://bedrock-kb-us-east-1-027437746815-406/valid_data.jsonl\n",
      "Test URI: s3://bedrock-kb-us-east-1-027437746815-406/test_data.jsonl\n",
      "CPU times: user 30.6 ms, sys: 10.1 ms, total: 40.7 ms\n",
      "Wall time: 509 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Function to upload files to S3 with error handling\n",
    "def upload_to_s3(file_name, local_path, s3_bucket, s3_key):\n",
    "    try:\n",
    "        # Check if the file exists locally\n",
    "        if not os.path.exists(local_path):\n",
    "            raise FileNotFoundError(f\"File {local_path} not found.\")\n",
    "        \n",
    "        try:\n",
    "            # Attempt to upload file to S3\n",
    "            s3_client.upload_file(local_path, s3_bucket, s3_key)\n",
    "            print(f\"Successfully uploaded {file_name} to s3://{s3_bucket}/{s3_key}\")\n",
    "            return f\"s3://{s3_bucket}/{s3_key}\"\n",
    "        except boto3.exceptions.S3UploadFailedError as e:\n",
    "            print(f\"S3 upload failed for {file_name}: {e}\")\n",
    "        except boto3.exceptions.NoCredentialsError as e:\n",
    "            print(f\"No AWS credentials found for {file_name}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error while uploading {file_name}: {e}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found error for {file_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "    \n",
    "    return None  # Return None if upload failed\n",
    "\n",
    "\n",
    "# Upload the files and store their S3 URIs in the respective variables\n",
    "train_file_path_s3_uri = upload_to_s3(train_file_name, os.path.join(data_directory, train_file_name), s3_bucket_name, train_file_name)\n",
    "validation_file_path_s3_uri = upload_to_s3(validation_file_name, os.path.join(data_directory, validation_file_name), s3_bucket_name, validation_file_name)\n",
    "test_file_path_s3_uri = upload_to_s3(test_file_name, os.path.join(data_directory, test_file_name), s3_bucket_name, test_file_name)\n",
    "\n",
    "# If all files are successfully uploaded, print the S3 URIs\n",
    "if all([train_file_path_s3_uri, validation_file_path_s3_uri, test_file_path_s3_uri]):\n",
    "    print(\"S3 URIs for the datasets:\")\n",
    "    print(f\"Train URI: {train_file_path_s3_uri}\")\n",
    "    print(f\"Validation URI: {validation_file_path_s3_uri}\")\n",
    "    print(f\"Test URI: {test_file_path_s3_uri}\")\n",
    "else:\n",
    "    print(\"One or more files failed to upload.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5552ca2e-a5e3-40b6-90e7-da417c4ea8ac",
   "metadata": {},
   "source": [
    "# Fine out role ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1ba7a26-6622-46d4-b920-3a5b0f429d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Role: book-demo-GenAIBookBedrockSageMakerExecutionRole-ppFsemKcIlvD | ARN: arn:aws:iam::027437746815:role/book-demo-GenAIBookBedrockSageMakerExecutionRole-ppFsemKcIlvD\n",
      "CPU times: user 28 ms, sys: 2.28 ms, total: 30.3 ms\n",
      "Wall time: 94 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Find out IAM role and ARN for this session\n",
    "\n",
    "def find_iam_role_by_name_substring(substring):\n",
    "    try:\n",
    "        # Use list_roles to retrieve IAM roles\n",
    "        response = iam_client.list_roles()\n",
    "\n",
    "        # Filter roles by name that contains the substring\n",
    "        matching_roles = [role for role in response['Roles'] if substring in role['RoleName']]\n",
    "\n",
    "        if matching_roles:\n",
    "            for role in matching_roles:\n",
    "                print(f\"Found Role: {role['RoleName']} | ARN: {role['Arn']}\")\n",
    "                genaibookedbedrocksagemakerexecutionrolearn = role['Arn']\n",
    "        else:\n",
    "            print(f\"No roles found with name containing '{substring}'.\")\n",
    "            \n",
    "        return genaibookedbedrocksagemakerexecutionrolearn\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# Call the function with the desired substring\n",
    "genaibookedbedrocksagemakerexecutionrolearn = find_iam_role_by_name_substring(\"GenAIBookBedrockSageMakerExecutionRole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667bfe07-16a0-4e81-ab40-699db813859c",
   "metadata": {},
   "source": [
    "# Define variable for fine tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29be2ea3-b5fa-431b-8727-8cd847e35870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_llm_foundation_model = \"amazon.titan-text-lite-v1:0:4k\"\n",
    "\n",
    "\n",
    "customization_job_name_suffix = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "bedrock_model_customization_job_name = f\"bedrock-model-finetune-job-{customization_job_name_suffix}\"\n",
    "bedrock_model_customization_model_name = f\"bedrock-finetuned-model-{customization_job_name_suffix}\"\n",
    "# Select the customization type from \"FINE_TUNING\" or \"CONTINUED_PRE_TRAINING\". \n",
    "bedrock_model_customization_type = \"FINE_TUNING\"\n",
    "bedrock_model_provisioned_model_name = f\"bedrock-provisioned-model-{customization_job_name_suffix}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a6898e-cdec-4fbb-ae36-78b2d6dc9b39",
   "metadata": {},
   "source": [
    "### Amazon Titan text model customization hyperparameters:\n",
    "\n",
    "    epochs: The number of complete passes through the training dataset. This parameter can take any integer value between 1 and 10, with a default value of 5.\n",
    "    batchSize: The number of samples processed before updating the model's parameters. It can take any integer value between 1 and 64, with a default value of 1.\n",
    "    learningRate: The rate at which the model's parameters are updated after each batch. This parameter can be any float value between 0.0 and 1.0, with a default value of 1.00E-5.\n",
    "    learningRateWarmupSteps: The number of iterations during which the learning rate is gradually increased to the specified value. This parameter can take any integer value between 0 and 250, with a default value of 5.\n",
    "\n",
    "##### Refer: https://docs.aws.amazon.com/bedrock/latest/userguide/cm-hp-titan-text.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbde0acf-f70f-40d3-b2ac-939df02f3655",
   "metadata": {},
   "source": [
    "#### The code defines the hyperparameters for fine-tuning the Titan text model, including the number of epochs, batch size, and learning rate. It then specifies the S3 URIs for the training, validation (optional), and output data. The code constructs the necessary configurations for the training and validation datasets and the output location. It attempts to create a model customization job using AWS Bedrock's create_model_customization_job function, providing the required parameters such as the customization type, job name, custom model name, and role ARN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1536ae28-d140-4040-aa0b-5069e3d7f80c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customization job created successfully.\n",
      "{'ResponseMetadata': {'RequestId': '65b8af2e-ea75-4e2d-bc9a-0ddf85dd0830', 'HTTPStatusCode': 201, 'HTTPHeaders': {'date': 'Fri, 06 Dec 2024 10:07:58 GMT', 'content-type': 'application/json', 'content-length': '119', 'connection': 'keep-alive', 'x-amzn-requestid': '65b8af2e-ea75-4e2d-bc9a-0ddf85dd0830'}, 'RetryAttempts': 0}, 'jobArn': 'arn:aws:bedrock:us-east-1:027437746815:model-customization-job/amazon.titan-text-lite-v1:0:4k/m5ou1fsp5tlb'}\n",
      "CPU times: user 17.1 ms, sys: 2.16 ms, total: 19.3 ms\n",
      "Wall time: 204 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "# Define the hyperparameters for fine-tuning Titan text model\n",
    "hyper_parameters = {\n",
    "    \"epochCount\": \"2\",\n",
    "    \"batchSize\": \"1\",\n",
    "    \"learningRate\": \"0.00003\",\n",
    "}\n",
    "\n",
    "output_file_path_s3_uri = f's3://{s3_bucket_name}/outputs/output-{bedrock_model_customization_model_name}'\n",
    "\n",
    "# Specify your data path for training, validation (optional), and output\n",
    "training_data_config = {\"s3Uri\": train_file_path_s3_uri}\n",
    "\n",
    "validation_data_config = {\n",
    "    \"validators\": [{\n",
    "        \"s3Uri\": validation_file_path_s3_uri\n",
    "    }]\n",
    "}\n",
    "\n",
    "output_data_config = {\"s3Uri\": output_file_path_s3_uri}\n",
    "\n",
    "# Try to create the customization job\n",
    "try:\n",
    "    # Create the customization job\n",
    "    training_job_response = boto3_bedrock_client.create_model_customization_job(\n",
    "        customizationType=bedrock_model_customization_type,\n",
    "        jobName=bedrock_model_customization_job_name,\n",
    "        customModelName=bedrock_model_customization_model_name,\n",
    "        roleArn=genaibookedbedrocksagemakerexecutionrolearn,\n",
    "        baseModelIdentifier=bedrock_llm_foundation_model,\n",
    "        hyperParameters=hyper_parameters,\n",
    "        trainingDataConfig=training_data_config,\n",
    "        validationDataConfig=validation_data_config,\n",
    "        outputDataConfig=output_data_config\n",
    "    )\n",
    "    \n",
    "    print(\"Customization job created successfully.\")\n",
    "    print(training_job_response)\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"Missing required parameter: {e}\")\n",
    "except boto3.exceptions.Boto3Error as e:\n",
    "    print(f\"Boto3 error occurred: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac785e0-f657-4535-a32a-000497256d7d",
   "metadata": {},
   "source": [
    "#### The code checks the status of a fine-tuning job in a loop until it is no longer \"InProgress.\" It defines a function check_fine_tune_job_status() to fetch the job status. Initially, the job status is checked, and if it is \"InProgress,\" the status is checked every 60 seconds. Once the job status is no longer \"InProgress,\" the job details are retrieved, and the output job name is derived from the job ARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd00e8-abac-4f5c-9c1f-340846df3eeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n",
      "Current fine-tune job status: InProgress\n",
      "Job is still in progress, checking again after 60 seconds...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Function to check the status of the fine-tuning job\n",
    "def check_fine_tune_job_status(job_name):\n",
    "    try:\n",
    "        # Fetch the current status of the fine-tuning job\n",
    "        job_status = boto3_bedrock_client.get_model_customization_job(jobIdentifier=job_name)[\"status\"]\n",
    "        return job_status\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Missing expected data in the response: {e}\")\n",
    "    except boto3.exceptions.Boto3Error as e:\n",
    "        print(f\"Boto3 error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error occurred while fetching job status: {e}\")\n",
    "    return None\n",
    "\n",
    "# Check the initial job status\n",
    "fine_tune_job = check_fine_tune_job_status(bedrock_model_customization_job_name)\n",
    "if fine_tune_job:\n",
    "    print(f\"Initial fine-tune job status: {fine_tune_job}\")\n",
    "\n",
    "    # Loop to check the status every 60 seconds until the job is no longer \"InProgress\"\n",
    "    while fine_tune_job == \"InProgress\":\n",
    "        print(\"Job is still in progress, checking again after 60 seconds...\")\n",
    "        time.sleep(60)  # Wait for 60 seconds before checking the status again\n",
    "        fine_tune_job = check_fine_tune_job_status(bedrock_model_customization_job_name)\n",
    "        if fine_tune_job:\n",
    "            print(f\"Current fine-tune job status: {fine_tune_job}\")\n",
    "    \n",
    "    # Once the job is no longer \"InProgress\", fetch and display the final job details\n",
    "    if fine_tune_job != \"InProgress\":\n",
    "        fine_tune_job_details = boto3_bedrock_client.get_model_customization_job(jobIdentifier=bedrock_model_customization_job_name)\n",
    "        print(fine_tune_job_details)  \n",
    "        output_job_name = \"model-customization-job-\" + fine_tune_job_details['jobArn'].split('/')[-1]\n",
    "        print(f\"Output job name: {output_job_name}\")\n",
    "else:\n",
    "    print(\"Error: Could not retrieve the fine-tuning job status.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21376cc-effb-43c2-a924-060d4292534e",
   "metadata": {},
   "source": [
    "# Test not done this point onwards\n",
    "\n",
    "# Test not done this point onwards\n",
    "\n",
    "\n",
    "# Test not done this point onwards\n",
    "\n",
    "\n",
    "# Test not done this point onwards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210e919-256e-4b86-bae5-83675541dc5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Provisioned throughput is required not only for evaluating the model's performance but also for handling custom model inferences. You specify provisioned throughput in Model Units (MU), where each model unit defines the throughput capacity for a given model. The MU determines the number of input and output tokens the model can process and generate per minute.\n",
    "\n",
    "#### For custom models, provisioned throughput ensures that the model can handle inference requests efficiently, especially when dealing with large volumes of data or high-frequency requests. Without sufficient throughput, the model may experience delays or be unable to process requests within an acceptable timeframe.\n",
    "\n",
    "#### Model unit quotas depend on the level of commitment to provisioned throughput. For custom models with no commitment, you are allocated one model unit per throughput, with a limit of two provisioned throughputs per account. For custom models with commitment, the default quota is 0 model units, and you may request an increase if necessary.\n",
    "\n",
    "#### Provisioned throughput is also required after a customization job is finished, to ensure the fine-tuned model can be used effectively for inference. You can create provisioned throughput either through the AWS console or using the relevant API call. It typically takes around 20-30 minutes to complete the provisioning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85321741-dab0-48f6-a434-e8b6b5facea6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error occurred: An error occurred (ValidationException) when calling the GetCustomModel operation: The provided model identifier is invalid.\n",
      "Unexpected error occurred: An error occurred (ServiceQuotaExceededException) when calling the CreateProvisionedModelThroughput operation: Your account does not currently have any no commitment model units reserved for amazon.titan-text-lite-v1:0:4k. Please see https://support.console.aws.amazon.com/support/home?region=us-east-1#/case/create?issueType=service-limit-increase to request a service quota increase.\n",
      "Unexpected error occurred while checking provisioning status: name 'provisioned_model_id_arn' is not defined\n",
      "CPU times: user 9.77 ms, sys: 134 μs, total: 9.9 ms\n",
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "'''%%time\n",
    "\n",
    "# Retrieve the custom model ARN (model identifier)\n",
    "try:\n",
    "    custom_model_id_arn = boto3_bedrock_client.get_custom_model(modelIdentifier=bedrock_model_customization_model_name)['modelArn']\n",
    "    print(f\"Custom model ARN: {custom_model_id_arn}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Custom model with identifier {bedrock_model_customization_model_name} not found: {e}\")\n",
    "except boto3.exceptions.Boto3Error as e:\n",
    "    print(f\"Boto3 error occurred while fetching model ARN: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error occurred: {e}\")\n",
    "\n",
    "# Create Provisioned Throughput\n",
    "try:\n",
    "    # Create provisioned throughput for the model\n",
    "    provisioned_model_response = boto3_bedrock_client.create_provisioned_model_throughput(\n",
    "        modelUnits=1,\n",
    "        provisionedModelName=bedrock_model_provisioned_model_name,\n",
    "        modelId=bedrock_llm_foundation_model\n",
    "    )\n",
    "    provisioned_model_id_arn = provisioned_model_response['provisionedModelArn']\n",
    "    print(f\"Provisioned throughput ARN: {provisioned_model_id_arn}\")\n",
    "    \n",
    "except KeyError as e:\n",
    "    print(f\"Error: Failed to create provisioned throughput: {e}\")\n",
    "except boto3.exceptions.Boto3Error as e:\n",
    "    print(f\"Boto3 error occurred while creating provisioned throughput: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error occurred: {e}\")\n",
    "\n",
    "# Check the status of the provisioned throughput until it's completed\n",
    "try:\n",
    "    status_provisioning = boto3_bedrock_client.get_provisioned_model_throughput(provisionedModelId=provisioned_model_id_arn)['status']\n",
    "    print(f\"Provisioned throughput status: {status_provisioning}\")\n",
    "\n",
    "    while status_provisioning == 'Creating':\n",
    "        time.sleep(60)  # Wait for a minute before checking the status again\n",
    "        status_provisioning = boto3_bedrock_client.get_provisioned_model_throughput(provisionedModelId=provisioned_model_id_arn)['status']\n",
    "        print(f\"Provisioned throughput status: {status_provisioning}\")\n",
    "        time.sleep(60)  # Wait for another minute before the next status check\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Failed to retrieve provisioning status: {e}\")\n",
    "except boto3.exceptions.Boto3Error as e:\n",
    "    print(f\"Boto3 error occurred while checking provisioning status: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error occurred while checking provisioning status: {e}\")'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd8464c-6cb6-4864-9036-01e3b9d38d13",
   "metadata": {},
   "source": [
    "#### The provided code demonstrates how to invoke both a fine-tuned model and a base model using the AWS Bedrock service. First, it loads test data, extracting the prompt and reference summary for the model evaluation. The prompt is used to create the request body for invoking the models. The base model's ARN is specified, and the body includes parameters such as maxTokenCount, stopSequences, and others to control the generation. It then uses the boto3 client to invoke both the fine-tuned model and the base model. The responses are parsed, and the results from both models, along with the reference summary, are printed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a68bcf48-dd3e-444a-b89f-33270db8f7af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%%time\\n\\n# Invoke the Custom Model\\ntry:\\n    # Load test data and extract prompt and reference summary\\n    with open(test_file_path) as f:\\n        lines = f.read().splitlines()\\n    \\n    # Get the prompt and reference summary from the test data\\n    test_prompt = json.loads(lines[10])[\\'prompt\\']\\n    reference_summary = json.loads(lines[3])[\\'completion\\']\\n    print(\"Test prompt: \", test_prompt)\\n    print(\"\\n\\n\")\\n    print(\"Reference summary: \", reference_summary)\\n    print(\"\\n\\n\")\\n\\n    # Prepare the prompt for model invocation\\n    prompt = f\"{test_prompt}\"\\n\\n    # Define the base model ARN\\n    bedrock_llm_foundation_model_arn = f\\'arn:aws:bedrock:{aws_region_name}::foundation-model/{bedrock_llm_foundation_model}\\'\\n\\n    # Create the body for the model invocation request\\n    body = json.dumps({\\n        \"inputText\": prompt,\\n        \"textGenerationConfig\": {\\n            \"maxTokenCount\": 2048,\\n            \"stopSequences\": [\\'User:\\'],\\n            \"temperature\": 0,\\n            \"topP\": 0.9\\n        }\\n    })\\n\\n    # Set request headers\\n    accept = \\'application/json\\'\\n    contentType = \\'application/json\\'\\n\\n    # Invoke the fine-tuned model\\n    fine_tuned_response = boto3_bedrock_runtime_client.invoke_model(\\n        body=body, \\n        modelId=provisioned_model_id_arn, \\n        accept=accept, \\n        contentType=contentType\\n    )\\n\\n    # Invoke the base model\\n    base_model_response = boto3_bedrock_runtime_client.invoke_model(\\n        body=body, \\n        modelId=bedrock_llm_foundation_model_arn, \\n        accept=accept, \\n        contentType=contentType\\n    )\\n\\n    # Parse the responses from both models\\n    fine_tuned_response_body = json.loads(fine_tuned_response.get(\\'body\\').read())\\n    base_model_response_body = json.loads(base_model_response.get(\\'body\\').read())\\n\\n    # Print the responses from the models\\n    print(\"Base model response: \", base_model_response_body[\"results\"][0][\"outputText\"] + \\'\\n\\')\\n    print(\"\\n\\n\")\\n    print(\"Fine-tuned model response:\", fine_tuned_response_body[\"results\"][0][\"outputText\"] + \\'\\n\\')\\n    print(\"\\n\\n\")\\n    print(\"Reference summary from test data: \", reference_summary)\\n    print(\"\\n\\n\")\\n\\nexcept FileNotFoundError as e:\\n    print(f\"Error: Test file not found at {test_file_path}: {e}\")\\nexcept json.JSONDecodeError as e:\\n    print(f\"Error: Failed to decode JSON from the test file: {e}\")\\nexcept boto3.exceptions.Boto3Error as e:\\n    print(f\"Boto3 error occurred while invoking the model: {e}\")\\nexcept KeyError as e:\\n    print(f\"Error: Missing key in the model response: {e}\")\\nexcept Exception as e:\\n    print(f\"Unexpected error occurred: {e}\")'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%%time\n",
    "\n",
    "# Invoke the Custom Model\n",
    "try:\n",
    "    # Load test data and extract prompt and reference summary\n",
    "    with open(test_file_path) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    \n",
    "    # Get the prompt and reference summary from the test data\n",
    "    test_prompt = json.loads(lines[10])['prompt']\n",
    "    reference_summary = json.loads(lines[3])['completion']\n",
    "    print(\"Test prompt: \", test_prompt)\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"Reference summary: \", reference_summary)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # Prepare the prompt for model invocation\n",
    "    prompt = f\"{test_prompt}\"\n",
    "\n",
    "    # Define the base model ARN\n",
    "    bedrock_llm_foundation_model_arn = f'arn:aws:bedrock:{aws_region_name}::foundation-model/{bedrock_llm_foundation_model}'\n",
    "\n",
    "    # Create the body for the model invocation request\n",
    "    body = json.dumps({\n",
    "        \"inputText\": prompt,\n",
    "        \"textGenerationConfig\": {\n",
    "            \"maxTokenCount\": 2048,\n",
    "            \"stopSequences\": ['User:'],\n",
    "            \"temperature\": 0,\n",
    "            \"topP\": 0.9\n",
    "        }\n",
    "    })\n",
    "\n",
    "    # Set request headers\n",
    "    accept = 'application/json'\n",
    "    contentType = 'application/json'\n",
    "\n",
    "    # Invoke the fine-tuned model\n",
    "    fine_tuned_response = boto3_bedrock_runtime_client.invoke_model(\n",
    "        body=body, \n",
    "        modelId=provisioned_model_id_arn, \n",
    "        accept=accept, \n",
    "        contentType=contentType\n",
    "    )\n",
    "\n",
    "    # Invoke the base model\n",
    "    base_model_response = boto3_bedrock_runtime_client.invoke_model(\n",
    "        body=body, \n",
    "        modelId=bedrock_llm_foundation_model_arn, \n",
    "        accept=accept, \n",
    "        contentType=contentType\n",
    "    )\n",
    "\n",
    "    # Parse the responses from both models\n",
    "    fine_tuned_response_body = json.loads(fine_tuned_response.get('body').read())\n",
    "    base_model_response_body = json.loads(base_model_response.get('body').read())\n",
    "\n",
    "    # Print the responses from the models\n",
    "    print(\"Base model response: \", base_model_response_body[\"results\"][0][\"outputText\"] + '\\n')\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"Fine-tuned model response:\", fine_tuned_response_body[\"results\"][0][\"outputText\"] + '\\n')\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"Reference summary from test data: \", reference_summary)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Test file not found at {test_file_path}: {e}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error: Failed to decode JSON from the test file: {e}\")\n",
    "except boto3.exceptions.Boto3Error as e:\n",
    "    print(f\"Boto3 error occurred while invoking the model: {e}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Missing key in the model response: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error occurred: {e}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b411ca-49b7-4c17-b19c-d9ff00d26b60",
   "metadata": {},
   "source": [
    "# End of NoteBook \n",
    "\n",
    "#### <ins>Step 1</ins> \n",
    "\n",
    "##### Please ensure that you close the kernel after using this notebook to avoid any potential charges to your account.\n",
    "\n",
    "##### Process: Go to \"Kernel\" at top option. Choose \"Shut Down Kernel\". \n",
    "##### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/studio-ui.html\n",
    "\n",
    "\n",
    "#### <ins>Step 2</ins> \n",
    "\n",
    "#### If you are not executing any further lab of this Chapter 10\n",
    "##### Uncomment and execute the below code to delete the provision thoughtput and custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dca85e69-9e7d-4591-9110-ba358e3b47cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%%time \\n\\n# Function to delete provisioned throughput\\ndef delete_provisioned_throughput(provisioned_model_id):\\n    try:\\n        # Attempt to delete provisioned model throughput\\n        print(f\"Attempting to delete provisioned throughput for model ID: {provisioned_model_id}\")\\n        response = boto3_bedrock_client.delete_provisioned_model_throughput(provisionedModelId=provisioned_model_id)\\n        \\n        # Log the response if successful\\n        print(\"Provisioned throughput deletion successful. Response:\")\\n        print(response)\\n        print(\"\\n\\n\")\\n        \\n    except ClientError as e:\\n        # Handle client error (e.g., invalid request or resource not found)\\n        print(f\"Client error occurred while deleting provisioned throughput: {e}\")\\n    except Exception as e:\\n        # Handle any other unexpected errors\\n        print(f\"Unexpected error occurred while deleting provisioned throughput: {e}\")\\n\\n# Function to delete a custom model\\ndef delete_custom_model(model_identifier):\\n    try:\\n        # Attempt to delete the custom model\\n        print(f\"Attempting to delete custom model with identifier: {model_identifier}\")\\n        response = boto3_bedrock_client.delete_custom_model(modelIdentifier=model_identifier)\\n        \\n        # Log the response if successful\\n        print(\"Custom model deletion successful. Response:\")\\n        print(response)\\n        \\n    except ClientError as e:\\n        # Handle client error (e.g., invalid request or resource not found)\\n        print(f\"Client error occurred while deleting custom model: {e}\")\\n    except Exception as e:\\n        # Handle any other unexpected errors\\n        print(f\"Unexpected error occurred while deleting custom model: {e}\")\\n\\n# Delete provisioned throughput for the custom model\\ndelete_provisioned_throughput(provisioned_model_id_arn)\\n\\n# Delete the custom model\\ndelete_custom_model(bedrock_model_customization_model_name)\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%%time \n",
    "\n",
    "# Function to delete provisioned throughput\n",
    "def delete_provisioned_throughput(provisioned_model_id):\n",
    "    try:\n",
    "        # Attempt to delete provisioned model throughput\n",
    "        print(f\"Attempting to delete provisioned throughput for model ID: {provisioned_model_id}\")\n",
    "        response = boto3_bedrock_client.delete_provisioned_model_throughput(provisionedModelId=provisioned_model_id)\n",
    "        \n",
    "        # Log the response if successful\n",
    "        print(\"Provisioned throughput deletion successful. Response:\")\n",
    "        print(response)\n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "    except ClientError as e:\n",
    "        # Handle client error (e.g., invalid request or resource not found)\n",
    "        print(f\"Client error occurred while deleting provisioned throughput: {e}\")\n",
    "    except Exception as e:\n",
    "        # Handle any other unexpected errors\n",
    "        print(f\"Unexpected error occurred while deleting provisioned throughput: {e}\")\n",
    "\n",
    "# Function to delete a custom model\n",
    "def delete_custom_model(model_identifier):\n",
    "    try:\n",
    "        # Attempt to delete the custom model\n",
    "        print(f\"Attempting to delete custom model with identifier: {model_identifier}\")\n",
    "        response = boto3_bedrock_client.delete_custom_model(modelIdentifier=model_identifier)\n",
    "        \n",
    "        # Log the response if successful\n",
    "        print(\"Custom model deletion successful. Response:\")\n",
    "        print(response)\n",
    "        \n",
    "    except ClientError as e:\n",
    "        # Handle client error (e.g., invalid request or resource not found)\n",
    "        print(f\"Client error occurred while deleting custom model: {e}\")\n",
    "    except Exception as e:\n",
    "        # Handle any other unexpected errors\n",
    "        print(f\"Unexpected error occurred while deleting custom model: {e}\")\n",
    "\n",
    "# Delete provisioned throughput for the custom model\n",
    "delete_provisioned_throughput(provisioned_model_id_arn)\n",
    "\n",
    "# Delete the custom model\n",
    "delete_custom_model(bedrock_model_customization_model_name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd2a004-c7a1-4a69-9e4f-bbdde85c3ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
