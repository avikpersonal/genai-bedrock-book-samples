{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed3a60a-5001-46ff-ae70-61d861a00c53",
   "metadata": {
    "tags": []
   },
   "source": [
    "# File Name: simple_knwl_bases_chunking_strategy.ipynb\n",
    "### Location: Chapter 7\n",
    "### Purpose: \n",
    "#####             1. Example of variety of Chunking Strategy. \n",
    "#####                a) Fixed chunking strategy with Character splitting\n",
    "#####                b) Fixed chunking strategy with Recursive Character Text Splitting\n",
    "#####                c) Semantic Chunking strategy with Langchain\n",
    "#####                d) Hierarchical chunking Strategy \n",
    "##### Dependency: simple-sagemaker-bedrock.ipynb at Chapter 3 should work properly. \n",
    "# <ins>-----------------------------------------------------------------------------------</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99464e3-0679-4a67-bb38-8d96892eefa5",
   "metadata": {},
   "source": [
    "# <ins>Amazon SageMaker Classic</ins>\n",
    "#### Those who are new to Amazon SageMaker Classic. Follow the link for the details. https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91003ca1-0026-46eb-bd09-81861135cb22",
   "metadata": {},
   "source": [
    "# <ins>Environment setup of Kernel</ins>\n",
    "##### Fill \"Image\" as \"Data Science\"\n",
    "##### Fill \"Kernel\" as \"Python 3\"\n",
    "##### Fill \"Instance type\" as \"ml-t3-medium\"\n",
    "##### Fill \"Start-up script\" as \"No Scripts\"\n",
    "##### Click \"Select\"\n",
    "\n",
    "###### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-create-open.html for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865b7f5-755a-498c-bff6-7647281b15b9",
   "metadata": {},
   "source": [
    "# <ins>Mandatory installation on the kernel through pip</ins>\n",
    "\n",
    "##### This lab will work with below software version. But, if you are trying with latest version of boto3, awscli, and botocore. This code may fail. You might need to change the corresponding api. \n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade0112-a4be-4ce8-be92-8cbf28b2acfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall -q \\\n",
    "    \"boto3>=1.34.84\" \\\n",
    "    \"opensearch-py>=2.7.1\" \\\n",
    "    \"retrying>=1.3.4\" \\\n",
    "    \"bs4\" \\\n",
    "    \"pypdf\" \\\n",
    "    \"langchain_experimental\" \\\n",
    "    \"langchain>=0.2.16\" \\\n",
    "    \"langchain_community>=0.2.17\" \\\n",
    "    \"awscli>=1.32.84\" \\\n",
    "    \"botocore>=1.34.84\" \\\n",
    "    \"langchain-aws>=0.1.7\" \\\n",
    "    \"langchain-core\" \\\n",
    "    \"llama-index\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e3b85-4e64-42ef-9355-ed2a61c68b23",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins>\n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ee934-e99c-49ce-9e94-27ffc5baabe1",
   "metadata": {},
   "source": [
    "# <ins>Restart the kernel</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf41bf-d2d2-46fa-b1d4-1dd19c45a069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5af76d-b2a9-4740-b8bf-134932804719",
   "metadata": {},
   "source": [
    "# <ins>Python package import</ins>\n",
    "\n",
    "##### boto3 offers various clients for Amazon Bedrock to execute various actions.\n",
    "##### botocore is a low-level interface to AWS tools, while boto3 is built on top of botocore and provides additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7268959-6ef0-4cbd-bd53-3b31369b4513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "from retrying import retry\n",
    "import warnings\n",
    "import time\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1b877-deab-4961-a249-7663e812fbb8",
   "metadata": {},
   "source": [
    "### Ignore warning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71587271-9f02-46dc-9eaf-5a5291132f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55339fc1-732a-415e-a4a9-a18ac24cca74",
   "metadata": {},
   "source": [
    "# Download and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88c0f1-0b4a-4baa-973d-0a36e8f7d262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Print the current working directory\n",
    "print(f\"Current working directory: {current_directory}\")\n",
    "\n",
    "# Navigate up one directory and then to 'data'\n",
    "data_directory = os.path.join(os.path.dirname(current_directory), 'data/rag_use_cases')\n",
    "\n",
    "# Print the resulting path\n",
    "print(f\"Data directory path: {data_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8868554e-a410-4e3c-ab2b-00de64819eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "def load_pdf_documents(data_directory, documents ):\n",
    "    \n",
    "\n",
    "    # Loop through all files in the folder\n",
    "    for filename in os.listdir(data_directory):\n",
    "        if filename.endswith('.pdf'):  # Check if the file is a PDF\n",
    "            file_path = os.path.join(data_directory, filename)\n",
    "            try:\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                # Load the PDF data\n",
    "                data = loader.load()\n",
    "                # Add the loaded data to the documents list\n",
    "                documents.extend(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")  # Handle exceptions during loading\n",
    "\n",
    "    # Print the text of the first page of the first document\n",
    "    if documents:\n",
    "        print(documents[0].page_content) #Printing page 1 \n",
    "    else:\n",
    "        print(\"No PDF files found in the folder.\")\n",
    "        \n",
    "    return documents\n",
    "\n",
    "documents = load_pdf_documents(data_directory, documents )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e29542-3a6c-41d1-8c2a-5b9bb42e0bc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define important environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf36a8-87e3-462a-9a13-4a286861c327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try-except block to handle potential errors\n",
    "try:\n",
    "    # Create a new Boto3 session to interact with AWS services\n",
    "    boto3_session_name = boto3.session.Session()\n",
    "\n",
    "    # Retrieve the current AWS region from the session\n",
    "    aws_region_name = boto3_session_name.region_name\n",
    "\n",
    "    # Create a Bedrock Agent client using the current session and region\n",
    "    bedrock_agent_client = boto3_session_name.client('bedrock-agent', region_name=aws_region_name)\n",
    "\n",
    "    # Create an STS client to interact with AWS Security Token Service (STS)\n",
    "    sts_client = boto3.client('sts')\n",
    "    \n",
    "    # Get the AWS account ID of the caller\n",
    "    aws_account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "\n",
    "    # Create boto3_bedrock_runtime_client\n",
    "    boto3_bedrock_runtime_client = boto3.client('bedrock-runtime', region_name = aws_region_name)\n",
    "       \n",
    "    # Create boto3_bedrock_agent_runtime_client\n",
    "    boto3_bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=aws_region_name)\n",
    "    \n",
    "    # Store all variables in a dictionary\n",
    "    variables_store = {\n",
    "        \"aws_region_name\": aws_region_name,\n",
    "        \"bedrock_agent_client\": bedrock_agent_client,\n",
    "        \"aws_account_id\": aws_account_id,\n",
    "        \"boto3_bedrock_runtime_client\": boto3_bedrock_runtime_client,\n",
    "        \"boto3_bedrock_agent_runtime_client\": boto3_bedrock_agent_runtime_client,\n",
    "        \"boto3_session_name\": boto3_session_name,\n",
    "        \"sts_client\": sts_client\n",
    "    }\n",
    "\n",
    "    # Print all variables\n",
    "    for var_name, value in variables_store.items():\n",
    "        print(f\"{var_name}: {value}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7786d56f-5612-48bb-a61a-e348c871a299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Model ID and prompt\n",
    "# List of Bedrock embed models with names and model codes\n",
    "bedrock_embed_model_id = \"amazon.titan-embed-text-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa703d8-8113-483f-9677-7a31ba7a7193",
   "metadata": {},
   "source": [
    "# Fixed chunking strategy with Character splitting\n",
    "\n",
    "##### The code is for splitting text documents into chunks using the CharacterTextSplitter from LangChain, which is particularly useful when dealing with large documents and ensuring that the chunks are of manageable size. \n",
    "\n",
    "##### Example of CharacterTextSplitter\n",
    "#####   a) CharacterTextSplitter: This class splits documents into chunks based on character count.\n",
    "#####   b) chunk_size=100: Each chunk will contain up to 100 characters.\n",
    "#####   c) chunk_overlap=10: There will be a 10-character overlap between consecutive chunks. This can be useful to preserve context between chunks.\n",
    "######  d) separator=\"\": Since it's character-based, there's no separator (such as newline or punctuation) for splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25119bf8-2feb-4da8-8894-205cd9182a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Important package for Fixed chunking with Character splitting\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Function to split text documents into chunks using CharacterTextSplitter\n",
    "def split_text_documents(documents):\n",
    "    # Initialize the text splitter with specified parameters\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=100,         # Maximum length (in characters) of each chunk\n",
    "        chunk_overlap=10,       # Number of characters to overlap between consecutive chunks\n",
    "        separator=\"\"            # Separator string to split the text; default is \"\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Split the documents into chunks\n",
    "        splits = text_splitter.split_documents(documents)\n",
    "        return splits  # Return the list of chunks\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while splitting documents: {e}\")  # Handle any errors during splitting\n",
    "\n",
    "# Call the function to split text documents and retrieve the chunks\n",
    "chunked_documents = split_text_documents(documents)\n",
    "\n",
    "# Print the first two chunks to verify the output\n",
    "if chunked_documents:\n",
    "    print(chunked_documents[:5])  # Display the first five chunks\n",
    "else:\n",
    "    print(\"No chunks were generated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fff5fc-fe96-4de5-b8ed-b46fe332a092",
   "metadata": {},
   "source": [
    "# Fixed chunking strategy with Recursive Character Text Splitting\n",
    "\n",
    "###### The code uses RecursiveCharacterTextSplitter from LangChain to split documents into smaller chunks. This class is useful for when you need to split documents into chunks while considering the structure of the text (e.g., paragraphs, sentences) and maintaining continuity.\n",
    "\n",
    "##### Example of RecursiveCharacterTextSplitter\n",
    "#####   a) chunk_size=100: This parameter sets the maximum size of each chunk (in characters). Each chunk will not exceed this size.\n",
    "#####   b) chunk_overlap=10: This parameter specifies how many characters should overlap between consecutive chunks, helping preserve context between the chunks.\n",
    "#####   c) Recursive Splitting: Unlike CharacterTextSplitter, which splits based on characters only, RecursiveCharacterTextSplitter attempts to split documents recursively by trying to respect the document structure, breaking at logical points such as sentences or paragraphs if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b38cfd9-98c1-4ea6-9cf4-360d6c439ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Important package for Fixed chunking with Recursive Character Text Splitting\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents_with_text_splitter(documents):\n",
    "    try:\n",
    "        # Initialize the RecursiveCharacterTextSplitter with chunk size and overlap\n",
    "        rec_text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=100,  # Set the desired chunk size (number of characters per chunk)\n",
    "            chunk_overlap=10  # Set the overlap between chunks for better continuity\n",
    "        )\n",
    "\n",
    "        # Split the documents into smaller chunks\n",
    "        rec_text_splits = rec_text_splitter.split_documents(documents)\n",
    "\n",
    "        # Return the first two splits as a sample output\n",
    "        return rec_text_splits\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle and print any errors that occur during the splitting process\n",
    "        print(f\"Error during document splitting: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "split_results = split_documents_with_text_splitter(documents)\n",
    "\n",
    "if split_results:\n",
    "    print(\"First five document splits:\", split_results[:5])\n",
    "else:\n",
    "    print(\"No splits generated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b220925c-74d2-47f9-996d-785cb9b4ff41",
   "metadata": {},
   "source": [
    "# Semantic Chunking strategy with Langchain\n",
    "\n",
    "Semantic chunking is a Natural Language Processing (NLP) technique that divides text into meaningful segments based on semantic similarity. This approach enhances information retrieval by focusing on meaning rather than syntax. The process is guided by embedding models that measure the similarity between sentences. Below is a summary of different methods used to determine the strategy for chunking text:\n",
    "\n",
    "<ins>Percentile:</ins> This is the default method, where chunks are split based on sentence similarity differences that exceed a certain percentile threshold.\n",
    "\n",
    "<ins>Standard Deviation:</ins> Chunks are created when the sentence similarity difference exceeds a specified number of standard deviations.\n",
    "\n",
    "<ins>Interquartile:</ins> This method uses the interquartile range to set breakpoints, ensuring that the chunks are more evenly sized.\n",
    "\n",
    "<ins>Gradient:</ins> This approach combines percentile-based splitting with anomaly detection based on gradient changes, making it particularly effective in domains with high semantic correlation, such as legal or medical texts.\n",
    "\n",
    "In all cases, the embedding model—such as Amazon's Titan Embeddings—plays a crucial role in calculating these differences and ensuring that text is divided meaningfully.\n",
    "\n",
    "<ins>Use Cases:</ins> Semantic chunking is especially beneficial in scenarios where the structure of the text matters more than simple syntactic cues like paragraph or sentence length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135dd7b-004e-49a6-9805-6dd33b17e834",
   "metadata": {},
   "source": [
    "# --------------------------------------\n",
    "##### The code implements a Semantic Chunking strategy using LangChain's SemanticChunker and Bedrock Embeddings. \n",
    "##### SemanticChunker: Performs document splitting based on semantic information instead of fixed-size or character-based splitting. It uses embeddings to identify breakpoints in the text where splits should occur.\n",
    "##### Breakpoint Types:\n",
    "#####    a) \"percentile\": Splits at semantic breakpoints based on percentile distribution.\n",
    "#####    b) \"standard_deviation\": Splits where semantic changes deviate significantly from the mean.\n",
    "#####    c) \"interquartile\": Focuses on semantic shifts within the interquartile range.\n",
    "#####    d) \"gradient\": Splits based on gradients in semantic change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4cbe71-b254-4fc5-b871-f2d50ccaa5e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Important packages for Semantic Chunking strategy with LangChain\n",
    "from langchain_aws.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "def initialize_embeddings(client, model_id):\n",
    "    \"\"\"Initialize BedrockEmbeddings with the specified model ID.\"\"\"\n",
    "    try:\n",
    "        embeddings_model = BedrockEmbeddings(client=client, model_id=model_id)\n",
    "        print(\"Embeddings model initialized successfully.\")\n",
    "        return embeddings_model\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing embeddings model: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_semantic_chunker(embeddings_model, breakpoint_type):\n",
    "    \"\"\"Create a SemanticChunker with the specified breakpoint method.\"\"\"\n",
    "    try:\n",
    "        chunker = SemanticChunker(embeddings_model, breakpoint_threshold_type=breakpoint_type)\n",
    "        print(f\"SemanticChunker created with breakpoint type: {breakpoint_type}.\")\n",
    "        return chunker\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating SemanticChunker: {e}\")\n",
    "        return None\n",
    "\n",
    "def split_documents(chunker, documents):\n",
    "    \"\"\"Split documents using the SemanticChunker.\"\"\"\n",
    "    try:\n",
    "        splits = chunker.split_documents(documents)\n",
    "        print(\"Documents split successfully.\")\n",
    "        return splits\n",
    "    except Exception as e:\n",
    "        print(f\"Error splitting documents: {e}\")\n",
    "        return []\n",
    "\n",
    "# Main execution section\n",
    "def get_semantic():\n",
    "    # Step 1: Initialize embeddings model\n",
    "    embeddings_model = initialize_embeddings(boto3_bedrock_runtime_client, bedrock_embed_model_id)\n",
    "    if embeddings_model is None:\n",
    "        return  # Exit if embeddings initialization failed\n",
    "\n",
    "    # Step 2: Create SemanticChunker\n",
    "    breakpoint_type = [ \"percentile\", \"standard_deviation\", \"interquartile\", \"gradient\" ]\n",
    "    for brk_typ in breakpoint_type:\n",
    "    #breakpoint_type = \"percentile\"  # Change this to \"standard_deviation\", \"interquartile\", or \"gradient\" as needed\n",
    "        semantic_text_splitter = create_semantic_chunker(embeddings_model, brk_typ)\n",
    "        if semantic_text_splitter is None:\n",
    "            return  # Exit if chunker creation failed\n",
    "\n",
    "        # Step 3: Split documents\n",
    "        semantic_text_splits = split_documents(semantic_text_splitter, documents)\n",
    "    \n",
    "        # Display the first two splits for verification\n",
    "        print(semantic_text_splits[:2])\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "get_semantic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f63b7ac-d70a-468d-858f-f990e169bd96",
   "metadata": {},
   "source": [
    "# Hierarchical chunking Strategy \n",
    "\n",
    "Hierarchical Chunking is a technique that organizes documents into parent and child chunks, creating a structured hierarchy. This approach allows models to better understand relationships between different parts of a document, resulting in more contextually relevant and coherent responses from large language models (LLMs).\n",
    "\n",
    "Using the HierarchicalNodeParser from the llama-index library, documents are segmented into multiple layers of chunks, where each smaller chunk (child) maintains a reference to its larger containing chunk (parent). This hierarchical structure preserves context, even when processing individual sections separately.\n",
    "\n",
    "When paired with the AutoMergingRetriever, the model can dynamically replace retrieved child nodes with their parent nodes if the majority of child nodes are retrieved. This ensures the model accesses a more complete and cohesive context, enhancing response quality.\n",
    "\n",
    "<ins>HierarchicalNodeParser:</ins> Segments documents into a hierarchy of nodes, with each node referencing its parent, enabling structured contextual representation.\n",
    "\n",
    "<ins>AutoMergingRetriever:</ins> Automatically substitutes retrieved child nodes with their parent to provide the model with a broader context for synthesizing responses.\n",
    "\n",
    "<ins>Use Cases:</ins> This technique is particularly advantageous for managing large or complex documents, as it maintains the integrity and context of information across varying levels of granularity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d76787-9a35-4b55-8a1b-6ed8f5c6e468",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### This code demonstrates a Hierarchical Chunking strategy using llama_index, with a focus on progressively splitting text into smaller, contextually relevant chunks. Here's a breakdown of its workflow, functionality, and suggestions for enhancement:\n",
    "##### chunk_sizes: A list of integers defining the hierarchical chunk sizes, e.g., [512, 254, 128]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785cbf45-d32a-4f69-9fb5-56169e227bb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Important package for Hierarchical chunking\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import HierarchicalNodeParser\n",
    "\n",
    "def load_documents(input_dir):\n",
    "    \"\"\"Load documents from the specified directory.\"\"\"\n",
    "    try:\n",
    "        reader = SimpleDirectoryReader(input_dir=input_dir)\n",
    "        documents = reader.load_data()\n",
    "        print(\"Documents loaded successfully.\")\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading documents: {e}\")\n",
    "        return None\n",
    "\n",
    "def initialize_hierarchical_node_parser(chunk_sizes):\n",
    "    \"\"\"Initialize HierarchicalNodeParser with specified chunk sizes.\"\"\"\n",
    "    try:\n",
    "        node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=chunk_sizes)\n",
    "        print(f\"HierarchicalNodeParser initialized with chunk sizes: {chunk_sizes}\")\n",
    "        return node_parser\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing HierarchicalNodeParser: {e}\")\n",
    "        return None\n",
    "\n",
    "def chunk_documents(node_parser, documents):\n",
    "    \"\"\"Chunk documents into hierarchical nodes.\"\"\"\n",
    "    try:\n",
    "        nodes = node_parser.get_nodes_from_documents(documents)\n",
    "        print(\"Documents chunked successfully.\")\n",
    "        return nodes\n",
    "    except Exception as e:\n",
    "        print(f\"Error chunking documents: {e}\")\n",
    "        return None\n",
    "\n",
    "def display_node_content(nodes, num_nodes=2):\n",
    "    \"\"\"Display the content of the first few nodes.\"\"\"\n",
    "    try:\n",
    "        for i in range(min(num_nodes, len(nodes))):\n",
    "            print(f\"Node {i} content: {nodes[i].text[:500]}...\")  # Display the first 500 chars for brevity\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying node content: {e}\")\n",
    "\n",
    "# Main execution function\n",
    "def get_hierarchical():\n",
    "    \n",
    "    chunk_sizes = [512, 254, 128]  # Define chunk sizes\n",
    "\n",
    "    # Step 1: Load documents\n",
    "    documents = load_documents(data_directory)\n",
    "    if documents is None:\n",
    "        return  # Exit if document loading failed\n",
    "\n",
    "    # Step 2: Initialize HierarchicalNodeParser\n",
    "    node_parser = initialize_hierarchical_node_parser(chunk_sizes)\n",
    "    if node_parser is None:\n",
    "        return  # Exit if node parser initialization failed\n",
    "\n",
    "    # Step 3: Chunk documents\n",
    "    nodes = chunk_documents(node_parser, documents)\n",
    "    if nodes is None:\n",
    "        return  # Exit if document chunking failed\n",
    "\n",
    "    # Step 4: Display node content for verification\n",
    "    display_node_content(nodes)\n",
    "\n",
    "\n",
    "get_hierarchical()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b411ca-49b7-4c17-b19c-d9ff00d26b60",
   "metadata": {},
   "source": [
    "# End of NoteBook \n",
    "\n",
    "#### <ins>Step 1</ins> \n",
    "\n",
    "##### Please ensure that you close the kernel after using this notebook to avoid any potential charges to your account.\n",
    "\n",
    "##### Process: Go to \"Kernel\" at top option. Choose \"Shut Down Kernel\". \n",
    "##### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/studio-ui.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c0ab09-ef6e-49ba-9e97-ff43200667c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
