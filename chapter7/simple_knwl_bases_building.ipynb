{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed3a60a-5001-46ff-ae70-61d861a00c53",
   "metadata": {
    "tags": []
   },
   "source": [
    "# File Name: simple_knwl_bases_building.ipynb\n",
    "### Location: Chapter 7\n",
    "### Purpose: \n",
    "#####             1. Create collection on serverless opensearch\n",
    "#####             2. Create a network policy for collection\n",
    "#####             3. Create a security policy for encryption using an AWS-owned key\n",
    "#####             4. Create a access policy for collection to define permissions for the collection and index\n",
    "#####             5. Call the create_access_policy method to define permissions for the collection and index\n",
    "#####             6. Create a vector search collection in OpenSearch Serverless\n",
    "#####             7. Collection will take some time to be \"ACTIVE\". So, checking when the collection is \"ACTIVE\" for the next steps\n",
    "#####             8. Index Creation on the collection\n",
    "#####             9. Create the Amazon Knowledge Bases\n",
    "#####             10. Create a DataSource in KnowledgeBase \n",
    "#####             11. Ingest data into the Amazon Knowledge Bases.  \n",
    "#####             12. Test the Amazon Knowledge Bases using RetrieveAndGenerate API and Retrieve API\n",
    "\n",
    "##### Dependency: simple-sageMaker-bedrock.ipynb at Chapter 3 should work properly. \n",
    "# <ins>-----------------------------------------------------------------------------------</ins>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99464e3-0679-4a67-bb38-8d96892eefa5",
   "metadata": {},
   "source": [
    "# <ins>Amazon SageMaker Classic</ins>\n",
    "#### Those who are new to Amazon SageMaker Classic. Follow the link for the details. https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91003ca1-0026-46eb-bd09-81861135cb22",
   "metadata": {},
   "source": [
    "# <ins>Environment setup of Kernel</ins>\n",
    "##### Fill \"Image\" as \"Data Science\"\n",
    "##### Fill \"Kernel\" as \"Python 3\"\n",
    "##### Fill \"Instance type\" as \"ml-t3-medium\"\n",
    "##### Fill \"Start-up script\" as \"No Scripts\"\n",
    "##### Click \"Select\"\n",
    "\n",
    "###### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-create-open.html for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865b7f5-755a-498c-bff6-7647281b15b9",
   "metadata": {},
   "source": [
    "# <ins>Mandatory installation on the kernel through pip</ins>\n",
    "\n",
    "##### This lab will work with below software version. But, if you are trying with latest version of boto3, awscli, and botocore. This code may fail. You might need to change the corresponding api. \n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade0112-a4be-4ce8-be92-8cbf28b2acfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall -q \\\n",
    "    \"boto3>=1.34.84\" \\\n",
    "    \"opensearch-py>=2.7.1\" \\\n",
    "    \"retrying>=1.3.4\" \\\n",
    "    \"ragas\" \\\n",
    "    \"ipywidgets>=7.6.5\" \\\n",
    "    \"iprogress>=0.4\" \\\n",
    "    \"langchain>=0.2.16\" \\\n",
    "    \"langchain_community>=0.2.17\" \\\n",
    "    \"awscli>=1.32.84\" \\\n",
    "    \"botocore>=1.34.84\" \\\n",
    "    \"langchain-aws>=0.1.7\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e3b85-4e64-42ef-9355-ed2a61c68b23",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins>\n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ee934-e99c-49ce-9e94-27ffc5baabe1",
   "metadata": {},
   "source": [
    "# <ins>Restart the kernel</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf41bf-d2d2-46fa-b1d4-1dd19c45a069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5af76d-b2a9-4740-b8bf-134932804719",
   "metadata": {},
   "source": [
    "# <ins>Python package import</ins>\n",
    "\n",
    "##### boto3 offers various clients for Amazon Bedrock to execute various actions.\n",
    "##### botocore is a low-level interface to AWS tools, while boto3 is built on top of botocore and provides additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7268959-6ef0-4cbd-bd53-3b31369b4513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "import pprint\n",
    "import random\n",
    "from retrying import retry\n",
    "import warnings\n",
    "import time\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, RequestError\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
    "import pprint as pp\n",
    "from botocore.exceptions import BotoCoreError, ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1b877-deab-4961-a249-7663e812fbb8",
   "metadata": {},
   "source": [
    "### Ignore warning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71587271-9f02-46dc-9eaf-5a5291132f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b83ea1-9c4e-4ad2-9f83-4dfb0721a166",
   "metadata": {},
   "source": [
    "## Define important environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf36a8-87e3-462a-9a13-4a286861c327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Try-except block to handle potential errors\n",
    "try:\n",
    "    # Create a new Boto3 session to interact with AWS services\n",
    "    boto3_session_name = boto3.session.Session()\n",
    "\n",
    "    # Retrieve the current AWS region from the session\n",
    "    aws_region_name = boto3_session_name.region_name\n",
    "\n",
    "    # Create a Bedrock Agent client using the current session and region\n",
    "    bedrock_agent_client = boto3_session_name.client('bedrock-agent', region_name=aws_region_name)\n",
    "\n",
    "    # Define the service name for Amazon OpenSearch Serverless (AOSS)\n",
    "    opensearch_service_name = 'aoss'\n",
    "\n",
    "    # Create an S3 client to interact with Amazon S3\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Create an STS client to interact with AWS Security Token Service (STS)\n",
    "    sts_client = boto3.client('sts')\n",
    "\n",
    "    # Get the AWS account ID of the caller\n",
    "    aws_account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "\n",
    "    # Generate a suffix using the region and account ID for the S3 bucket name\n",
    "    s3_suffix = f\"{aws_region_name}-{aws_account_id}\"\n",
    "\n",
    "    # Generate a random suffix number between 200 and 900\n",
    "    random_suffix = random.randrange(200, 900)\n",
    "\n",
    "    # Define the name of the S3 bucket (you can replace this with your actual bucket name)\n",
    "    s3_bucket_name = f'bedrock-kb-{s3_suffix}-{random_suffix}'\n",
    "\n",
    "    # PrettyPrinter instance for formatted output\n",
    "    pretty_printer = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "    # Create an OpenSearch Serverless (AOSS) client using the current session\n",
    "    aoss_client = boto3_session_name.client('opensearchserverless')\n",
    "\n",
    "    # Generate unique names for the vector store and index based on the suffix\n",
    "    vector_store_name = f'bedrock-sample-rag-{random_suffix}'\n",
    "    index_name = f\"bedrock-sample-rag-index-{random_suffix}\"\n",
    "\n",
    "    # Create an IAM client to interact with Identity and Access Management (IAM) service\n",
    "    iam_client = boto3_session_name.client('iam')\n",
    "\n",
    "    # Retrieve the current AWS account number and ARN of the caller\n",
    "    sts_client = boto3.client('sts')\n",
    "    identity_arn = sts_client.get_caller_identity().get('Arn')\n",
    "    \n",
    "    # Create security policy name for aoss collection\n",
    "    security_policy_name = f'aoss-collection-sec-policy-{random_suffix}'\n",
    "    network_policy_name = f'aoss-collection-net-policy-{random_suffix}'\n",
    "    access_policy_name = f'aoss-collection-acs-policy-{random_suffix}'\n",
    "    \n",
    "    # Embedding model ARN for Bedrock\n",
    "    embeddingModelArn = f\"arn:aws:bedrock:{aws_region_name}::foundation-model/amazon.titan-embed-text-v1\"\n",
    "    \n",
    "    # Amazon Knowledges Bases variable \n",
    "    bedrock_knowledge_bases_name = f\"bedrock-sample-knowledge-bases-{random_suffix}\"\n",
    "    description = \"Amazon shareholder letter knowledge base.\"\n",
    "\n",
    "    # Store all variables in a dictionary\n",
    "    variables_store = {\n",
    "        \"aws_region_name\": aws_region_name,\n",
    "        \"bedrock_agent_client\": bedrock_agent_client,\n",
    "        \"opensearch_service_name\": opensearch_service_name,\n",
    "        \"s3_client\": s3_client,\n",
    "        \"sts_client\": sts_client,\n",
    "        \"aws_account_id\": aws_account_id,\n",
    "        \"s3_suffix\": s3_suffix,\n",
    "        \"s3_bucket_name\": s3_bucket_name,\n",
    "        \"random_suffix\": random_suffix,\n",
    "        \"aoss_client\": aoss_client,\n",
    "        \"vector_store_name\": vector_store_name,\n",
    "        \"index_name\": index_name,\n",
    "        \"iam_client\": iam_client,\n",
    "        \"sts_client\": sts_client,\n",
    "        \"identity_arn\": identity_arn,\n",
    "        \"security_policy_name\": security_policy_name,\n",
    "        \"network_policy_name\": network_policy_name,\n",
    "        \"access_policy_name\": access_policy_name,\n",
    "        \"embeddingModelArn\": embeddingModelArn,\n",
    "        \"bedrock_knowledge_bases_name\": bedrock_knowledge_bases_name,\n",
    "        \"description\": description\n",
    "    }\n",
    "\n",
    "    # Print all variables\n",
    "    for var_name, value in variables_store.items():\n",
    "        print(f\"{var_name}: {value}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a0b500-3c80-42dc-a828-3872fb4249b6",
   "metadata": {},
   "source": [
    "### ---------------\n",
    "##### The provided code snippet uses the AWS Boto3 library to manage an Amazon S3 bucket for a knowledge base data source. It begins by creating an S3 client and defines a bucket name, s3_bucket_name. A function, check_bucket_exists, checks whether the specified bucket exists by attempting to retrieve its metadata using the head_bucket method. If the bucket exists, a message is printed confirming its existence. If it does not exist (error code '404'), the function returns False. If the bucket is missing, the script proceeds to create it using the create_bucket method, ensuring the data source bucket is always available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c75dc4-e4ee-4183-984e-5b459bd2dc3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Check if s3 bucket exists, and if not create S3 bucket for knowledge base data source\n",
    "\n",
    "# Try-except block to handle potential errors\n",
    "try:\n",
    "    # Create an S3 client to interact with Amazon S3\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Define the bucket name (you can replace this with your actual bucket name)\n",
    "    bucket_name = s3_bucket_name\n",
    "\n",
    "    # Check if the bucket exists\n",
    "    def check_bucket_exists(bucket_name):\n",
    "        try:\n",
    "            s3_client.head_bucket(Bucket=bucket_name)\n",
    "            print(f\"Bucket '{bucket_name}' already exists.\")\n",
    "            return True\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            error_code = e.response['Error']['Code']\n",
    "            if error_code == '404':\n",
    "                print(f\"Bucket '{bucket_name}' does not exist.\")\n",
    "                return False\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    # If the bucket doesn't exist, create it\n",
    "    if not check_bucket_exists(bucket_name):\n",
    "        # Create the S3 bucket\n",
    "        s3_client.create_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "\n",
    "except botocore.exceptions.BotoCoreError as boto_error:\n",
    "    print(f\"An error occurred with Boto3: {boto_error}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761c5750-9c32-46d0-afdc-6fbd44785b22",
   "metadata": {},
   "source": [
    "### %store magic command to store the variable for use in other notebook cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ee542-8bb1-47f3-b2ea-a9b907691d60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store bucket_name aws_region_name opensearch_service_name embeddingModelArn description\n",
    "%store aws_account_id s3_suffix s3_bucket_name random_suffix bedrock_knowledge_bases_name\n",
    "%store vector_store_name index_name identity_arn security_policy_name network_policy_name access_policy_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97b66a5-a3ca-423d-b5ec-024b0b19204a",
   "metadata": {},
   "source": [
    "# Create collection on serverless opensearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6bd77a-6761-4075-b7ee-da9839a3c03b",
   "metadata": {},
   "source": [
    "### Create a network policy for collection\n",
    "\n",
    "##### This code creates a network security policy for an Amazon OpenSearch Serverless (AOSS) collection using the aoss_client from the AWS Boto3 library. The policy is named network_policy_name and specifies access rules in JSON format, targeting a resource identified as collection/<vector_store_name>. The policy type is set to network, with an option (AllowFromPublic) to allow public access, customizable based on the use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af877c24-6b7f-4798-b585-8ce44ec1b47e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create a network policy for collection\n",
    "\n",
    "try:\n",
    "    # Creating a network security policy\n",
    "    network_policy_name_res = aoss_client.create_security_policy(\n",
    "        name=network_policy_name,  # Name of the security policy\n",
    "        policy=json.dumps(  # JSON-formatted policy rules\n",
    "            [\n",
    "                {\n",
    "                    'Rules': [\n",
    "                        {\n",
    "                            'Resource': ['collection/' + vector_store_name],  # Define the resource\n",
    "                            'ResourceType': 'collection'  # Specify that it's a collection resource\n",
    "                        }\n",
    "                    ],\n",
    "                    'AllowFromPublic': True  # Allow public access (may need to change based on your use case)\n",
    "                }\n",
    "            ]\n",
    "        ),\n",
    "        type='network'  # Define the type of security policy as 'network'\n",
    "    )\n",
    "\n",
    "    # If the security policy is created successfully, print the success message\n",
    "    print(f\"Security policy '{network_policy_name}' created successfully.\")\n",
    "\n",
    "# Handle the case where the security policy already exists\n",
    "except aoss_client.exceptions.ConflictException:\n",
    "    print(f\"Security policy '{network_policy_name}' already exists.\")\n",
    "\n",
    "# Handle validation errors such as incorrect policy structure\n",
    "except aoss_client.exceptions.ValidationException as e:\n",
    "    print(f\"Validation error when creating security policy: {str(e)}\")\n",
    "\n",
    "# Catch any other general exceptions\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while creating the security policy: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969b252f-75f8-446a-8f55-cbaec1b1e48b",
   "metadata": {},
   "source": [
    "### Create a security policy for encryption using an AWS-owned key\n",
    "\n",
    "##### This code snippet creates an encryption security policy for an Amazon OpenSearch Serverless (AOSS) collection using the AWS Boto3 aoss_client. The policy, named security_policy_name, is configured to use an AWS-owned key for encryption (AWSOwnedKey: True). The target resource is specified as collection/<vector_store_name>, ensuring encryption rules apply specifically to the intended collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c9af15-f20b-4a5a-bdc2-5fd2f2ea10c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create a security policy for encryption using an AWS-owned key\n",
    "try:\n",
    "    security_policy_response = aoss_client.create_security_policy(\n",
    "        name=security_policy_name,\n",
    "        policy=json.dumps(\n",
    "            {\n",
    "                'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                           'ResourceType': 'collection'}],\n",
    "                'AWSOwnedKey': True\n",
    "            }),\n",
    "        type='encryption'\n",
    "    )\n",
    "    \n",
    "    print(f\"Security policy '{security_policy_name}' created successfully.\")\n",
    "except aoss_client.exceptions.ConflictException:\n",
    "    print(f\"Security policy '{security_policy_name}' already exists.\")\n",
    "except aoss_client.exceptions.ValidationException as e:\n",
    "    print(f\"Validation error when creating security policy: {str(e)}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while creating security policy: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56194ff3-d453-42c1-b51f-8fe468b52326",
   "metadata": {},
   "source": [
    "### Create a access policy for collection to define permissions for the collection and index\n",
    "\n",
    "##### This code defines a function, find_iam_role_by_name_substring, to locate an IAM role and retrieve its ARN based on a specified substring within the role name. Using the AWS Boto3 iam_client, it lists all IAM roles and filters them for names containing the substring \"GenAIBookBedrockSageMakerExecutionR\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d2612b-fa71-4215-9bc5-46e73bade035",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Find out IAM role and ARN for this session\n",
    "\n",
    "def find_iam_role_by_name_substring(substring):\n",
    "    try:\n",
    "        # Use list_roles to retrieve IAM roles\n",
    "        response = iam_client.list_roles()\n",
    "\n",
    "        # Filter roles by name that contains the substring\n",
    "        matching_roles = [role for role in response['Roles'] if substring in role['RoleName']]\n",
    "\n",
    "        if matching_roles:\n",
    "            for role in matching_roles:\n",
    "                print(f\"Found Role: {role['RoleName']} | ARN: {role['Arn']}\")\n",
    "                genaibookedbedrocksagemakerexecutionrolearn = role['Arn']\n",
    "        else:\n",
    "            print(f\"No roles found with name containing '{substring}'.\")\n",
    "            \n",
    "        return genaibookedbedrocksagemakerexecutionrolearn\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# Call the function with the desired substring\n",
    "genaibookedbedrocksagemakerexecutionrolearn = find_iam_role_by_name_substring(\"GenAIBookBedrockSageMakerExecutionRole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cce7cf-e986-4638-af2d-349ddef43338",
   "metadata": {},
   "source": [
    "### %store magic command to store the variable for use in other notebook cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbf9842-eae6-455c-997e-f5be98edd657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store genaibookedbedrocksagemakerexecutionrolearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b69efa-384d-4b93-a718-aee131e3ae88",
   "metadata": {},
   "source": [
    "### Call the create_access_policy method to define permissions for the collection and index\n",
    "\n",
    "##### This code creates an access policy for managing permissions on an Amazon OpenSearch Serverless (AOSS) collection and index using the AWS Boto3 aoss_client. The policy, named access_policy_name, defines detailed rules for both the collection and index resources, specifying actions allowed for each.\n",
    "##### Policy Rules:\n",
    "#####               a) For collection resources (collection/<vector_store_name>)\n",
    "#####               b) For index resources (index/<vector_store_name>/*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62e54ef-d19b-4e03-ab21-0c3c51f9515e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    \n",
    "    access_policy_res = aoss_client.create_access_policy(\n",
    "        name=access_policy_name,  # The name of the access policy being created\n",
    "        policy=json.dumps(  # The access policy body, provided in JSON format\n",
    "            [\n",
    "                {\n",
    "                    'Rules': [  # Define the access rules for the resources\n",
    "                        {\n",
    "                            'Resource': ['collection/' + vector_store_name],  # Specify the resource collection\n",
    "                            'Permission': [  # Define allowed actions for the collection\n",
    "                                'aoss:CreateCollectionItems',  # Allows creating items in the collection\n",
    "                                'aoss:DeleteCollectionItems',  # Allows deleting items from the collection\n",
    "                                'aoss:UpdateCollectionItems',  # Allows updating items in the collection\n",
    "                                'aoss:DescribeCollectionItems'  # Allows describing items in the collection\n",
    "                            ],\n",
    "                            'ResourceType': 'collection'  # Define resource type as collection\n",
    "                        },\n",
    "                        {\n",
    "                            'Resource': ['index/' + vector_store_name + '/*'],  # Specify the index resource path\n",
    "                            'Permission': [  # Define allowed actions for the index\n",
    "                                'aoss:CreateIndex',  # Allows creating an index\n",
    "                                'aoss:DeleteIndex',  # Allows deleting an index\n",
    "                                'aoss:UpdateIndex',  # Allows updating an index\n",
    "                                'aoss:DescribeIndex',  # Allows describing an index\n",
    "                                'aoss:ReadDocument',  # Allows reading documents from the index\n",
    "                                'aoss:WriteDocument'  # Allows writing documents to the index\n",
    "                            ],\n",
    "                            'ResourceType': 'index'  # Define resource type as index\n",
    "                        }\n",
    "                    ],\n",
    "                    'Principal': [  # Define who has access to this policy\n",
    "                        identity_arn,  # The primary ARN to which the policy applies\n",
    "                        genaibookedbedrocksagemakerexecutionrolearn  # Example of an additional ARN\n",
    "                    ],\n",
    "                    'Description': 'Easy data policy'  # Description of the policy\n",
    "                }\n",
    "            ]\n",
    "        ),\n",
    "        type='data'  \n",
    "    )\n",
    "    \n",
    "    # If the policy is created successfully, print a success message\n",
    "    print(f\"Access policy '{access_policy_name}' created successfully.\")\n",
    "\n",
    "# Handle case where a policy with the same name already exists\n",
    "except aoss_client.exceptions.ConflictException:\n",
    "    print(f\"Access policy '{access_policy_name}' already exists.\")\n",
    "\n",
    "# Handle validation errors during policy creation\n",
    "except aoss_client.exceptions.ValidationException as e:\n",
    "    print(f\"Validation error when creating access policy: {str(e)}\")\n",
    "\n",
    "# Handle any other exceptions that occur during the process\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while creating access policy: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677d7fd3-bb89-4d79-bf1f-fd6ba111d82e",
   "metadata": {},
   "source": [
    "### Create a vector search collection in OpenSearch Serverless\n",
    "\n",
    "##### This code attempts to create a vector search collection in Amazon OpenSearch Serverless using the AWS Boto3 aoss_client. The collection, named vector_store_name, is configured for VECTORSEARCH, a specialized type of collection used for vector-based information retrieval.\n",
    "\n",
    "##### The create_collection method is called with the specified name and type (VECTORSEARCH) and followed by extracting collection details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c294aa1e-43a7-4d81-b20d-65dc343080c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Try to create a vector search collection in OpenSearch Serverless\n",
    "try:\n",
    "    response = aoss_client.create_collection(\n",
    "        name=vector_store_name,\n",
    "        type='VECTORSEARCH'\n",
    "    )\n",
    "    print(f\"Collection '{vector_store_name}' creation is in progress.\")\n",
    "    print(\"Response:\", response)\n",
    "    \n",
    "    aoss_collection_host = response['createCollectionDetail']['id'] + '.' + aws_region_name + '.aoss.amazonaws.com'\n",
    "    aoss_collectionarn = response['createCollectionDetail']['arn']\n",
    "    \n",
    "    print(f\"aoss_collection_host '{aoss_collection_host}' creation is in progress.\")\n",
    "    print(f\"aoss_collectionarn '{aoss_collectionarn}' creation is in progress.\")\n",
    "    \n",
    "except aoss_client.exceptions.ConflictException:\n",
    "    print(f\"Collection '{vector_store_name}' already exists.\")\n",
    "except aoss_client.exceptions.ValidationException as e:\n",
    "    print(f\"Validation error: {str(e)}\")\n",
    "except aoss_client.exceptions.ServiceQuotaExceededException as e:\n",
    "    print(f\"Service quota exceeded: {str(e)}\")\n",
    "except aoss_client.exceptions.OcuLimitExceededException as e:\n",
    "    print(f\"OCU limit exceeded: {str(e)}\")\n",
    "except aoss_client.exceptions.InternalServerException as e:\n",
    "    print(f\"Internal server error: {str(e)}\")\n",
    "except aoss_client.exceptions.ResourceNotFoundException as e:\n",
    "    print(f\"Resource not found: {str(e)}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f35e1d2-f7e4-494c-887d-45f435089b03",
   "metadata": {},
   "source": [
    "### %store magic command to store the variable for use in other notebook cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b1da4-2267-4c23-939c-02986a056540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store aoss_collection_host aoss_collectionarn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b12467-cbe7-42fe-a77f-ceb115bad5cc",
   "metadata": {},
   "source": [
    "### Collection will take some time to be \"ACTIVE\". So, checking when the collection is \"ACTIVE\" for the next steps.\n",
    "\n",
    "##### This code provides a mechanism to wait for an Amazon OpenSearch Serverless vector search collection to transition to the \"ACTIVE\" state, which is necessary before proceeding with subsequent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ffbdf-c872-426d-b3e4-6660812a9f95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Collection will take some time to be \"ACTIVE\". So, checking when the collection is \"ACTIVE\" for the next steps.\n",
    "\n",
    "def interactive_sleep(seconds):\n",
    "    \"\"\"A simple sleep function that could be replaced with more complex logic.\"\"\"\n",
    "    time.sleep(seconds)\n",
    "\n",
    "def wait_for_collection_creation(aoss_client, vector_store_name):\n",
    "    try:\n",
    "        # Initial call to batch_get_collection\n",
    "        response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "        \n",
    "        # Periodically check collection status\n",
    "        while response['collectionDetails'][0]['status'] == 'CREATING':\n",
    "            print('Creating collection...')\n",
    "            interactive_sleep(30)\n",
    "            response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "        \n",
    "        print(f'\\nCollection successfully created: {vector_store_name}')\n",
    "    \n",
    "    except ClientError as e:\n",
    "        print(f\"An error occurred: {e.response['Error']['Message']}\")\n",
    "    except IndexError:\n",
    "        print(\"No collection details found. Please check the collection name.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "wait_for_collection_creation(aoss_client, vector_store_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdabda37-2819-4ed3-924f-0be0f48a3e5f",
   "metadata": {},
   "source": [
    "# Index Creation on the collection\n",
    "\n",
    "##### This script defines a modular approach to create a KNN (k-Nearest Neighbor) vector index in an Amazon OpenSearch Serverless collection using Python and Boto3. The code provides functions for authentication, OpenSearch client creation, and index management while handling errors gracefully.\n",
    "\n",
    "##### 1. AWS Authentication (get_aws_auth): Retrieves AWS credentials using boto3.Session() and constructs an AWSV4SignerAuth object.\n",
    "##### 2. OpenSearch Client Creation (create_opensearch_client): Establishes a connection to OpenSearch using the provided host and AWS authentication.\n",
    "##### 3. Vector Index Creation (create_vector_index): Checks if the specified index already exists using client.indices.exists()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f91d6e7-8007-4397-b097-76734d4dc20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_aws_auth(region_name, service):\n",
    "    \"\"\"Retrieve AWS authentication credentials.\"\"\"\n",
    "    try:\n",
    "        credentials = boto3.Session().get_credentials()\n",
    "        awsauth = auth = AWSV4SignerAuth(credentials, region_name, service)\n",
    "        return awsauth\n",
    "    except (NoCredentialsError, PartialCredentialsError) as e:\n",
    "        print(f\"Error retrieving AWS credentials: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_opensearch_client(host, awsauth):\n",
    "    \"\"\"Build the OpenSearch client.\"\"\"\n",
    "    try:\n",
    "        client = OpenSearch(\n",
    "            hosts=[{'host': host, 'port': 443}],\n",
    "            http_auth=awsauth,\n",
    "            use_ssl=True,\n",
    "            verify_certs=True,\n",
    "            connection_class=RequestsHttpConnection,\n",
    "            timeout=300\n",
    "        )\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating OpenSearch client: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_vector_index(client, index_name, index_body):\n",
    "    \"\"\"Create the vector index in OpenSearch.\"\"\"\n",
    "    try:\n",
    "        if not client.indices.exists(index=index_name):\n",
    "            client.indices.create(index=index_name, body=json.dumps(index_body))\n",
    "            print(f\"Index '{index_name}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Index '{index_name}' already exists.\")\n",
    "    except RequestError as e:\n",
    "        print(f\"Error creating index '{index_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "def create_index():\n",
    "\n",
    "    index_body = {\n",
    "           \"settings\": {\n",
    "              \"index.knn\": \"true\",\n",
    "               \"number_of_shards\": 1,\n",
    "               \"knn.algo_param.ef_search\": 512,\n",
    "               \"number_of_replicas\": 0,\n",
    "           },\n",
    "           \"mappings\": {\n",
    "              \"properties\": {\n",
    "                 \"vector\": {\n",
    "                    \"type\": \"knn_vector\",\n",
    "                    \"dimension\": 1536,\n",
    "                     \"method\": {\n",
    "                         \"name\": \"hnsw\",\n",
    "                         \"engine\": \"faiss\",\n",
    "                         \"space_type\": \"l2\"\n",
    "                     },\n",
    "                 },\n",
    "                 \"text\": {\n",
    "                    \"type\": \"text\"\n",
    "                 },\n",
    "                 \"text-metadata\": {\n",
    "                    \"type\": \"text\"         }\n",
    "              }\n",
    "           }\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        # Get AWS authentication\n",
    "        awsauth = get_aws_auth(aws_region_name, opensearch_service_name)\n",
    "\n",
    "        # Create OpenSearch client\n",
    "        oss_client = create_opensearch_client(aoss_collection_host, awsauth)\n",
    "\n",
    "        # Create index\n",
    "        try:\n",
    "            response = oss_client.indices.create(index=index_name, body=json.dumps(index_body))\n",
    "            print('\\nCreating index:')\n",
    "\n",
    "            # index creation can take up to a minute\n",
    "            interactive_sleep(60)\n",
    "        except RequestError as e:\n",
    "            # you can delete the index if its already exists\n",
    "            # oss_client.indices.delete(index=index_name)\n",
    "            print(f'Error while trying to create the index, with error {e.error}\\nyou may unmark the delete above to delete, and recreate the index')\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the process: {e}\")\n",
    "\n",
    "create_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57e3a9d-f121-4b61-9ff6-777cfc8261d0",
   "metadata": {},
   "source": [
    "# Download and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b783f0-76b1-4869-b4cf-447a5dd56789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Print the current working directory\n",
    "print(f\"Current working directory: {current_directory}\")\n",
    "\n",
    "# Construct the path to 'data/rag_use_cases' inside the current directory\n",
    "data_directory = os.path.join(current_directory, 'data', 'rag_use_cases')\n",
    "\n",
    "# Print the resulting path\n",
    "print(f\"Data directory path: {data_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bbf789-92d9-4e22-928d-0c0546b970db",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Disclaimer\n",
    "##### Make Sure that data_directory is pointing to the right path and data files are present. Otherwise, you need to change the above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfb1be7-651a-4d93-bb7f-fe2101e04f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def upload_directory_to_s3(directory_path, bucket_name, s3_prefix=''):\n",
    "    \"\"\"\n",
    "    Uploads all files from a local directory to an S3 bucket.\n",
    "\n",
    "    :param directory_path: Local path to the directory\n",
    "    :param bucket_name: The name of the target S3 bucket\n",
    "    :param s3_prefix: The S3 folder (prefix) where the files will be uploaded (optional)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Walk through the directory and upload files to S3\n",
    "        for root, dirs, files in os.walk(directory_path):\n",
    "            for file_name in files:\n",
    "                # Construct the full local path\n",
    "                local_file_path = os.path.join(root, file_name)\n",
    "                \n",
    "                # Construct the S3 key (upload destination path)\n",
    "                relative_path = os.path.relpath(local_file_path, directory_path)\n",
    "                s3_file_path = os.path.join(s3_prefix, relative_path)\n",
    "                \n",
    "                # Upload file to S3\n",
    "                s3_client.upload_file(local_file_path, bucket_name, s3_file_path)\n",
    "                print(f\"Uploaded {local_file_path} to s3://{bucket_name}/{s3_file_path}\")\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: The directory {directory_path} does not exist: {e}\")\n",
    "    except NoCredentialsError:\n",
    "        print(\"Error: AWS credentials not found.\")\n",
    "    except PartialCredentialsError:\n",
    "        print(\"Error: Incomplete AWS credentials.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "\n",
    "# Upload the directory to S3\n",
    "upload_directory_to_s3(data_directory, s3_bucket_name, 'data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3826a0-09ca-4399-91ff-ab3b95e0038d",
   "metadata": {},
   "source": [
    "# Create the Amazon Knowledge Bases\n",
    "\n",
    "##### This script is designed to create a knowledge base in Amazon Bedrock with OpenSearch Serverless as storage, using embeddings for vector-based search. It includes configurations for chunking document ingestion, retry mechanisms, and functions to handle the creation and retrieval of the knowledge base.\n",
    "\n",
    "##### Chunking Strategy (chunkingStrategyConfiguration): Defines how documents are chunked before ingestion like \n",
    "##### Fixed Size: The documents are chunked into parts, each having a maximum size (maxTokens: 512).\n",
    "##### Overlap Percentage: 20% overlap between chunks to ensure that context is maintained across chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62d567d-7dff-4ef5-8162-16bef1b987d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Global configuration\n",
    "opensearchServerlessConfiguration = {\n",
    "    \"collectionArn\": aoss_collectionarn,\n",
    "    \"vectorIndexName\": index_name,\n",
    "    \"fieldMapping\": {\n",
    "        \"vectorField\": \"vector\",\n",
    "        \"textField\": \"text\",\n",
    "        \"metadataField\": \"text-metadata\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Chunking strategy for document ingestion\n",
    "chunkingStrategyConfiguration = {\n",
    "    \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "    \"fixedSizeChunkingConfiguration\": {\n",
    "        \"maxTokens\": 512,\n",
    "        \"overlapPercentage\": 20\n",
    "    }\n",
    "}\n",
    "\n",
    "# S3 configuration for the data source\n",
    "s3Configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{s3_bucket_name}\"\n",
    "    # Optional: \"inclusionPrefixes\": [\"*.*\"]  # Specify prefixes if needed\n",
    "}\n",
    "\n",
    "# Retry mechanism for creating the knowledge base\n",
    "# Retry Mechanism: The @retry decorator retries the creation process with a random delay to handle transient failures.\n",
    "@retry(wait_random_min=1000, wait_random_max=2000, stop_max_attempt_number=7)\n",
    "def create_knowledge_base_func(bedrock_client, name, description, roleArn, opensearch_config, embedding_arn):\n",
    "    \"\"\"\n",
    "    Create a knowledge base using Bedrock with OpenSearch as storage.\n",
    "    Retries on failure, with exponential backoff and random delay.\n",
    "    \n",
    "    :param bedrock_client: Boto3 Bedrock client\n",
    "    :param name: Name of the knowledge base\n",
    "    :param description: Description of the knowledge base\n",
    "    :param roleArn: IAM Role ARN for execution\n",
    "    :param opensearch_config: OpenSearch serverless configuration\n",
    "    :param embedding_arn: ARN of the embedding model\n",
    "    :return: Knowledge base details on success\n",
    "    \"\"\"\n",
    "    try:\n",
    "        create_kb_response = bedrock_client.create_knowledge_base(\n",
    "            name=name,\n",
    "            description=description,\n",
    "            roleArn=roleArn,\n",
    "            knowledgeBaseConfiguration={\n",
    "                \"type\": \"VECTOR\",\n",
    "                \"vectorKnowledgeBaseConfiguration\": {\n",
    "                    \"embeddingModelArn\": embedding_arn\n",
    "                }\n",
    "            },\n",
    "            storageConfiguration={\n",
    "                \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "                \"opensearchServerlessConfiguration\": opensearch_config\n",
    "            }\n",
    "        )\n",
    "        return create_kb_response[\"knowledgeBase\"]\n",
    "    except Exception as err:\n",
    "        print(f\"Error occurred while creating knowledge base: {err}, {type(err)}\")\n",
    "        raise\n",
    "\n",
    "# Function to get knowledge base details\n",
    "def get_knowledge_base(bedrock_client, knowledge_base_id):\n",
    "    \"\"\"\n",
    "    Retrieve details of the created knowledge base.\n",
    "    \n",
    "    :param bedrock_client: Boto3 Bedrock client\n",
    "    :param knowledge_base_id: Knowledge base ID to retrieve\n",
    "    :return: Knowledge base details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        get_kb_response = bedrock_client.get_knowledge_base(knowledgeBaseId=knowledge_base_id)\n",
    "        return get_kb_response\n",
    "    except Exception as err:\n",
    "        print(f\"Error retrieving knowledge base: {err}, {type(err)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Main section to execute the creation and retrieval process\n",
    "\n",
    "# Create knowledge base\n",
    "try:\n",
    "    kb = create_knowledge_base_func(\n",
    "            bedrock_agent_client, bedrock_knowledge_bases_name, description, genaibookedbedrocksagemakerexecutionrolearn, opensearchServerlessConfiguration, embeddingModelArn\n",
    "        )\n",
    "    print(\"Knowledge base created successfully.\")\n",
    "    print(kb)\n",
    "        \n",
    "    # Retrieve knowledge base details\n",
    "    kb_details = get_knowledge_base(bedrock_agent_client, kb['knowledgeBaseId'])\n",
    "    print(\"Knowledge base details:\")\n",
    "    print(kb_details)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Failed to create or retrieve knowledge base: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d590b411-eedc-4c6b-9672-d0ed4c026dd0",
   "metadata": {},
   "source": [
    "# Create a DataSource in KnowledgeBase \n",
    "\n",
    "##### This script provides functions to create and retrieve data sources for a knowledge base in Amazon Bedrock. It uses S3 as the data source and supports vector ingestion with a specified chunking strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9add9-c435-4b0b-a7ec-de20e92f143f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Function to create a data source in a knowledge base\n",
    "def create_data_source(bedrock_agent_client, kb, name, description, s3_configuration, chunking_strategy_configuration):\n",
    "    \"\"\"\n",
    "    Creates a data source in the knowledge base.\n",
    "\n",
    "    Parameters:\n",
    "    - bedrock_agent_client: The Boto3 client for Bedrock.\n",
    "    - kb: Dictionary containing the knowledge base details.\n",
    "    - name: Name of the data source.\n",
    "    - description: Description of the data source.\n",
    "    - s3_configuration: Configuration details for S3.\n",
    "    - chunking_strategy_configuration: Chunking configuration for vector ingestion.\n",
    "\n",
    "    Returns:\n",
    "    - ds: The created data source object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to create the data source\n",
    "        create_ds_response = bedrock_agent_client.create_data_source(\n",
    "            name=name,\n",
    "            description=description,\n",
    "            knowledgeBaseId=kb['knowledgeBaseId'],\n",
    "            dataSourceConfiguration={\n",
    "                \"type\": \"S3\",\n",
    "                \"s3Configuration\": s3_configuration\n",
    "            },\n",
    "            vectorIngestionConfiguration={\n",
    "                \"chunkingConfiguration\": chunking_strategy_configuration\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Retrieve the data source from the response\n",
    "        ds = create_ds_response[\"dataSource\"]\n",
    "        print(ds)\n",
    "        return ds\n",
    "\n",
    "    except ClientError as e:\n",
    "        # Handle any Boto3 client errors\n",
    "        print(f\"Error creating data source: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to get details of a data source in the knowledge base\n",
    "def get_data_source(bedrock_agent_client, kb, data_source_id):\n",
    "    \"\"\"\n",
    "    Fetches the details of a data source by its ID.\n",
    "\n",
    "    Parameters:\n",
    "    - bedrock_agent_client: The Boto3 client for Bedrock.\n",
    "    - kb: Dictionary containing the knowledge base details.\n",
    "    - data_source_id: The ID of the data source.\n",
    "\n",
    "    Returns:\n",
    "    - response: The response containing the data source details.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to retrieve the data source details\n",
    "        response = bedrock_agent_client.get_data_source(\n",
    "            knowledgeBaseId=kb['knowledgeBaseId'],\n",
    "            dataSourceId=data_source_id\n",
    "        )\n",
    "        print(response)\n",
    "        return response\n",
    "\n",
    "    except ClientError as e:\n",
    "        # Handle any Boto3 client errors\n",
    "        print(f\"Error retrieving data source: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assume bedrock_agent_client is already initialized\n",
    "# Assume kb is a dictionary containing the knowledge base details\n",
    "# Assume s3_configuration and chunking_strategy_configuration are dictionaries containing relevant configuration details\n",
    "\n",
    "knowledgeBaseId = kb['knowledgeBaseId'] # Example knowledge base\n",
    "\n",
    "# Amazon Knowledges Bases variable \n",
    "bedrock_kb_datasouces_name = f\"bedrock-sample-kb-ds-{random_suffix}\"\n",
    "bedrock_kb_datasouces_des = \"A description of my data source.\"\n",
    "\n",
    "# Create the data source\n",
    "data_source = create_data_source(bedrock_agent_client, kb, bedrock_kb_datasouces_name, bedrock_kb_datasouces_des, s3Configuration, chunkingStrategyConfiguration)\n",
    "\n",
    "# Get details of the data source if it was successfully created\n",
    "if data_source:\n",
    "    get_data_source(bedrock_agent_client, kb, data_source[\"dataSourceId\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be3769-0576-40ce-b895-a3d4e76c914c",
   "metadata": {},
   "source": [
    "### %store magic command to store the variable for use in other notebook cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba37558-861d-427b-ac56-9e9210ce0d73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print dataSourceId\n",
    "dataSourceId = data_source[\"dataSourceId\"]\n",
    "\n",
    "%store dataSourceId kb bedrock_kb_datasouces_name bedrock_kb_datasouces_des s3Configuration chunkingStrategyConfiguration knowledgeBaseId"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4baf8a2-f594-4c1f-9f82-9ea718d99259",
   "metadata": {},
   "source": [
    "##### The provided function check_data_source_status is designed to check the status of a data source within an Amazon Bedrock knowledge base. It polls for the status of the data source in a loop, printing the current status and retrying if the data source creation is still in progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af43e028-6640-4f38-b42d-cf687bfd84b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def check_data_source_status(client, knowledge_base_id, data_source_id, max_attempts=10, delay=30):\n",
    "    \"\"\"Check the status of a data source in a loop until it's created or reaches a terminal state.\"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < max_attempts:\n",
    "        try:\n",
    "            # Fetch the current state of the knowledge base\n",
    "            response = client.get_knowledge_base(knowledgeBaseId=knowledge_base_id)\n",
    "            \n",
    "            # Locate the specific data source in the knowledge base's data sources\n",
    "            data_sources = response['knowledgeBase'].get('dataSources', [])\n",
    "            data_source = next((ds for ds in data_sources if ds['dataSourceId'] == data_source_id), None)\n",
    "            \n",
    "            if not data_source:\n",
    "                print(f\"Data source {data_source_id} not found in knowledge base {knowledge_base_id}.\")\n",
    "                return\n",
    "            \n",
    "            # Check the status of the data source\n",
    "            status = data_source['status']\n",
    "            print(f\"Data source {data_source_id} status: {status}\")\n",
    "            \n",
    "            # Check if the data source creation is complete or failed\n",
    "            if status == 'CREATED':\n",
    "                print(f\"Data source {data_source_id} has been successfully created.\")\n",
    "                return True\n",
    "            elif status in ['FAILED', 'ERROR']:\n",
    "                print(f\"Data source {data_source_id} creation failed with status: {status}\")\n",
    "                return False\n",
    "\n",
    "            # Wait for the specified delay before polling again\n",
    "            print(f\"Waiting for {delay} seconds before checking again...\")\n",
    "            time.sleep(delay)\n",
    "            attempt += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error checking data source status: {e}\")\n",
    "            return False\n",
    "\n",
    "    print(f\"Max attempts reached. Data source {data_source_id} creation not confirmed.\")\n",
    "    return False\n",
    "\n",
    "# Call the function to check the data source status in a loop\n",
    "check_data_source_status(bedrock_agent_client, knowledgeBaseId, dataSourceId)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7837ac-e03a-41d2-be7c-ba25262ca2e9",
   "metadata": {},
   "source": [
    "### Wait untill Both Amazon Knowledge Bases and Data Source are ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b26b56f-acea-4cf0-a87a-8ff158bbf86d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Fetch knowledge base status\n",
    "        response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId=knowledgeBaseId)\n",
    "        kb_status = response['knowledgeBase']['status']\n",
    "        \n",
    "        # Fetch data source status\n",
    "        response = bedrock_agent_client.get_data_source(knowledgeBaseId=knowledgeBaseId, dataSourceId=dataSourceId)\n",
    "        data_source_status = response['dataSource']['status']\n",
    "        \n",
    "        # Check if both statuses are as desired\n",
    "        if kb_status == 'ACTIVE' and data_source_status == 'AVAILABLE':\n",
    "            print(\"Both Knowledge Base and Data Source are ready.\")\n",
    "            break\n",
    "        \n",
    "        # Wait for a few seconds before the next check\n",
    "        time.sleep(30)  # Wait for 30 seconds before checking again\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        break  # Exit the loop if there's an error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f44ef87-a689-4400-aea8-aa03e8a925df",
   "metadata": {},
   "source": [
    "# Start an ingestion job\n",
    "\n",
    "##### The provided code outlines the ingestion job process for Amazon Bedrock, including starting the ingestion job, checking its status, and polling until it completes.\n",
    "\n",
    "##### start_ingestion_job: This function initiates an ingestion job for a specified data source and knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0797d839-a6bf-494b-8346-651e0d430458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Function to start the ingestion job\n",
    "def start_ingestion_job(bedrock_client, knowledge_base_id, data_source_id):\n",
    "    try:\n",
    "        # Start the ingestion job\n",
    "        print(\"Starting ingestion job...\")\n",
    "        start_job_response = bedrock_client.start_ingestion_job(\n",
    "            knowledgeBaseId=knowledge_base_id, \n",
    "            dataSourceId=data_source_id\n",
    "        )\n",
    "        job = start_job_response[\"ingestionJob\"]\n",
    "        print(job)\n",
    "        return job\n",
    "    except Exception as e:\n",
    "        print(f\"Error while starting ingestion job: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Function to get the status of the ingestion job\n",
    "def get_ingestion_job_status(bedrock_client, knowledge_base_id, data_source_id, ingestion_job_id):\n",
    "    try:\n",
    "        # Fetch the job status\n",
    "        get_job_response = bedrock_client.get_ingestion_job(\n",
    "            knowledgeBaseId=knowledge_base_id,\n",
    "            dataSourceId=data_source_id,\n",
    "            ingestionJobId=ingestion_job_id\n",
    "        )\n",
    "        job = get_job_response[\"ingestionJob\"]\n",
    "        return job\n",
    "    except Exception as e:\n",
    "        print(f\"Error while fetching ingestion job status: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Function to poll the ingestion job status until complete\n",
    "def poll_ingestion_job_completion(bedrock_client, knowledge_base_id, data_source_id, ingestion_job_id, poll_interval=30):\n",
    "    try:\n",
    "        # Continuously poll the job status until it's complete\n",
    "        job = get_ingestion_job_status(bedrock_client, knowledge_base_id, data_source_id, ingestion_job_id)\n",
    "        while job and job['status'] != 'COMPLETE':\n",
    "            print(f\"Job status: {job['status']}. Waiting for completion...\")\n",
    "            interactive_sleep(poll_interval)\n",
    "            job = get_ingestion_job_status(bedrock_client, knowledge_base_id, data_source_id, ingestion_job_id)\n",
    "        if job:\n",
    "            print(\"Ingestion job completed:\")\n",
    "            print(job)\n",
    "        else:\n",
    "            print(\"Failed to retrieve final job status.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during job polling: {str(e)}\")\n",
    "\n",
    "# Main function to orchestrate the job ingestion process\n",
    "def execute_ingestion_process(bedrock_client, knowledge_base_id, data_source_id):\n",
    "    try:\n",
    "        # Start the ingestion job\n",
    "        job = start_ingestion_job(bedrock_client, knowledge_base_id, data_source_id)\n",
    "        \n",
    "        if job:\n",
    "            # Poll until the job is complete\n",
    "            poll_ingestion_job_completion(bedrock_client, knowledge_base_id, data_source_id, job[\"ingestionJobId\"])\n",
    "        else:\n",
    "            print(\"Failed to start ingestion job.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in the ingestion process: {str(e)}\")\n",
    "\n",
    "\n",
    "        # Execute the ingestion process\n",
    "execute_ingestion_process(bedrock_agent_client, knowledgeBaseId , dataSourceId)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8906fcc-4a0d-4dad-bde7-012f7cde5aaa",
   "metadata": {},
   "source": [
    "# Test the Amazon Knowledge Bases "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f15d7ee-8fc7-49eb-8179-a8057969d657",
   "metadata": {},
   "source": [
    "#### Using RetrieveAndGenerate API \n",
    "#### Refer: https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent-runtime_RetrieveAndGenerate.html \n",
    "####        https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve_and_generate.html#\n",
    "###### Purpose: Queries a knowledge base and generates responses based on the retrieved results and using the specified foundation model or inference profile. \n",
    "######          The response only cites sources that are relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd5815a-c9d9-428b-a206-13bfd0348b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Function to query the knowledge base and generate a response\n",
    "def ask_bedrock_llm_with_knowledge_base(client, query: str, model_arn: str, kb_id: str) -> dict:\n",
    "    try:\n",
    "        response = client.retrieve_and_generate(\n",
    "            input={'text': query},\n",
    "            retrieveAndGenerateConfiguration={\n",
    "                'type': 'KNOWLEDGE_BASE',\n",
    "                'knowledgeBaseConfiguration': {\n",
    "                    'knowledgeBaseId': kb_id,\n",
    "                    'modelArn': model_arn\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "        return response\n",
    "    except (BotoCoreError, ClientError) as e:\n",
    "        print(f\"Error during API call to retrieve and generate response: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Function to process and print the generated response and its citations\n",
    "def process_and_print_response(model_name: str, response: dict):\n",
    "    try:\n",
    "        generated_text = response['output']['text']\n",
    "        citations = response.get(\"citations\", [])\n",
    "        contexts = []\n",
    "\n",
    "        for citation in citations:\n",
    "            retrieved_references = citation.get(\"retrievedReferences\", [])\n",
    "            for reference in retrieved_references:\n",
    "                contexts.append(reference[\"content\"][\"text\"])\n",
    "\n",
    "        print(f\"---------- Generated using {model_name}:\")\n",
    "        print(generated_text)\n",
    "        print()\n",
    "        print()\n",
    "        print(f\"---------- Citations for the response generated by {model_name}:\")\n",
    "        print(contexts)\n",
    "        print()\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Key error while processing response: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Main execution section\n",
    "prompt = \"What is Amazon doing and cashflow?\"\n",
    "\n",
    "# List of Bedrock models with names and model codes\n",
    "bedrock_model_ids = [\n",
    "        [\"Anthropic Claude Haiku\", \"anthropic.claude-3-haiku-20240307-v1:0\"]\n",
    "    ]\n",
    "\n",
    "# Initialize Bedrock client\n",
    "try:\n",
    "    \n",
    "    boto3_bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=aws_region_name)\n",
    "\n",
    "# Loop through each Claude model to generate and print results\n",
    "    for model_id in bedrock_model_ids:\n",
    "        model_name, model_code = model_id\n",
    "        model_arn = f'arn:aws:bedrock:{aws_region_name}::foundation-model/{model_code}'\n",
    "            \n",
    "        try:\n",
    "            # Query the knowledge base with the specified model\n",
    "            response = ask_bedrock_llm_with_knowledge_base(boto3_bedrock_agent_runtime_client, prompt, model_arn, knowledgeBaseId)\n",
    "            # Process and display the generated results\n",
    "            process_and_print_response(model_name, response)\n",
    "            \n",
    "        except Exception as e:\n",
    "                print(f\"An error occurred with model {model_name}: {str(e)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while initializing the client or querying models: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a29054-1726-48e9-8432-52dfed6aad80",
   "metadata": {},
   "source": [
    "#### Using Retrieve API\n",
    "#### Refer: https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent-runtime_Retrieve.html\n",
    "####        https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve.html#retrieve#\n",
    "###### Purpose: Queries a knowledge base and retrieves information from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19224485-8b64-4c1d-93a4-4ff96bd9b944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Function to retrieve relevant documents from the knowledge base\n",
    "def retrieve_relevant_documents(client, query: str, kb_id: str, num_results: int = 3) -> dict:\n",
    "    try:\n",
    "        response = client.retrieve(\n",
    "            retrievalQuery={'text': query},\n",
    "            knowledgeBaseId=kb_id,\n",
    "            retrievalConfiguration={\n",
    "                'vectorSearchConfiguration': {\n",
    "                    'numberOfResults': num_results  # Fetch top `num_results` documents matching the query\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        return response\n",
    "    except (BotoCoreError, ClientError) as e:\n",
    "        print(f\"Error retrieving relevant documents: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Function to process and print the retrieved documents\n",
    "def process_and_print_retrieved_documents(response: dict):\n",
    "    try:\n",
    "        retrieval_results = response.get(\"retrievalResults\", [])\n",
    "        if not retrieval_results:\n",
    "            print(\"No relevant documents found.\")\n",
    "            return\n",
    "\n",
    "        print(\"---------- Relevant Documents Retrieved:\")\n",
    "        print(retrieval_results)\n",
    "        print()\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Key error while processing retrieved documents: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Main execution section\n",
    "prompt = \"What is Amazon doing and cashflow?\"\n",
    "\n",
    "\n",
    "# Initialize Bedrock client\n",
    "try:\n",
    "        \n",
    "    boto3_bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=aws_region_name)\n",
    "\n",
    "    # Retrieve relevant documents based on the query\n",
    "    response = retrieve_relevant_documents(boto3_bedrock_agent_runtime_client, prompt, knowledgeBaseId, num_results=3)\n",
    "\n",
    "    # Process and display the retrieved documents\n",
    "    process_and_print_retrieved_documents(response)\n",
    "\n",
    "except Exception as e:\n",
    "        print(f\"An error occurred during retrieval: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b411ca-49b7-4c17-b19c-d9ff00d26b60",
   "metadata": {},
   "source": [
    "# End of NoteBook \n",
    "\n",
    "#### <ins>Step 1</ins> \n",
    "\n",
    "##### Please ensure that you close the kernel after using this notebook to avoid any potential charges to your account.\n",
    "\n",
    "##### Process: Go to \"Kernel\" at top option. Choose \"Shut Down Kernel\". \n",
    "##### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/studio-ui.html\n",
    "\n",
    "\n",
    "#### <ins>Step 2</ins> \n",
    "\n",
    "#### If you are not executing any further lab of this Chapter 7\n",
    "##### Execute the simple_knwl_bases_clean_up.ipynb to delete all the instances to avoid any potential charges to your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f78ab-ff89-4dd4-9d9c-29e946170ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
