{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed3a60a-5001-46ff-ae70-61d861a00c53",
   "metadata": {
    "tags": []
   },
   "source": [
    "# File Name: simple_prompt_mgmt.ipynb\n",
    "### Location: Chapter 16\n",
    "### Purpose: \n",
    "#####       1. Create Start and End node of prompt flow\n",
    "#####       2. Create invoke and eval prompt and prompt version using Prompt management \n",
    "#####        3. Create evaluate and invoke node of prompt flow \n",
    "#####        4. Create connections between nodes\n",
    "#####        5. Create prompt flow with nodes and connections \n",
    "#####        6. Prepare the prompt flow and find out the status of the prompt flow\n",
    "#####        7. Create prompt flow version\n",
    "#####        8. Create prompt alias\n",
    "#####        9. Testing the prompt flow \n",
    "#####        10. Deleting resources associated with an AWS Bedrock prompt flow, including aliases, versions, the flow itself, and associated prompts.\n",
    "        \n",
    "##### Dependency: Not Applicable\n",
    "# <ins>-----------------------------------------------------------------------------------</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99464e3-0679-4a67-bb38-8d96892eefa5",
   "metadata": {},
   "source": [
    "# <ins>Amazon SageMaker Classic</ins>\n",
    "#### Those who are new to Amazon SageMaker Classic. Follow the link for the details. https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91003ca1-0026-46eb-bd09-81861135cb22",
   "metadata": {},
   "source": [
    "# <ins>Environment setup of Kernel</ins>\n",
    "##### Fill \"Image\" as \"Data Science\"\n",
    "##### Fill \"Kernel\" as \"Python 3\"\n",
    "##### Fill \"Instance type\" as \"ml-t3-medium\"\n",
    "##### Fill \"Start-up script\" as \"No Scripts\"\n",
    "##### Click \"Select\"\n",
    "\n",
    "###### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-create-open.html for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865b7f5-755a-498c-bff6-7647281b15b9",
   "metadata": {},
   "source": [
    "# <ins>Mandatory installation on the kernel through pip</ins>\n",
    "\n",
    "##### This lab will work with below software version. But, if you are trying with latest version of boto3, awscli, and botocore. This code may fail. You might need to change the corresponding api. \n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade0112-a4be-4ce8-be92-8cbf28b2acfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall -q \\\n",
    "    \"boto3\" \\\n",
    "    \"awscli\" \\\n",
    "    \"botocore\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e3b85-4e64-42ef-9355-ed2a61c68b23",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins>\n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ee934-e99c-49ce-9e94-27ffc5baabe1",
   "metadata": {},
   "source": [
    "# <ins>Restart the kernel</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf41bf-d2d2-46fa-b1d4-1dd19c45a069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5af76d-b2a9-4740-b8bf-134932804719",
   "metadata": {},
   "source": [
    "# <ins>Python package import</ins>\n",
    "\n",
    "##### boto3 offers various clients for Amazon Bedrock to execute various actions.\n",
    "##### botocore is a low-level interface to AWS tools, while boto3 is built on top of botocore and provides additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7268959-6ef0-4cbd-bd53-3b31369b4513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "import warnings\n",
    "import time\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError, ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1b877-deab-4961-a249-7663e812fbb8",
   "metadata": {},
   "source": [
    "### Ignore warning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71587271-9f02-46dc-9eaf-5a5291132f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a707d91-23c2-4786-bd2a-6287ca51ef82",
   "metadata": {},
   "source": [
    "## Define important environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4247b5-5fe7-4ec8-9f25-bff782734606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try-except block to handle potential errors\n",
    "try:\n",
    "    # Create a new Boto3 session to interact with AWS services\n",
    "    # This session is responsible for managing credentials and region configuration\n",
    "    boto3_session = boto3.session.Session()\n",
    "\n",
    "    # Retrieve the current AWS region from the session (e.g., 'us-east-1', 'us-west-2')\n",
    "    aws_region_name = boto3_session.region_name\n",
    "    \n",
    "    # Initialize Bedrock and Bedrock Runtime and Bedrock Agent clients using Boto3\n",
    "    # These clients will allow interactions with Bedrock-related AWS services\n",
    "    boto3_bedrock_client = boto3.client('bedrock', region_name=aws_region_name)\n",
    "    boto3_bedrock_runtime_client = boto3.client('bedrock-runtime', region_name=aws_region_name)\n",
    "    boto3_bedrock_agent_client = boto3.client(service_name=\"bedrock-agent\", region_name=aws_region_name)\n",
    "    boto3_bedrock_agent_runtime_client = boto3.client(service_name=\"bedrock-agent-runtime\", region_name=aws_region_name)\n",
    "    \n",
    "    \n",
    "    # Define the name of the invoke prompt \n",
    "    invoke_prompt_name = \"iPhone_accessories_recommendation\"\n",
    "    invoke_prompt_description=\"Initial prompt for iPhone accessories recommendation\"  # Description of the prompt\n",
    "    \n",
    "    # Define the name of the eval prompt \n",
    "    eval_prompt_name = \"eval_iPhone_accessories_recommendation\"\n",
    "    eval_prompt_description=\"Eval prompt for iPhone accessories recommendation\"  # Description of the prompt\n",
    "    \n",
    "    # Define the name of the prompt flow\n",
    "    flow_name = \"Flow_iPhone_accessories_recommendation\"\n",
    "    flow_description=\"Prompt flow for iPhone accessories recommendation\"  # Description of the prompt flow\n",
    "    \n",
    "    ### Adjust with your preferred model IDs for invocations and evaluation - Note some models are only available in certain regions:\n",
    "    bedrock_model_invoke_id = \"amazon.titan-text-express-v1\"\n",
    "    bedrock_model_eval_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "    # Store all relevant variables in a dictionary for easier access and management\n",
    "    variables_store = {\n",
    "        \"aws_region_name\": aws_region_name,                          # AWS region name\n",
    "        \"boto3_bedrock_client\": boto3_bedrock_client,                # Bedrock client instance\n",
    "        \"boto3_bedrock_runtime_client\": boto3_bedrock_runtime_client,  # Bedrock Runtime client instance\n",
    "        \"boto3_bedrock_agent_client\": boto3_bedrock_agent_client,  # Bedrock agent client instance\n",
    "        \"boto3_bedrock_agent_runtime_client\": boto3_bedrock_agent_runtime_client,  # Bedrock agent client instance\n",
    "        \"invoke_prompt_name\": invoke_prompt_name,\n",
    "        \"invoke_prompt_description\": invoke_prompt_description,\n",
    "        \"eval_prompt_name\": eval_prompt_name,\n",
    "        \"eval_prompt_description\": eval_prompt_description,\n",
    "        \"flow_name\": flow_name,\n",
    "        \"flow_description\": flow_description,\n",
    "        \"bedrock_model_invoke_id\": bedrock_model_invoke_id,\n",
    "        \"bedrock_model_eval_id\": bedrock_model_eval_id,\n",
    "        \"boto3_session\": boto3_session                               # Current Boto3 session object\n",
    "    }\n",
    "\n",
    "    # Print all stored variables for debugging and verification\n",
    "    for var_name, value in variables_store.items():\n",
    "        print(f\"{var_name}: {value}\")\n",
    "\n",
    "# Handle any exceptions that occur during the execution\n",
    "except Exception as e:\n",
    "    # Print the error message if an unexpected error occurs\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5fa21a-ebdc-4aee-bdde-52ac877b3e57",
   "metadata": {},
   "source": [
    "# Find out ARN of IAM role for Flows Service Role \n",
    "\n",
    "### list_filtered_iam_roles(): Retrieves and filters AWS IAM roles based on specific conditions. This is required for executing the prompt flow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a927b1-5582-4116-8327-3c86c2c70b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_filtered_iam_roles():\n",
    "    \"\"\"\n",
    "    Lists IAM roles in the AWS account that match specific ARN patterns\n",
    "    and do not contain 'Lambda' or 'Stack', with error handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create an IAM client\n",
    "        iam_client = boto3.client('iam')\n",
    "        \n",
    "        # Retrieve the list of IAM roles\n",
    "        roles = iam_client.list_roles()\n",
    "\n",
    "        # Define the keywords to include and exclude\n",
    "        include_keywords = [\"genalbookbedrocksagemaker\"]\n",
    "        exclude_keywords = [\"Lambda\", \"Stack\"]\n",
    "\n",
    "        # Initialize a variable to store the matching role ARN\n",
    "        flows_service_role = None\n",
    "\n",
    "        # Iterate through the roles and filter based on the ARN\n",
    "        for role in roles['Roles']:\n",
    "            arn = role['Arn']\n",
    "            # Check for inclusion and exclusion conditions\n",
    "            if (any(keyword.lower() in arn.lower() for keyword in include_keywords) and\n",
    "                    not any(keyword.lower() in arn.lower() for keyword in exclude_keywords)):\n",
    "                flows_service_role = arn  # Store the matching ARN\n",
    "\n",
    "        if flows_service_role:\n",
    "            return flows_service_role\n",
    "        else:\n",
    "            print(\"No matching roles found.\")\n",
    "            return None\n",
    "\n",
    "    except NoCredentialsError:\n",
    "        print(\"Error: AWS credentials not found.\")\n",
    "    except PartialCredentialsError:\n",
    "        print(\"Error: AWS credentials are incomplete.\")\n",
    "    except ClientError as e:\n",
    "        print(f\"Client error occurred: {e.response['Error']['Message']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "flows_service_role = list_filtered_iam_roles()\n",
    "if flows_service_role:\n",
    "    print(f\"Found Matching Role ARN: {flows_service_role}\")\n",
    "else:\n",
    "    print(\"No valid role ARN found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67338c7-6286-43bb-ba3b-6b9f8d0544a5",
   "metadata": {},
   "source": [
    "# Prompt Flow Architecture Diagram "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88171e9c-e3b3-41fc-ab38-7387d54e69ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./prompt_flow_arc_diagram.png\" style=\"width: 600px; height: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d4f01a-0956-418c-a4d2-89403e63a14f",
   "metadata": {},
   "source": [
    "# Create start node \n",
    "#### Refer: Architecture diagram step (1) above \n",
    "\n",
    "##### The create_start_node function defines the \"Start\" node for a flow, which is an input node that initiates the process. It includes a configuration for the input (an empty object) and outputs a string named document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdc3616-c0bc-482a-984b-611e0babd5b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to define the Start node\n",
    "def create_start_node():\n",
    "    try:\n",
    "        # Start node definition: This node is used to define the input for the flow\n",
    "        return {\n",
    "            \"name\": \"Start_Node\",  # The name of the node\n",
    "            \"type\": \"Input\",  # The type of node (Input node to start the flow)\n",
    "            \"configuration\": {\n",
    "                \"input\": {}  # Configuration for input, here it's just an empty object\n",
    "            },\n",
    "            \"outputs\": [  # The outputs for this node\n",
    "                {\n",
    "                    \"name\": \"document\",  # Output name\n",
    "                    \"type\": \"String\"  # Output type (String, in this case)\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Catching any exceptions that occur while defining the node\n",
    "        print(f\"Error in creating Start node: {e}\")\n",
    "        return None  # Returning None in case of an error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36cdcbf-6c63-4b18-b359-c50ae46276c3",
   "metadata": {},
   "source": [
    "# Create end node \n",
    "#### Refer: Architecture diagram step (4) above \n",
    "\n",
    "##### The create_end_node function defines the configuration for the \"End\" node in a flow, which represents the final step in processing. This node takes the input data, processes it, and produces the final output. The node is configured as an \"Output\" type with no specific output configuration, and it expects input data under the name \"response\", using the expression $.data to refer to the flow's data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f3b02-3c63-451a-946e-c5d756acb3c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to define the End node with error handling\n",
    "def create_end_node():\n",
    "    try:\n",
    "        # The \"End\" node represents the final step in the flow.\n",
    "        # It receives input data and outputs the result.\n",
    "        return {\n",
    "            \"name\": \"End_Node\",  # Name of the node, representing the end of the flow.\n",
    "            \"type\": \"Output\",  # The type of node, indicating it's an output node.\n",
    "            \"configuration\": {\n",
    "                \"output\": {}  # No specific configuration for output, it's the result of the flow.\n",
    "            },\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                    \"expression\": \"$.data\",  # Expression for the input, referring to data flowing in the flow.\n",
    "                    \"name\": \"document\",  # The name of the input variable.\n",
    "                    \"type\": \"String\"  # The expected data type for this input.\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Catch any exceptions that occur during the creation of the End node\n",
    "        print(f\"An error occurred while creating the End node: {e}\")\n",
    "        return None  # Return None if an error occurs, indicating failure to create the node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2961b623-3924-4d05-b265-05025298f965",
   "metadata": {},
   "source": [
    "# Create invoke and eval prompt using Prompt management one by one. \n",
    "\n",
    "#### Details is in Chapter 15 simple_prompt_mgmt.ipynb to Create prompt and prompt version using Prompt management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1cc93e-1ed7-45e1-9c29-08a66b384c53",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 1. Create a eval prompt using Prompt management\n",
    "##### 2. Create a version of a eval prompt in Prompt management  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e93d7ad-711f-434a-a9cc-2336294e65b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Step 1: Create a new prompt\n",
    "    response = boto3_bedrock_agent_client.create_prompt(\n",
    "        name=eval_prompt_name,\n",
    "        description=eval_prompt_description,\n",
    "        variants=[\n",
    "            {\n",
    "                \"inferenceConfiguration\": {\n",
    "                    \"text\": {\n",
    "                        \"maxTokens\": 2000,\n",
    "                        \"temperature\": 0,\n",
    "                    }\n",
    "                },\n",
    "                \"modelId\": bedrock_model_eval_id,\n",
    "                \"name\": \"eval_variant\",\n",
    "                \"templateConfiguration\": {\n",
    "                    \"text\": {\n",
    "                        \"inputVariables\": [\n",
    "                            {\"name\": \"input\"},\n",
    "                            {\"name\": \"output\"}\n",
    "                        ],\n",
    "                        \"text\": \"\"\"\n",
    "                        You're tasked with evaluating the prompts and answers generated by an generative AI model. The input prompt is enclosed within the <input> tags, the output response within the <output> tags, the evaluation criteria for the prompt within <prompt_criteria> tags, and the criteria for the answer within <answer_criteria> tags.\n",
    "\n",
    "                        <input> {{input}} </input> <output> {{output}} </output>\n",
    "\n",
    "                        <prompt_criteria>\n",
    "                        The prompt must be clear, precise, and detailed.\n",
    "                        The prompt's objective should be well-defined, grammatically correct, and easy to understand.\n",
    "                        Including examples improves the quality of the prompt.\n",
    "                        Setting a role or providing context makes the prompt more effective.\n",
    "                        Adding details about the expected answer's format and tone is beneficial. \n",
    "                        </prompt_criteria>\n",
    "\n",
    "                        <answer_criteria>\n",
    "                        The answers must be accurate, well-structured, and technically complete.\n",
    "                        The answers must avoid hallucinations, fabricated content, or offensive language.\n",
    "                        The answers must adhere to proper grammar rules.\n",
    "                        The answers must align entirely with the instruction or question given in the prompt. \n",
    "                        </answer_criteria>\n",
    "\n",
    "                        Evaluate the response generated by the AI model within the <output> tags with a score ranging from 0 to 100, based on the <answer_criteria>. Even minor hallucinations should significantly affect the evaluation score. Similarly, assess the input prompt within the <input> tags using a score from 0 to 100 based on the <prompt_criteria>.\n",
    "\n",
    "                        Provide your evaluation as a JSON object containing:\n",
    "\n",
    "                        An 'answer-score' key with the answer's evaluation score.\n",
    "                        A 'prompt-score' key with the prompt's evaluation score.\n",
    "                        A 'justification' key with a detailed explanation for the scores, highlighting any specific issues such as hallucinations or grammatical errors.\n",
    "                        An 'input' key containing the content of the <input> tags.\n",
    "                        An 'output' key containing the content of the <output> tags.\n",
    "                        A 'prompt-recommendations' key with actionable suggestions for improving the prompt based on your evaluations.\n",
    "                        Ensure your response includes only the JSON object without any preamble or additional text.\n",
    "                        \"\"\"\n",
    "                    }\n",
    "                },\n",
    "                \"templateType\": \"TEXT\"\n",
    "            }\n",
    "        ],\n",
    "        defaultVariant=\"eval_variant\"\n",
    "    )\n",
    "\n",
    "    # Extract key details from the response\n",
    "    promptEvalId = response[\"id\"]\n",
    "    promptEvalArn = response[\"arn\"]\n",
    "    promptEvalName = response[\"name\"]\n",
    "\n",
    "    # Display the extracted details\n",
    "    print(f\"Prompt ID: {promptEvalId}\\nPrompt ARN: {promptEvalArn}\\nPrompt Name: {promptEvalName}\")\n",
    "\n",
    "    # Step 2: Create a new prompt version\n",
    "    version_response = boto3_bedrock_agent_client.create_prompt_version(\n",
    "        promptIdentifier=promptEvalId\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Extracting version from the versioned response\n",
    "    eval_versioned_version = version_response.get(\"version\", \"Unknown\")\n",
    "    print(f\"Versioned version: {eval_versioned_version}\")\n",
    "\n",
    "except NoCredentialsError:\n",
    "    print(\"Error: AWS credentials were not found. Please configure your credentials.\")\n",
    "except PartialCredentialsError:\n",
    "    print(\"Error: Incomplete AWS credentials provided. Check your configuration.\")\n",
    "except ClientError as e:\n",
    "    # Handle specific AWS client errors\n",
    "    print(f\"Client error occurred: {e.response['Error']['Message']}\")\n",
    "except Exception as e:\n",
    "    # Handle any other unexpected exceptions\n",
    "    print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1e5e1d-2087-4a4f-a76f-33075218503f",
   "metadata": {},
   "source": [
    "##### 1. Create a invoke prompt using Prompt management\n",
    "##### 2. Create a version of a invoke prompt in Prompt management  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05b56ae-647a-4f52-801c-f8d305f9cb2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Step 1: Create a new prompt\n",
    "    response = boto3_bedrock_agent_client.create_prompt(\n",
    "        name=invoke_prompt_name,\n",
    "        description=invoke_prompt_description,\n",
    "        variants=[\n",
    "            {\n",
    "                \"inferenceConfiguration\": {\n",
    "                    \"text\": {\n",
    "                        \"maxTokens\": 2000,\n",
    "                        \"temperature\": 0,\n",
    "                    }\n",
    "                },\n",
    "                \"modelId\": bedrock_model_invoke_id,\n",
    "                \"name\": \"invoke_variant\",\n",
    "                \"templateConfiguration\": {\n",
    "                    \"text\": {\n",
    "                        \"inputVariables\": [\n",
    "                            {\"name\": \"input\"}\n",
    "                        ],\n",
    "                        \"text\": \"{{input}}\"\n",
    "                    }\n",
    "                },\n",
    "                \"templateType\": \"TEXT\"\n",
    "            }\n",
    "        ],\n",
    "        defaultVariant=\"invoke_variant\"\n",
    "    )\n",
    "\n",
    "    # Extract key details from the response\n",
    "    promptInvokeId = response[\"id\"]\n",
    "    promptInvokeArn = response[\"arn\"]\n",
    "    promptInvokeName = response[\"name\"]\n",
    "\n",
    "    # Display the extracted details\n",
    "    print(f\"Prompt ID: {promptInvokeId}\\nPrompt ARN: {promptInvokeArn}\\nPrompt Name: {promptInvokeName}\")\n",
    "\n",
    "    # Step 2: Create a new prompt version\n",
    "    version_response = boto3_bedrock_agent_client.create_prompt_version(\n",
    "        promptIdentifier=promptInvokeId\n",
    "    )\n",
    "    \n",
    "    # Extracting version from the versioned response\n",
    "    invoke_versioned_version = version_response.get(\"version\", \"Unknown\")\n",
    "    print(f\"Versioned version: {invoke_versioned_version}\")\n",
    "\n",
    "except NoCredentialsError:\n",
    "    print(\"Error: AWS credentials were not found. Please configure your credentials.\")\n",
    "except PartialCredentialsError:\n",
    "    print(\"Error: Incomplete AWS credentials provided. Check your configuration.\")\n",
    "except ClientError as e:\n",
    "    # Handle specific AWS client errors\n",
    "    print(f\"Client error occurred: {e.response['Error']['Message']}\")\n",
    "except Exception as e:\n",
    "    # Handle any other unexpected exceptions\n",
    "    print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46f81bf-58d2-40dd-acd6-468adb6a4c92",
   "metadata": {},
   "source": [
    "# Create eval node \n",
    "#### Refer: Architecture diagram step (3) above \n",
    "\n",
    "##### Attaching eval prompt with evaluate node\n",
    "##### The create_evaluate_node() function defines a modular \"Evaluate\" node within a prompt flow. It constructs a dictionary representing the node's structure, including its name, type, input and output configurations, and a reference to a predefined prompt ARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd448814-3a6b-412d-b2d3-c2d5842e617e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to define the Evaluate node\n",
    "def create_evaluate_node():\n",
    "    try:\n",
    "        # Attempt to define the Evaluate node\n",
    "        evaluate_node = {\n",
    "            \"name\": \"Evaluate_Node\",  # The name of the node\n",
    "            \"type\": \"Prompt\",  # The type of the node, in this case, a prompt\n",
    "            \"configuration\": {\n",
    "                \"prompt\": {\n",
    "                    \"sourceConfiguration\": {\n",
    "                        \"resource\": {\n",
    "                            \"promptArn\": promptEvalArn  # Reference to the prompt ARN (Replace with your ARN)\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                    \"expression\": \"$.data\",  # The expression pointing to the input data\n",
    "                    \"name\": \"input\",  # The name of the input variable\n",
    "                    \"type\": \"String\"  # The type of the input data\n",
    "                },\n",
    "                {\n",
    "                    \"expression\": \"$.data\",  # Another expression pointing to the output data from previous nodes\n",
    "                    \"name\": \"output\",  # The name of the output variable\n",
    "                    \"type\": \"String\"  # The type of the output data\n",
    "                }\n",
    "            ],\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"modelCompletion\",  # The name of the output from this node\n",
    "                    \"type\": \"String\"  # The type of the output (string in this case)\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        # Return the created evaluate node structure\n",
    "        return evaluate_node\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch any errors that occur while defining the node\n",
    "        print(f\"An error occurred while creating the Evaluate node: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40da206-3389-4d0c-8fea-64ad38a49044",
   "metadata": {},
   "source": [
    "# Create invoke node \n",
    "#### Refer: Architecture diagram step (2) above \n",
    "\n",
    "##### Attaching invoke prompt with invoke node\n",
    "##### The create_invoke_node() function defines a modular \"Invoke\" node within a prompt flow, which invokes a model using a predefined prompt ARN (promptInvokeArn). It accepts input data in the form of a string, processes it through the model, and outputs the model's response as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff96f34-be63-45b6-a007-232f894c71aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to define the Invoke node\n",
    "def create_invoke_node():\n",
    "    \"\"\"\n",
    "    # Create Invoke Node for Prompt-based Interaction\n",
    "    #### Purpose: This function defines an 'Invoke' node that will be part of a larger prompt flow.\n",
    "    #### It includes configuration for a Prompt node that utilizes a predefined Prompt ARN (promptInvokeArn).\n",
    "    #### The node is responsible for taking input data, invoking a model, and providing output data.\n",
    "    \n",
    "    ##### Workflow:\n",
    "    1. The `Invoke_Node` uses a `Prompt` type to invoke an LLM-based model.\n",
    "    2. The node references an existing prompt using its `promptArn`.\n",
    "    3. It takes input from the `Start` node and sends output to the next node in the flow.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Return the structure of the Invoke node\n",
    "        return {\n",
    "            \"name\": \"Invoke_Node\",  # Name of the node within the flow\n",
    "            \"type\": \"Prompt\",  # Node type, indicating it invokes a prompt (LLM model)\n",
    "            \"configuration\": {\n",
    "                \"prompt\": {\n",
    "                    \"sourceConfiguration\": {\n",
    "                        # Referencing an existing prompt via its ARN\n",
    "                        \"resource\": {\n",
    "                            \"promptArn\": promptInvokeArn  # Replace with actual ARN of the prompt\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                    \"expression\": \"$.data\",  # Expression to access input data from previous nodes\n",
    "                    \"name\": \"input\",  # Name of the input variable expected by the model\n",
    "                    \"type\": \"String\"  # Data type for input\n",
    "                }\n",
    "            ],\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"modelCompletion\",  # Output name of the model's response\n",
    "                    \"type\": \"String\"  # Data type for output (in this case, a string response from the model)\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Catch any exceptions during node creation and print an error message\n",
    "        print(f\"An error occurred while creating the Invoke node: {e}\")\n",
    "        return None  # Returning None if an error occurs during node creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4358681-d364-4706-9abb-4d406e70c45e",
   "metadata": {},
   "source": [
    "# Create connections between nodes\n",
    "#### Refer: Architecture diagram above \n",
    "\n",
    "##### The create_connections() function defines the data flow connections between nodes in a prompt flow. It specifies three main connections: from the \"Start\" node to the \"Invoke\" node, from the \"Invoke\" node to the \"Evaluate\" node, and from the \"Evaluate\" node to the \"End\" node. Each connection defines the source and target nodes, along with the data mapping between them, specifying how the output from one node is passed as input to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c588ce8-c432-4fb0-95cd-30a493ba5ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to define the connections between nodes\n",
    "def create_connections():\n",
    "    \"\"\"\n",
    "    This function defines the connections between nodes in a modular prompt flow.\n",
    "    Each connection maps the output of one node to the input of another, enabling data flow between components.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries representing the connections between nodes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connection from Start_Node to Invoke_Node\n",
    "        start_to_invoke = {\n",
    "            \"name\": \"start_2_invoke_connection\",\n",
    "            \"source\": \"Start_Node\",\n",
    "            \"target\": \"Invoke_Node\",\n",
    "            \"type\": \"Data\",\n",
    "            \"configuration\": {\n",
    "                \"data\": {\n",
    "                    \"sourceOutput\": \"document\",\n",
    "                    \"targetInput\": \"input\"\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "        # Connection from Invoke_Node to Evaluate_Node\n",
    "        invoke_to_evaluate = {\n",
    "            \"name\": \"invoke_2_evaluate_connection\",\n",
    "            \"source\": \"Invoke_Node\",\n",
    "            \"target\": \"Evaluate_Node\",\n",
    "            \"type\": \"Data\",\n",
    "            \"configuration\": {\n",
    "                \"data\": {\n",
    "                    \"sourceOutput\": \"modelCompletion\",\n",
    "                    \"targetInput\": \"output\"\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "        # Connection from Start_Node to Evaluate_Node\n",
    "        start_to_evaluate = {\n",
    "            \"name\": \"start_2_evaluate_connection\",\n",
    "            \"source\": \"Start_Node\",\n",
    "            \"target\": \"Evaluate_Node\",\n",
    "            \"type\": \"Data\",\n",
    "            \"configuration\": {\n",
    "                \"data\": {\n",
    "                    \"sourceOutput\": \"document\",\n",
    "                    \"targetInput\": \"input\"\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "        # Connection from Evaluate_Node to End_Node\n",
    "        evaluate_to_end = {\n",
    "            \"name\": \"evaluate_2_end_connection\",\n",
    "            \"source\": \"Evaluate_Node\",\n",
    "            \"target\": \"End_Node\",\n",
    "            \"type\": \"Data\",\n",
    "            \"configuration\": {\n",
    "                \"data\": {\n",
    "                    \"sourceOutput\": \"modelCompletion\",\n",
    "                    \"targetInput\": \"document\"\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "        \n",
    "        # Returning the list of all connections\n",
    "        return [start_to_invoke, invoke_to_evaluate, start_to_evaluate, evaluate_to_end]\n",
    "    except Exception as e:\n",
    "        # Handling any exceptions that occur during connection creation\n",
    "        print(f\"Error in creating connections: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe535c0-7e1c-4827-9b03-5c419a6290e1",
   "metadata": {},
   "source": [
    "# Create prompt flow with nodes and connections\n",
    "#### Refer: Architecture diagram above \n",
    "\n",
    "##### The create_flow() function is responsible for defining and creating a prompt flow using the AWS Bedrock service. It first constructs a flow definition that includes node configurations (Start, End, Invoke, and Evaluate nodes) and the data flow connections between them. The function attempts to create the flow by calling the create_flow API from the Bedrock agent client, and upon successful creation, it extracts and prints the flow's ID, ARN, and name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d24b8f-aa32-4c59-a0c9-3727e641b63f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to create the flow\n",
    "def create_flow():\n",
    "    try:\n",
    "        # Create the flow definition with nodes and connections\n",
    "        flow_definition = {\n",
    "            \"name\": flow_name,\n",
    "            \"description\": flow_description,\n",
    "            \"executionRoleArn\": flows_service_role,\n",
    "            \"definition\": {\n",
    "                \"nodes\": [\n",
    "                    create_start_node(),\n",
    "                    create_end_node(),\n",
    "                    create_invoke_node(),\n",
    "                    create_evaluate_node()\n",
    "                ],\n",
    "                \"connections\": create_connections()\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "        # Create the flow\n",
    "        response = boto3_bedrock_agent_client.create_flow(**flow_definition)\n",
    "\n",
    "        # Handle the response\n",
    "        flowEvalId = response[\"id\"]\n",
    "        flowEvalArn = response[\"arn\"]\n",
    "        flowEvalName = response[\"name\"]\n",
    "        print(f\"Flow ID: {flowEvalId}\\nFlow ARN: {flowEvalArn}\\nFlow Name: {flowEvalName}\")\n",
    "\n",
    "        # Returning the flow response for further usage if necessary\n",
    "        return ( flowEvalId, flowEvalArn, flowEvalName )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while creating the flow: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Ensure the flow creation result is valid before unpacking\n",
    "flow_response = create_flow()\n",
    "\n",
    "if flow_response:\n",
    "    flowEvalId, flowEvalArn, flowEvalName = flow_response\n",
    "else:\n",
    "    print(\"Flow creation failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a734b9-b0b1-4cab-8566-d74e9000f81d",
   "metadata": {},
   "source": [
    "# Prepare the prompt flow and find out the status of the prompt flow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4480b471-81ce-4fd7-b2f8-0ece962fef75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to prepare a flow and check its status\n",
    "def prepare_flow(flow_eval_id):\n",
    "    try:\n",
    "        # Prepare the flow using the provided flow ID\n",
    "        response = boto3_bedrock_agent_client.prepare_flow(\n",
    "            flowIdentifier=flow_eval_id\n",
    "        )\n",
    "        \n",
    "        # Extract and print the flow status\n",
    "        flow_status = response[\"status\"]\n",
    "        print(f\"Flow ID: {flow_eval_id}\\nStatus: {flow_status}\")\n",
    "        \n",
    "        # Return the flow status for further use\n",
    "        return flow_status\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle and log any errors that occur during the preparation\n",
    "        print(f\"An error occurred while preparing the flow: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "flow_status = prepare_flow(flowEvalId)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba144fc5-6120-465d-bee5-f3dabb72b13d",
   "metadata": {},
   "source": [
    "# Create prompt flow version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5796c463-7d4b-444e-877e-3954884f1ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to create a new flow version using the Bedrock agent\n",
    "try:\n",
    "    # Attempt to create a new version of the flow using the provided flowEvalId\n",
    "    response = boto3_bedrock_agent_client.create_flow_version(\n",
    "            flowIdentifier=flowEvalId\n",
    "        )\n",
    "    \n",
    "    status_response = response[\"status\"]\n",
    "    status_version = response[\"version\"]\n",
    "        \n",
    "    print(f\"Version: {status_version}\\nStatus: {status_response}\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle any exceptions that occur during the flow version creation\n",
    "    print(f\"Error occurred while creating flow version for flow ID {flowEvalId}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f962d31a-aed5-4b90-95c6-0402cd428f2e",
   "metadata": {},
   "source": [
    "# Create prompt alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4355f0a9-c452-4d6a-a8a3-4c75332c7580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to create an alias for a flow\n",
    "try:\n",
    "    # Attempt to create an alias for the flow\n",
    "    response = boto3_bedrock_agent_client.create_flow_alias(\n",
    "        flowIdentifier=flowEvalId,\n",
    "        name=flowEvalName,\n",
    "        description=\"Alias Flow_iPhone_accessories_recommendation\",\n",
    "        routingConfiguration=[\n",
    "            {\n",
    "                \"flowVersion\": \"1\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Print the response in a readable JSON format\n",
    "    print(json.dumps(response, indent=2, default=str))\n",
    "\n",
    "    # Extract the alias ID from the response\n",
    "    flowEvalAliasId = response['id']\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle any exceptions that occur during alias creation\n",
    "    print(f\"Error occurred while creating alias for flow ID {flowEvalId}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38630eca-1a42-48c2-8ae7-1b106dd31ab5",
   "metadata": {},
   "source": [
    "# Testing the prompt flow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2fceb7-3f4b-4d52-8b5d-085499acecfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def execute_flow(prompt):\n",
    "    \"\"\"\n",
    "    Function to execute a Bedrock flow using a prompt and process the streamed response.\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The prompt to be sent to the flow.\n",
    "\n",
    "    Returns:\n",
    "        None: The response content is printed during the streaming process.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure flow and alias identifiers are defined\n",
    "        if not flowEvalId or not flowEvalAliasId:\n",
    "            raise ValueError(\"Flow ID or Flow Alias ID is not defined. Ensure they are initialized.\")\n",
    "\n",
    "        # Invoke the flow with the provided prompt\n",
    "        response = boto3_bedrock_agent_runtime_client.invoke_flow(\n",
    "            flowIdentifier=flowEvalId,\n",
    "            flowAliasIdentifier=flowEvalAliasId,\n",
    "            inputs=[\n",
    "                { \n",
    "                    \"content\": { \n",
    "                        \"document\": prompt  # Pass the prompt to the flow\n",
    "                    },\n",
    "                    \"nodeName\": \"Start_Node\",  # Match the node name in the flow definition\n",
    "                    \"nodeOutputName\": \"document\"  # Match the output name from the Start node\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Stream the response events\n",
    "        event_stream = response[\"responseStream\"]\n",
    "        print(\"Processing flow response stream...\")\n",
    "\n",
    "        # Iterate through each event in the stream\n",
    "        for event in event_stream:\n",
    "            # Check for flow output events and print the document content\n",
    "            if \"flowOutputEvent\" in event and \"content\" in event[\"flowOutputEvent\"]:\n",
    "                document = event[\"flowOutputEvent\"][\"content\"].get(\"document\", \"No document found\")\n",
    "                print(f\"Flow Output: {document}\")\n",
    "            else:\n",
    "                print(\"Unexpected event format:\", event)\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: Missing expected key in response or event: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during flow execution: {e}\")\n",
    "\n",
    "# Test the function with a prompt\n",
    "prompt = \"Recommend three popular accessories under $75 for a school student who recently bought an iPhone.\"\n",
    "execute_flow(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7210d989-71d0-4c91-99a9-6ff8f7ca4ee4",
   "metadata": {},
   "source": [
    "# Clean up steps \n",
    "\n",
    "#### deleting resources associated with an AWS Bedrock prompt flow, including aliases, versions, the flow itself, and associated prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d230c1-e3f3-426a-a9d9-d9759e636b1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_flow_alias(flow_id, alias_id):\n",
    "    \"\"\"\n",
    "    Deletes a specific flow alias.\n",
    "\n",
    "    Parameters:\n",
    "        flow_id (str): Identifier for the flow.\n",
    "        alias_id (str): Identifier for the alias to delete.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Deleting prompt flow alias...\")\n",
    "        response = boto3_bedrock_agent_client.delete_flow_alias(\n",
    "            flowIdentifier=flow_id,\n",
    "            aliasIdentifier=alias_id\n",
    "        )\n",
    "        print(json.dumps(response, indent=2, default=str))\n",
    "        print(\"Prompt flow alias deletion completed.\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while deleting prompt flow alias: {e}\\n\")\n",
    "\n",
    "def delete_flow_version(flow_id, version):\n",
    "    \"\"\"\n",
    "    Deletes a specific version of a flow.\n",
    "\n",
    "    Parameters:\n",
    "        flow_id (str): Identifier for the flow.\n",
    "        version (str): Version of the flow to delete.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Deleting prompt flow version...\")\n",
    "        response = boto3_bedrock_agent_client.delete_flow_version(\n",
    "            flowIdentifier=flow_id,\n",
    "            flowVersion=version\n",
    "        )\n",
    "        print(json.dumps(response, indent=2, default=str))\n",
    "        print(\"Prompt flow version deletion completed.\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while deleting prompt flow version: {e}\\n\")\n",
    "\n",
    "def delete_flow(flow_id):\n",
    "    \"\"\"\n",
    "    Deletes an entire flow.\n",
    "\n",
    "    Parameters:\n",
    "        flow_id (str): Identifier for the flow.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Deleting prompt flow...\")\n",
    "        response = boto3_bedrock_agent_client.delete_flow(\n",
    "            flowIdentifier=flow_id\n",
    "        )\n",
    "        print(json.dumps(response, indent=2, default=str))\n",
    "        print(\"Prompt flow deletion completed.\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while deleting prompt flow: {e}\\n\")\n",
    "\n",
    "def delete_prompt(prompt_id, prompt_type):\n",
    "    \"\"\"\n",
    "    Deletes a specific prompt.\n",
    "\n",
    "    Parameters:\n",
    "        prompt_id (str): Identifier for the prompt to delete.\n",
    "        prompt_type (str): Type of the prompt ('invoke' or 'eval') for descriptive purposes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Deleting {prompt_type} prompt...\")\n",
    "        response = boto3_bedrock_agent_client.delete_prompt(\n",
    "            promptIdentifier=prompt_id\n",
    "        )\n",
    "        print(json.dumps(response, indent=2, default=str))\n",
    "        print(f\"{prompt_type.capitalize()} prompt deletion completed.\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while deleting {prompt_type} prompt: {e}\\n\")\n",
    "\n",
    "# Main Execution\n",
    "try:\n",
    "\n",
    "    delete_flow_alias(flowEvalId, flowEvalAliasId)\n",
    "    delete_flow_version(flowEvalId, \"1\")\n",
    "    delete_flow(flowEvalId)\n",
    "    delete_prompt(promptInvokeId, \"invoke\")\n",
    "    delete_prompt(promptEvalId, \"eval\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during the cleanup process: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51745d43-360d-42a1-8617-a7a3a2769ab8",
   "metadata": {},
   "source": [
    "# End of NoteBook \n",
    "\n",
    "## Please ensure that you close the kernel after using this notebook to avoid any potential charges to your account.\n",
    "\n",
    "## Process: Go to \"Kernel\" at top option. Choose \"Shut Down Kernel\". \n",
    "##### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/studio-ui.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872612a2-f79b-4d8a-bbd7-9821210ded9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
