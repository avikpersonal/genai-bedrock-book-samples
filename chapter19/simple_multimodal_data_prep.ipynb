{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed3a60a-5001-46ff-ae70-61d861a00c53",
   "metadata": {
    "tags": []
   },
   "source": [
    "# File Name: simple_multimodal_data_prep.ipynb\n",
    "### Location: Chapter 19\n",
    "### Purpose: \n",
    "#####       1. Processing a dataset of images, saving them to a specified directory while generating and storing metadata descriptions.\n",
    "#####       2. Generates multimodal embeddings by accepting an image or text description, processes the input, and invokes a model via the Bedrock runtime client, returning the resulting embeddings.\n",
    "#####       3. Adds multimodal embeddings to each dictionary in image_metadata_list.\n",
    "##### Dependency: simple-sageMaker-bedrock.ipynb at Chapter 3 should work properly.\n",
    "# <ins>-----------------------------------------------------------------------------------</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99464e3-0679-4a67-bb38-8d96892eefa5",
   "metadata": {},
   "source": [
    "# <ins>Amazon SageMaker Classic</ins>\n",
    "#### Those who are new to Amazon SageMaker Classic. Follow the link for the details. https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91003ca1-0026-46eb-bd09-81861135cb22",
   "metadata": {},
   "source": [
    "# <ins>Environment setup of Kernel</ins>\n",
    "##### Fill \"Image\" as \"Data Science\"\n",
    "##### Fill \"Kernel\" as \"Python 3\"\n",
    "##### Fill \"Instance type\" as \"ml-t3-medium\"\n",
    "##### Fill \"Start-up script\" as \"No Scripts\"\n",
    "##### Click \"Select\"\n",
    "\n",
    "###### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-create-open.html for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865b7f5-755a-498c-bff6-7647281b15b9",
   "metadata": {},
   "source": [
    "# <ins>Mandatory installation on the kernel through pip</ins>\n",
    "\n",
    "##### This lab will work with below software version. But, if you are trying with latest version of boto3, awscli, and botocore. This code may fail. You might need to change the corresponding api. \n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade0112-a4be-4ce8-be92-8cbf28b2acfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall -q \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\" \\\n",
    "    \"utils\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e3b85-4e64-42ef-9355-ed2a61c68b23",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins>\n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ee934-e99c-49ce-9e94-27ffc5baabe1",
   "metadata": {},
   "source": [
    "# <ins>Restart the kernel</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf41bf-d2d2-46fa-b1d4-1dd19c45a069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5af76d-b2a9-4740-b8bf-134932804719",
   "metadata": {},
   "source": [
    "# <ins>Python package import</ins>\n",
    "\n",
    "##### boto3 offers various clients for Amazon Bedrock to execute various actions.\n",
    "##### botocore is a low-level interface to AWS tools, while boto3 is built on top of botocore and provides additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7268959-6ef0-4cbd-bd53-3b31369b4513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "import warnings\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import base64\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from utils import *\n",
    "from datasets import load_dataset\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1b877-deab-4961-a249-7663e812fbb8",
   "metadata": {},
   "source": [
    "### Ignore warning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71587271-9f02-46dc-9eaf-5a5291132f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a707d91-23c2-4786-bd2a-6287ca51ef82",
   "metadata": {},
   "source": [
    "## Define important environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4247b5-5fe7-4ec8-9f25-bff782734606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try-except block to handle potential errors\n",
    "try:\n",
    "    # Create a new Boto3 session to interact with AWS services\n",
    "    # This session is responsible for managing credentials and region configuration\n",
    "    boto3_session = boto3.session.Session()\n",
    "\n",
    "    # Retrieve the current AWS region from the session (e.g., 'us-east-1', 'us-west-2')\n",
    "    aws_region_name = boto3_session.region_name\n",
    "    \n",
    "    # Initialize Bedrock and Bedrock Runtime clients using Boto3\n",
    "    # These clients will allow interactions with Bedrock-related AWS services\n",
    "    boto3_bedrock_client = boto3.client('bedrock', region_name=aws_region_name)\n",
    "    boto3_bedrock_runtime_client = boto3.client('bedrock-runtime', region_name=aws_region_name)\n",
    "    \n",
    "    # Create a SageMaker session and retrieve the execution role ARN\n",
    "    # The role ARN is used to authorize SageMaker to perform tasks on behalf of the user\n",
    "    sagemaker_role_arn = sagemaker.get_execution_role()\n",
    "    \n",
    "    # Select Amazon titan-embed-image-v1 as Embedding model for multimodal indexing\n",
    "    multimodal_embed_model_id = \"amazon.titan-embed-image-v1\"\n",
    "\n",
    "    # Store all relevant variables in a dictionary for easier access and management\n",
    "    variables_store = {\n",
    "        \"aws_region_name\": aws_region_name,                          # AWS region name\n",
    "        \"boto3_bedrock_client\": boto3_bedrock_client,                # Bedrock client instance\n",
    "        \"boto3_bedrock_runtime_client\": boto3_bedrock_runtime_client,  # Bedrock Runtime client instance\n",
    "        \"boto3_session\": boto3_session,                               # Current Boto3 session object\n",
    "        \"sagemaker_role_arn\" : sagemaker_role_arn,\n",
    "        \"multimodal_embed_model_id\": multimodal_embed_model_id\n",
    "    }\n",
    "\n",
    "    # Print all stored variables for debugging and verification\n",
    "    for var_name, value in variables_store.items():\n",
    "        print(f\"{var_name}: {value}\")\n",
    "\n",
    "# Handle any exceptions that occur during the execution\n",
    "except Exception as e:\n",
    "    # Print the error message if an unexpected error occurs\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b51077b",
   "metadata": {},
   "source": [
    "Vector DB Creation Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81140ad0",
   "metadata": {},
   "source": [
    "<img src=\"./vector_db_arc_diagram.png\" style=\"width: 600px; height: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674cb33d-c1f3-4b22-806a-cc13983dcc33",
   "metadata": {},
   "source": [
    "# Datset for this use cases\n",
    "\n",
    "### Refer: https://huggingface.co/datasets/ashraq/fashion-product-images-small\n",
    "\n",
    "### Metadata\n",
    "\n",
    "    dataset_info:\n",
    "      features:\n",
    "        - name: id\n",
    "          dtype: int64\n",
    "        - name: gender\n",
    "          dtype: string\n",
    "        - name: masterCategory\n",
    "          dtype: string\n",
    "        - name: subCategory\n",
    "          dtype: string\n",
    "        - name: articleType\n",
    "          dtype: string\n",
    "        - name: baseColour\n",
    "          dtype: string\n",
    "        - name: season\n",
    "          dtype: string\n",
    "        - name: year\n",
    "          dtype: float64\n",
    "        - name: usage\n",
    "          dtype: string\n",
    "        - name: productDisplayName\n",
    "          dtype: string\n",
    "        - name: image\n",
    "          dtype: image\n",
    "      splits:\n",
    "        - name: train\n",
    "          num_bytes: 546202015.44\n",
    "          num_examples: 44072\n",
    "      download_size: 271496441\n",
    "      dataset_size: 546202015.44\n",
    "    Dataset Card for \"fashion-p\n",
    "\n",
    "\n",
    "### Note: We will only use 200 random data from this datasets to save the computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da4146-3b5d-46ee-9fa6-f37d3f70f39f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fashion_product_images = load_dataset(\"ashraq/fashion-product-images-small\")\n",
    "\n",
    "# metadata check\n",
    "fashion_product_images\n",
    "\n",
    "# look one sample data\n",
    "fashion_product_images[\"train\"][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d21e759-5d85-4097-aefb-f83088f24b93",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Function to display the image\n",
    "\n",
    "    The display_image function is designed to handle image validation, resizing, saving, and displaying with robust error handling. \n",
    "    It first checks if the provided image is valid and meets the minimum size requirement. \n",
    "    If the image passes the checks, it is resized to the specified target size and saved to a file. \n",
    "    The function then displays the saved image inline and prints a message with details about the original and resized image sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac45bdf-0a06-41a2-b852-d2489e09d54f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def display_image(image, min_size=(64, 64), target_size=(128, 128), save_path=\"resized_image.png\"):\n",
    "    try:\n",
    "        if image is None:\n",
    "            raise ValueError(\"No image provided.\")\n",
    "        \n",
    "        if not isinstance(image, Image.Image):\n",
    "            raise TypeError(\"The input is not a valid PIL Image instance.\")\n",
    "        \n",
    "        if image.size[0] < min_size[0] or image.size[1] < min_size[1]:\n",
    "            raise ValueError(\n",
    "                f\"Image size {image.size} is smaller than the minimum required size {min_size}.\"\n",
    "            )\n",
    "        \n",
    "        # Resize the image\n",
    "        resized_image = image.resize(target_size)\n",
    "        \n",
    "        # Save resized image\n",
    "        resized_image.save(save_path)\n",
    "        print(f\"Image resized and saved successfully. Original size: {image.size}, Resized size: {resized_image.size}\")\n",
    "        \n",
    "        # Display inline\n",
    "        saved_image = Image.open(save_path)\n",
    "        display(saved_image)\n",
    "        print(f\"Image displayed inline from saved file: {save_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying image: {e}\")\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "display_image(fashion_product_images[\"train\"][10][\"image\"], min_size=fashion_product_images[\"train\"][10][\"image\"].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c829b95a-2081-4f76-a19a-f63bce96a669",
   "metadata": {},
   "source": [
    "# Processing a dataset of images, saving them to a specified directory while generating and storing metadata descriptions\n",
    "\n",
    "    The below code provides a modular approach to processing and saving images from a dataset. \n",
    "    It includes three main functions: save_image, generate_description, and process_images. \n",
    "    The save_image function ensures proper error handling and validation before saving the image to the specified path. \n",
    "    The generate_description function formats and returns a string description from the metadata associated with each image, including attributes like gender, category, color, and season. \n",
    "    The process_images function iterates through a subset of the dataset, saving each image and generating its metadata description. \n",
    "\n",
    "##### The example usage processes a limited number of images (200) for efficiency, storing them in the downloaded_images folder. You can run this file with all the dataset. Be aware of the cost of execution for entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df48eb-10c3-4de2-8115-1accd41e177c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def save_image(image, image_path):\n",
    "    \"\"\"\n",
    "    Saves an image to the specified path with error handling.\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image.Image): The image to save.\n",
    "        image_path (str): The path to save the image.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the image or path is invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if image is None:\n",
    "            raise ValueError(\"No image provided for saving.\")\n",
    "        if not isinstance(image, Image.Image):\n",
    "            raise TypeError(\"Provided image is not a valid PIL Image instance.\")\n",
    "        \n",
    "        image.save(image_path)\n",
    "        print(f\"Image saved successfully to {image_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving image: {e}\")\n",
    "        raise\n",
    "\n",
    "def generate_description(item):\n",
    "    \"\"\"\n",
    "    Generates a description string from the dataset item.\n",
    "    \n",
    "    Args:\n",
    "        item (dict): A dictionary containing image metadata.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted description string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return (f\"gender: {item['gender']}, master_category: {item['masterCategory']}, \"\n",
    "                f\"sub_category: {item['subCategory']}, article_type: {item['articleType']}, \"\n",
    "                f\"base_colour: {item['baseColour']}, season: {item['season']}, \"\n",
    "                f\"year: {int(item['year'])}, usage: {item['usage']}, productDisplayName: {item['productDisplayName']}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing key in dataset item: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating description: {e}\")\n",
    "        raise\n",
    "\n",
    "def process_images(dataset, output_dir, max_images=100):\n",
    "    \"\"\"\n",
    "    Processes images from the dataset and saves them to the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        dataset (list): A list of image data items.\n",
    "        output_dir (str): The directory to save images.\n",
    "        max_images (int): The maximum number of images to process.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries containing image metadata and file paths.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"Output directory created: {output_dir}\")\n",
    "\n",
    "        image_data_list = []\n",
    "\n",
    "        for index, item in enumerate(dataset.select(range(max_images))):\n",
    "            try:\n",
    "                image_id = item[\"id\"]\n",
    "                image = item[\"image\"]\n",
    "                image_filename = f\"image_{image_id}.jpg\"\n",
    "                image_path = os.path.join(output_dir, image_filename)\n",
    "                \n",
    "                # Save image\n",
    "                save_image(image, image_path)\n",
    "                \n",
    "                # Generate description\n",
    "                description = generate_description(item)\n",
    "                \n",
    "                # Append metadata to the list\n",
    "                image_data_list.append({\n",
    "                    \"ID\": image_id,\n",
    "                    \"Description\": description,\n",
    "                    \"Image_path\": image_path\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {index + 1}: {e}\")\n",
    "        \n",
    "        print(f\"Processed {len(image_data_list)} images successfully.\")\n",
    "        return image_data_list\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "output_directory = \"downloaded_images\"\n",
    "max_images=200 # taking 200 sample data to reduce computational time. You can consider all the dataset. \n",
    "\n",
    "try:\n",
    "    image_metadata_list = process_images(fashion_product_images[\"train\"], output_directory, max_images)\n",
    "    print()\n",
    "    print()\n",
    "    print(\"First image metadata:\", image_metadata_list[0])\n",
    "except Exception as e:\n",
    "    print(f\"Unhandled error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e37286-35ac-4528-bc46-5b7f88cdf548",
   "metadata": {},
   "source": [
    "# Generates multimodal embeddings by accepting an image or text description, processes the input, and invokes a model via the Bedrock runtime client, returning the resulting embeddings\n",
    "\n",
    "##### The get_titan_multimodal_embedding function is designed to generate multimodal embeddings by accepting either an image or a text description as input. The function supports custom image dimensions and embedding sizes, with the default dimension set to 1024. It first checks if an image path is provided and ensures the image exists; if so, it reads and encodes the image in base64 before adding it to the payload. If a description is provided, it is added to the payload as well. The function requires either an image or a text description for the request and raises an error if neither is provided. The payload, including the embedding configuration, is sent to the model using the Bedrock runtime client, and the model response is returned as a parsed JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12786dfa-e5f1-4b6f-9759-ea6f197e49cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def get_titan_multimodal_embedding(\n",
    "    image_path: str = None,  # Maximum image dimensions: 2048 x 2048 pixels\n",
    "    description: str = None,  # Text description in English (max 128 tokens)\n",
    "    dimension: int = 1024,  # Desired embedding dimension (default 1024, other options: 384, 256)\n",
    "    model_id: str = multimodal_embed_model_id  # Predefined model ID for the multimodal embedding\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to obtain multimodal embeddings by providing either an image or a text description.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file (optional).\n",
    "        description (str): Text description for embedding (optional).\n",
    "        dimension (int): The dimensionality of the embedding output (default is 1024).\n",
    "        model_id (str): Model identifier for the multimodal embedding model.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The response from the Bedrock model containing the multimodal embeddings.\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If the image file does not exist at the given path.\n",
    "        AssertionError: If neither image nor description is provided.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the payload to send to the model\n",
    "    payload_body = {}\n",
    "\n",
    "    # Embedding configuration with the specified output dimension\n",
    "    embedding_config = {\n",
    "        \"embeddingConfig\": { \n",
    "            \"outputEmbeddingLength\": dimension\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Process image input if provided\n",
    "    if image_path:\n",
    "        # Check if the provided image path exists locally\n",
    "        if os.path.exists(image_path):\n",
    "            # Open the image file in binary mode and encode it in base64\n",
    "            with open(image_path, \"rb\") as image_file:\n",
    "                encoded_image = base64.b64encode(image_file.read()).decode('utf8')\n",
    "            # Add the base64 encoded image to the payload\n",
    "            payload_body[\"inputImage\"] = encoded_image\n",
    "        else:\n",
    "            # Raise an error if the image file does not exist\n",
    "            raise FileNotFoundError(f\"The image file at {image_path} does not exist.\")\n",
    "    \n",
    "    # Process text description input if provided\n",
    "    if description:\n",
    "        payload_body[\"inputText\"] = description\n",
    "\n",
    "    # Ensure that either image or text is provided for the request\n",
    "    assert payload_body, \"Please provide either an image and/or a text description.\"\n",
    "\n",
    "    try:\n",
    "        # Invoke the model using the Bedrock runtime client to get multimodal embeddings\n",
    "        response = boto3_bedrock_runtime_client.invoke_model(\n",
    "            body=json.dumps({**payload_body, **embedding_config}), \n",
    "            modelId=model_id,\n",
    "            accept=\"application/json\", \n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "        # Return the parsed JSON response from the model\n",
    "        return json.loads(response.get(\"body\").read())\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that might occur during the model invocation\n",
    "        print(f\"An error occurred while invoking the model: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage: Processing a list of image metadata to obtain embeddings\n",
    "multimodal_embeddings_img = []\n",
    "\n",
    "# Assuming image_metadata_list contains metadata with image paths\n",
    "for idx, image_metadata in enumerate(image_metadata_list):\n",
    "    # Get the multimodal embedding for each image\n",
    "    embedding = get_titan_multimodal_embedding(image_path=image_metadata[\"Image_path\"], dimension=1024)\n",
    "    # Store the obtained embedding\n",
    "    multimodal_embeddings_img.append(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ae17ed-d30e-4f33-a2e6-3e10c27542b1",
   "metadata": {},
   "source": [
    "# The code adds multimodal embeddings to each dictionary in image_metadata_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf49b851-e2b9-44eb-9c8f-9c3a0c6ef6e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "    for idx, item in enumerate(image_metadata_list):\n",
    "        # Ensure the index exists in the multimodal_embeddings_img list\n",
    "        if idx < len(multimodal_embeddings_img):\n",
    "            item[\"embedding_img\"] = multimodal_embeddings_img[idx][\"embedding\"]\n",
    "        else:\n",
    "            raise IndexError(f\"Index {idx} out of range in multimodal_embeddings_img.\")\n",
    "    \n",
    "    # Example: Print the first entry with the embedding\n",
    "    print(image_metadata_list[0])\n",
    "\n",
    "except IndexError as e:\n",
    "    # Handle case where the index is out of range\n",
    "    print(f\"IndexError: {e}\")\n",
    "\n",
    "except KeyError as e:\n",
    "    # Handle case where expected keys are missing in the dictionaries\n",
    "    print(f\"KeyError: Missing key {e} in the item dictionary.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Catch any other unforeseen errors\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77484f74-d653-4f2d-bda1-ea9bd939feaf",
   "metadata": {},
   "source": [
    "# Plot similarity heatmap\n",
    "\n",
    "###### The function plot_similarity_heatmap is used to visualize the similarity between two sets of embeddings using a heatmap. The heatmap represents the inner product (dot product) between the two sets of embeddings, which is commonly used to measure the similarity between vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b78ca48-d28f-4eaf-992f-1b11191c3a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def plot_similarity_heatmap(embeddings_set_a, embeddings_set_b):\n",
    "    \"\"\"\n",
    "    Function to plot a heatmap showing the similarity between two sets of embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings_set_a (list or np.array): First set of embeddings (e.g., image or text embeddings).\n",
    "        embeddings_set_b (list or np.array): Second set of embeddings (e.g., image or text embeddings).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If embeddings are not 2D arrays or are empty.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure embeddings are numpy arrays (and are 2D)\n",
    "        embeddings_set_a = np.array(embeddings_set_a)\n",
    "        embeddings_set_b = np.array(embeddings_set_b)\n",
    "\n",
    "        # Check if embeddings are 2D arrays\n",
    "        if embeddings_set_a.ndim != 2 or embeddings_set_b.ndim != 2:\n",
    "            raise ValueError(\"Both embeddings must be 2D arrays.\")\n",
    "        \n",
    "        # Compute the inner product (dot product) between the embeddings\n",
    "        similarity_matrix = np.inner(embeddings_set_a, embeddings_set_b)\n",
    "\n",
    "        # Create a heatmap to visualize the similarity\n",
    "        sns.set(font_scale=1.1)\n",
    "        plt.figure(figsize=(10, 8))  # Optional: Adjust the figure size for better readability\n",
    "        heatmap = sns.heatmap(\n",
    "            similarity_matrix,\n",
    "            vmin=np.min(similarity_matrix),\n",
    "            vmax=1,\n",
    "            cmap=\"OrRd\",\n",
    "            cbar_kws={'label': 'Similarity'}\n",
    "        )\n",
    "        plt.title('Embedding Similarity Heatmap')\n",
    "        plt.show()\n",
    "\n",
    "    except ValueError as ve:\n",
    "        # Handle invalid data format for embeddings (not 2D)\n",
    "        print(f\"Error: {ve}\")\n",
    "    except Exception as e:\n",
    "        # Handle any other unexpected errors\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage (assuming 'embedding_img' contains embeddings)\n",
    "try:\n",
    "    \n",
    "    image_metadata_list_df = pd.DataFrame(image_metadata_list)\n",
    "    \n",
    "    # Ensure 'embedding_img' is a list or array of embeddings, here applying the transformation\n",
    "    embeddings_sample = image_metadata_list_df['embedding_img'][:20].apply(lambda x: np.array(x)).tolist()  # Convert embeddings to numpy arrays\n",
    "\n",
    "    # Plot similarity heatmap using the same embeddings for both inputs\n",
    "    plot_similarity_heatmap(embeddings_sample, embeddings_sample)\n",
    "\n",
    "except KeyError as ke:\n",
    "    print(f\"Error: The specified column 'embedding_img' was not found in the dataframe. {ke}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while preparing the data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58dde2a-54b2-4a3d-b717-289b137a34e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store image_metadata_list_df aws_region_name sagemaker_role_arn multimodal_embed_model_id max_images image_metadata_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51745d43-360d-42a1-8617-a7a3a2769ab8",
   "metadata": {},
   "source": [
    "# End of NoteBook \n",
    "\n",
    "## Please ensure that you close the kernel after using this notebook to avoid any potential charges to your account.\n",
    "\n",
    "## Process: Go to \"Kernel\" at top option. Choose \"Shut Down Kernel\" if you are not executing the next notebook. Here simple_multimodal_knwl_bases_building.ipynb\n",
    "##### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/studio-ui.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617929d0-b5ab-4c37-8763-7034350c1838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
