{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed3a60a-5001-46ff-ae70-61d861a00c53",
   "metadata": {
    "tags": []
   },
   "source": [
    "# File Name: simple_converse_api.ipynb\n",
    "### Location: Chapter 4\n",
    "### Purpose: \n",
    "#####             1. Understanding Amazon Bedrock client and Amazon Bedrock runtime client.\n",
    "#####             2. Example of Amazon Titan LLM foundation model with and without parameters using the Converse API.\n",
    "#####             3. Example of Anthropic LLM foundation model with and without parameters using the Converse API.\n",
    "#####             4. Example of Amazon Titan LLM foundation model with streaming API with and with out parameters using the Converse API.\n",
    "##### Dependency: Not Applicable\n",
    "# <ins>-----------------------------------------------------------------------------------</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99464e3-0679-4a67-bb38-8d96892eefa5",
   "metadata": {},
   "source": [
    "# <ins>Amazon SageMaker Classic</ins>\n",
    "#### Those who are new to Amazon SageMaker Classic. Follow the link for the details. https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91003ca1-0026-46eb-bd09-81861135cb22",
   "metadata": {},
   "source": [
    "# <ins>Environment setup of Kernel</ins>\n",
    "##### Fill \"Image\" as \"Data Science\"\n",
    "##### Fill \"Kernel\" as \"Python 3\"\n",
    "##### Fill \"Instance type\" as \"ml-t3-medium\"\n",
    "##### Fill \"Start-up script\" as \"No Scripts\"\n",
    "##### Click \"Select\"\n",
    "\n",
    "###### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-create-open.html for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865b7f5-755a-498c-bff6-7647281b15b9",
   "metadata": {},
   "source": [
    "# <ins>Mandatory installation on the kernel through pip</ins>\n",
    "\n",
    "##### This lab will work with below software version. But, if you are trying with latest version of boto3, awscli, and botocore. This code may fail. You might need to change the corresponding api. \n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ade0112-a4be-4ce8-be92-8cbf28b2acfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "sphinx 7.2.6 requires docutils<0.21,>=0.18.1, but you have docutils 0.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --no-build-isolation --force-reinstall -q \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e3b85-4e64-42ef-9355-ed2a61c68b23",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins>\n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ee934-e99c-49ce-9e94-27ffc5baabe1",
   "metadata": {},
   "source": [
    "# <ins>Restart the kernel</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aaf41bf-d2d2-46fa-b1d4-1dd19c45a069",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5af76d-b2a9-4740-b8bf-134932804719",
   "metadata": {},
   "source": [
    "# <ins>Python package import</ins>\n",
    "\n",
    "##### boto3 offers various clients for Amazon Bedrock to execute various actions.\n",
    "##### botocore is a low-level interface to AWS tools, while boto3 is built on top of botocore and provides additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7268959-6ef0-4cbd-bd53-3b31369b4513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import botocore\n",
    "import boto3\n",
    "import warnings\n",
    "import time\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1b877-deab-4961-a249-7663e812fbb8",
   "metadata": {},
   "source": [
    "### Ignore warning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71587271-9f02-46dc-9eaf-5a5291132f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac02fbe-2513-4898-992a-8a5b29ce4ee3",
   "metadata": {},
   "source": [
    "# <ins>Amazon Bedrock Runtime Client</ins>\n",
    "\n",
    "##### Purpose: used for making inference requests for models hosted in Amazon Bedrock. \n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Amazon_Bedrock_Runtime.html for details about Amazon Bedrock runtime client "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b83ea1-9c4e-4ad2-9f83-4dfb0721a166",
   "metadata": {},
   "source": [
    "## Define important environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362bc059-93ce-4812-851e-8e359fb4026b",
   "metadata": {},
   "source": [
    "# <ins>Amazon Bedrock Client</ins>\n",
    "\n",
    "##### Purpose: used for managing, training, and deploying models on Amazon Bedrock\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Amazon_Bedrock.html for details about Amazon Bedrock client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ccf36a8-87e3-462a-9a13-4a286861c327",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3_session_name: Session(region_name='us-east-1')\n",
      "aws_region_name: us-east-1\n",
      "boto3_bedrock_client: <botocore.client.Bedrock object at 0x7f389353b6d0>\n",
      "random_suffix: 805\n",
      "boto3_bedrock_runtime_client: <botocore.client.BedrockRuntime object at 0x7f3892e352d0>\n"
     ]
    }
   ],
   "source": [
    "# Try-except block to handle potential errors\n",
    "try:\n",
    "    # Create a new Boto3 session to interact with AWS services\n",
    "    boto3_session_name = boto3.session.Session()\n",
    "\n",
    "    # Retrieve the current AWS region from the session\n",
    "    aws_region_name = boto3_session_name.region_name\n",
    "    \n",
    "    # Create a new Boto3 bedrock client to interact with AWS services\n",
    "    boto3_bedrock_client = boto3.client('bedrock')\n",
    "    \n",
    "    # Create a new Boto3 bedrock runtime client to interact with AWS services\n",
    "    boto3_bedrock_runtime_client = boto3.client('bedrock-runtime', region_name = aws_region_name,)\n",
    "    \n",
    "    # Generate a random suffix number between 200 and 900\n",
    "    random_suffix = random.randrange(200, 900)\n",
    "    \n",
    "    # Store all variables in a dictionary\n",
    "    variables_store = {\n",
    "        \"boto3_session_name\": boto3_session_name,\n",
    "        \"aws_region_name\": aws_region_name,\n",
    "        \"boto3_bedrock_client\": boto3_bedrock_client,\n",
    "        \"random_suffix\": random_suffix,\n",
    "        \"boto3_bedrock_runtime_client\": boto3_bedrock_runtime_client\n",
    "    }\n",
    "\n",
    "    # Print all variables\n",
    "    for var_name, value in variables_store.items():\n",
    "        print(f\"{var_name}: {value}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04c796-eb62-4883-bb33-17c8301f7cdb",
   "metadata": {},
   "source": [
    "# <ins>Amazon Bedrock Runtime Client</ins>\n",
    "\n",
    "##### Purpose: used for making inference requests for models hosted in Amazon Bedrock. \n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Amazon_Bedrock_Runtime.html for details about Amazon Bedrock runtime client "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e728f5cd-0a3d-4488-8cb6-07e6cd5fd54e",
   "metadata": {},
   "source": [
    "# <ins>Example of Amazon Titan LLM foundation model</ins>\n",
    "\n",
    "##### This example is based on Titan Text G1 - Express v1 foundation model. \n",
    "##### Model ID: amazon.titan-text-express-v1\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-text.html\n",
    "\n",
    "##### API: converse\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d61e1-fea0-43a0-ae8a-32601eceea39",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins> \n",
    "\n",
    "##### Make sure that amazon.titan-text-express-v1 is allowlisted on Amazon Bedrock model access. Refer Section 3.3 of Chapter 3 of the Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbd78bc8-cd55-4638-8419-0fc5fda1621f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining model_id, prompt and other variables\n",
    "## You can try out different model id, your own prompt. \n",
    "model_id = \"amazon.titan-text-express-v1\"\n",
    "prompt = \"\"\"User: Generate a story for a kid about beauty of a rainbow within 100 words\n",
    "bot:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f7e2be-92a2-450a-b934-bd805dcc821d",
   "metadata": {},
   "source": [
    "##### <ins>Example with default inferance parameters</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8943f32-97bf-4bcb-83fd-9806b2be1011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Text: Once upon a time, a young child named Lily was walking through the park when she saw a beautiful rainbow. A stunning array of colors stretched across the sky, and Lily was mesmerized by its beauty. She ran to her mother, excited to show her what she had seen. Her mother smiled and explained that rainbows were created by the reflection of light from water droplets in the air. Lily was amazed and couldn't stop talking about the rainbow she had seen. From that day on, Lily always looked for rainbows in the sky and felt grateful for the beauty of nature.\n",
      "Latency (ms): 5240\n",
      "Input Tokens: 26\n",
      "Output Tokens: 120\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Constructing the message payload for the converse API request\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Sending the request to the Amazon Bedrock service using the converse API\n",
    "    response = boto3_bedrock_runtime_client.converse(\n",
    "        modelId=model_id, messages=messages\n",
    "    )\n",
    "\n",
    "    # Extracting relevant fields from the response\n",
    "    output_text = response['output']['message']['content'][0]['text']\n",
    "    latency = response['metrics']['latencyMs']\n",
    "    input_tokens = response['usage']['inputTokens']\n",
    "    output_tokens = response['usage']['outputTokens']\n",
    "\n",
    "    # Printing extracted values\n",
    "    print(f\"Output Text: {output_text}\")\n",
    "    print(f\"Latency (ms): {latency}\")\n",
    "    print(f\"Input Tokens: {input_tokens}\")\n",
    "    print(f\"Output Tokens: {output_tokens}\")\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Retrieving the error code from the exception response\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    \n",
    "    # Handling specific error codes for more tailored responses\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\\x1b[0m\")\n",
    "    else:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "        # Raising the exception again to ensure it is visible in the outer scope if necessary\n",
    "        raise\n",
    "except KeyError as key_error:\n",
    "    # Handles missing keys in the response structure\n",
    "    print(f\"Key error: {key_error}. The response format may have changed.\")\n",
    "    raise\n",
    "except Exception as general_error:\n",
    "    # Catch-all for any other exceptions\n",
    "    print(f\"An unexpected error occurred: {general_error}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8bab9-2b05-4747-b1d9-510c2fdede75",
   "metadata": {},
   "source": [
    "##### <ins>Example with  inferance parameters configuration</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cef8536b-102a-4873-aa44-740669fd6c64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Text: Once upon a time, in a magical land, there was a rainbow that appeared only during sunrise and sunset. Its colors were vibrant and bright, and they danced across the sky. The villagers believed that the rainbow was a gift from the gods, and they would gather every morning and evening to watch it. The rainbow was a reminder of the beauty and magic of the world, and it reminded the villagers to appreciate the small things in life. Despite the passing of time, the rainbow remained a cherished symbol of the village, and its beauty continued to inspire generations to come.\n",
      "Latency (ms): 5043\n",
      "Input Tokens: 26\n",
      "Output Tokens: 115\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Constructing the message payload and inference configuration for the converse API request\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    inferenceConfig = {\n",
    "        \"temperature\": 1.0,\n",
    "        \"maxTokens\": 2000,\n",
    "        \"topP\": 0.9\n",
    "    }\n",
    "\n",
    "    # Sending the request to the Amazon Bedrock service using the converse API\n",
    "    response = boto3_bedrock_runtime_client.converse(\n",
    "        modelId=model_id, messages=messages, inferenceConfig=inferenceConfig\n",
    "    )\n",
    "\n",
    "    # Extracting and printing relevant fields from the response\n",
    "    output_text = response['output']['message']['content'][0]['text']\n",
    "    latency = response['metrics']['latencyMs']\n",
    "    input_tokens = response['usage']['inputTokens']\n",
    "    output_tokens = response['usage']['outputTokens']\n",
    "\n",
    "    print(f\"Output Text: {output_text}\")\n",
    "    print(f\"Latency (ms): {latency}\")\n",
    "    print(f\"Input Tokens: {input_tokens}\")\n",
    "    print(f\"Output Tokens: {output_tokens}\")\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Retrieve the error code from the exception response\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    \n",
    "    # Specific handling for AccessDeniedException\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\\x1b[0m\")\n",
    "    else:\n",
    "        # General error message with code and message details\n",
    "        print(f\"An error occurred: {error_code} - {error.response['Error'].get('Message', 'No message available')}\")\n",
    "        raise\n",
    "except KeyError as key_error:\n",
    "    # Handles missing keys in the response structure\n",
    "    print(f\"Key error: {key_error}. Check if the response structure has changed.\")\n",
    "    raise\n",
    "except Exception as general_error:\n",
    "    # Catch-all for any other exceptions\n",
    "    print(f\"An unexpected error occurred: {general_error}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb17e0-0bb3-45c5-9fd7-53bf0f9813cd",
   "metadata": {},
   "source": [
    "# <ins>Example of Anthropic LLM foundation model</ins>\n",
    "\n",
    "##### This example is based on Claude 3 Haiku foundation model. \n",
    "##### Model ID: anthropic.claude-3-haiku-20240307-v1:0\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html\n",
    "\n",
    "##### API: converse\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fc9c3c-db0c-4b93-a14f-4bc504747674",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins> \n",
    "\n",
    "##### Make sure that anthropic.claude-3-haiku-20240307-v1:0 is allowlisted on Amazon Bedrock model access. Refer Section 3.3 of Chapter 3 of the Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfab0db0-aa7f-4d22-8ac1-b601b3878489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining model_id, prompt and other variables\n",
    "## You can try out different model id, your own prompt. \n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "prompt = \"\"\"Human: Generate a story for a kid about beauty of a rainbow within 100 words\n",
    "\n",
    "Assitance:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7519271f-b78b-4353-a90c-57782eb6e52a",
   "metadata": {},
   "source": [
    "##### <ins>Example with default inferance parameters</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd9c19f-2d7d-41ab-b45f-31ab611171a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Text: Here is a 100-word story about the beauty of a rainbow for a kid:\n",
      "\n",
      "After a rainy afternoon, the clouds parted, and a beautiful rainbow appeared in the sky. The colors were so bright and vibrant - red, orange, yellow, green, blue, and purple. \n",
      "\n",
      "Samantha stared up at the rainbow in awe. She had never seen anything so breathtaking before. The colors blended together so perfectly, creating a mesmerizing arc across the horizon. \n",
      "\n",
      "\"Wow, look at that!\" Samantha exclaimed. \"It's the most beautiful thing I've ever seen!\"\n",
      "\n",
      "Her dad smiled and explained that rainbows are nature's masterpieces, created when sunlight hits the raindrops just right. Samantha felt lucky to witness such an incredible natural wonder.\n",
      "Latency (ms): 2065\n",
      "Input Tokens: 31\n",
      "Output Tokens: 178\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Constructing the message payload for the converse API request\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Sending the request to the Amazon Bedrock service using the converse API\n",
    "    response = boto3_bedrock_runtime_client.converse(\n",
    "        modelId=model_id, messages=messages\n",
    "    )\n",
    "\n",
    "    # Extracting and printing relevant fields from the response\n",
    "    output_text = response['output']['message']['content'][0]['text']\n",
    "    latency = response['metrics']['latencyMs']\n",
    "    input_tokens = response['usage']['inputTokens']\n",
    "    output_tokens = response['usage']['outputTokens']\n",
    "\n",
    "    print(f\"Output Text: {output_text}\")\n",
    "    print(f\"Latency (ms): {latency}\")\n",
    "    print(f\"Input Tokens: {input_tokens}\")\n",
    "    print(f\"Output Tokens: {output_tokens}\")\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Retrieve the error code from the exception response\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    \n",
    "    # Handle specific error cases\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\\x1b[0m\")\n",
    "    else:\n",
    "        # Print general error message with code and details\n",
    "        print(f\"An error occurred: {error_code} - {error.response['Error'].get('Message', 'No message available')}\")\n",
    "        raise\n",
    "\n",
    "except KeyError as key_error:\n",
    "    # Handle missing keys in the response structure\n",
    "    print(f\"Key error: {key_error}. Check if the response structure has changed.\")\n",
    "    raise\n",
    "\n",
    "except Exception as general_error:\n",
    "    # Catch-all for any other exceptions\n",
    "    print(f\"An unexpected error occurred: {general_error}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1ac6cd-bd39-4e52-9af2-fb2ad6998531",
   "metadata": {},
   "source": [
    "##### <ins>Example with  inferance parameters configuration</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec2e0b46-2ad3-4c0f-8400-6ec2b78f9f58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Text: Once upon a time, there was a little girl named Emma who loved to look up at the sky. One day, after a gentle rain, she saw something magical â€“ a beautiful rainbow stretching across the horizon.\n",
      "\n",
      "Emma's eyes grew wide with wonder as she admired the vibrant colors. The red, orange, yellow, green, blue, and purple bands of light danced across the sky, creating a breathtaking display. She reached up, trying to touch the rainbow, but it was too high up.\n",
      "\n",
      "Emma learned that rainbows are created when sunlight refracts through raindrops, and she felt grateful to witness such a natural wonder. From that day on, she always kept an eye out for the beauty of the rainbow.\n",
      "Latency (ms): 2199\n",
      "Input Tokens: 31\n",
      "Output Tokens: 158\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Constructing the message payload and inference configuration for the converse API request\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    inferenceConfig = {\n",
    "        \"temperature\": 1.0,\n",
    "        \"maxTokens\": 2000,\n",
    "        \"topP\": 0.9\n",
    "    }\n",
    "\n",
    "    # Sending the request to the Amazon Bedrock service using the converse API\n",
    "    response = boto3_bedrock_runtime_client.converse(\n",
    "        modelId=model_id, messages=messages, inferenceConfig=inferenceConfig\n",
    "    )\n",
    "\n",
    "    # Extracting and printing relevant fields from the response\n",
    "    output_text = response['output']['message']['content'][0]['text']\n",
    "    latency = response['metrics']['latencyMs']\n",
    "    input_tokens = response['usage']['inputTokens']\n",
    "    output_tokens = response['usage']['outputTokens']\n",
    "\n",
    "    print(f\"Output Text: {output_text}\")\n",
    "    print(f\"Latency (ms): {latency}\")\n",
    "    print(f\"Input Tokens: {input_tokens}\")\n",
    "    print(f\"Output Tokens: {output_tokens}\")\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Retrieve the error code from the exception response\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    \n",
    "    # Handle specific error cases\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\\x1b[0m\")\n",
    "    else:\n",
    "        # Print general error message with code and details\n",
    "        print(f\"An error occurred: {error_code} - {error.response['Error'].get('Message', 'No message available')}\")\n",
    "        raise\n",
    "\n",
    "except KeyError as key_error:\n",
    "    # Handle missing keys in the response structure\n",
    "    print(f\"Key error: {key_error}. The response structure may have changed.\")\n",
    "    raise\n",
    "\n",
    "except Exception as general_error:\n",
    "    # Catch-all for any other exceptions\n",
    "    print(f\"An unexpected error occurred: {general_error}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cae03e7-7586-43ab-8884-ee3a56804f54",
   "metadata": {},
   "source": [
    "# <ins>Example of Amazon Titan LLM foundation model with streaming API</ins>\n",
    "\n",
    "\n",
    "##### This example is based on Titan Text G1 - Express v1 foundation model. \n",
    "##### Model ID: amazon.titan-text-express-v1\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-text.html\n",
    "\n",
    "##### API: converse_stream\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ConverseStream.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a302b99-fa5c-43be-983d-22e67d2fd3c7",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins> \n",
    "\n",
    "##### Make sure that amazon.titan-text-express-v1 is allowlisted on Amazon Bedrock model access. Refer Section 3.3 of Chapter 3 of the Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c3c5b67-8354-4746-bc2a-f697ee27764a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining model_id, prompt and other variables\n",
    "## You can try out different model id, your own prompt. \n",
    "model_id = \"amazon.titan-text-express-v1\"\n",
    "prompt = \"\"\"User: Generate a story for a kid about beauty of a rainbow within 1000 words\n",
    "bot:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060c611b-aea5-4108-beca-b9dd037d6b5c",
   "metadata": {},
   "source": [
    "##### <ins>Example with default inferance parameters</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc043b1d-0367-4ba2-af4e-7f2647aa2059",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry - this model is unable to respond to this request."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "try:\n",
    "    # Constructing the message payload for the streaming API request\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Sending the request to the Amazon Bedrock service using the converse streaming API\n",
    "    response = boto3_bedrock_runtime_client.converse_stream(\n",
    "        modelId=model_id, messages=messages\n",
    "    )\n",
    "\n",
    "    # Processing and streaming the response in chunks\n",
    "    for event in response['stream']:\n",
    "        if 'contentBlockDelta' in event:\n",
    "            # Extract and print each chunk of text as it streams in\n",
    "            chunk = event['contentBlockDelta']\n",
    "            sys.stdout.write(chunk['delta']['text'])\n",
    "            sys.stdout.flush()\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Retrieve the error code from the exception response\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    \n",
    "    # Specific handling for AccessDeniedException\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\\x1b[0m\")\n",
    "    else:\n",
    "        # Print general error message with code and details\n",
    "        print(f\"An error occurred: {error_code} - {error.response['Error'].get('Message', 'No message available')}\")\n",
    "        raise\n",
    "\n",
    "except KeyError as key_error:\n",
    "    # Handle missing keys in the streaming response structure\n",
    "    print(f\"Key error: {key_error}. The response structure may have changed.\")\n",
    "    raise\n",
    "\n",
    "except Exception as general_error:\n",
    "    # Catch-all for any other exceptions\n",
    "    print(f\"An unexpected error occurred: {general_error}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814b4e1a-c9df-470f-842d-3e9a82306f40",
   "metadata": {},
   "source": [
    "##### <ins>Example with  inferance parameters configuration</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ed73fb0-1026-406a-841f-1dff2886c674",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a land of vibrant colors, there was a beautiful rainbow. A wise old owl named Olive had discovered this enchanting sight and wanted to share it with the world. She flew around the world, showing everyone the beauty of the rainbow.\n",
      "\n",
      "One day, a little girl named Lily saw the rainbow and was amazed by its colors. Olive told Lily that the rainbow was a symbol of hope and that it could bring joy to anyone who saw it. Lily began to draw pictures of the rainbow and tell her friends and family about it.\n",
      "\n",
      "As the years went by, the rainbow became even more popular. People from all over the world came to see it. Olive felt proud of her discovery and happy that she had shared it with the world.\n",
      "\n",
      "One day, a group of travelers came to Olive's village. They had heard of the rainbow and wanted to see it for themselves. Olive welcomed them with open arms and showed them the way to the rainbow.\n",
      "\n",
      "As they approached the rainbow, they were amazed by its beauty. The colors were so bright and vibrant that it seemed to light up the entire sky. The travelers could feel the warmth of the rainbow on their faces and smelled the fresh scent of the flowers that surrounded it.\n",
      "\n",
      "As they gazed at the rainbow, the travelers felt a sense of peace and happiness wash over them. They knew that the rainbow was a symbol of unity and that everyone who saw it was connected by its colors.\n",
      "\n",
      "The travelers thanked Olive for sharing the rainbow with them and returned home. They told everyone they knew about the rainbow and encouraged them to come and see it for themselves.\n",
      "\n",
      "Over time, the rainbow became a symbol of hope and unity throughout the world. People would draw pictures of it and sing songs about it. They would visit the village where Olive lived and tell her stories about how the rainbow had changed their lives.\n",
      "\n",
      "Olive felt proud of her discovery and knew that the rainbow would continue to bring joy to people for generations to come. She knew that the rainbow was a gift from the universe and that it was up to each of us to cherish it and share it with others.\n",
      "\n",
      "And so, the rainbow continued to shine bright, bringing joy and happiness to everyone who saw it. It became a symbol of hope and unity, reminding us that no matter how different we may seem, we are all connected by the colors of the rainbow."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "try:\n",
    "    # Construct the message payload and inference configuration for the streaming API request\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    inferenceConfig = {\n",
    "        \"temperature\": 1.0,\n",
    "        \"maxTokens\": 2000,\n",
    "        \"topP\": 0.9\n",
    "    }\n",
    "\n",
    "    # Sending the request to the Amazon Bedrock service using the converse streaming API\n",
    "    response = boto3_bedrock_runtime_client.converse_stream(\n",
    "        modelId=model_id, messages=messages, inferenceConfig=inferenceConfig\n",
    "    )\n",
    "\n",
    "    # Processing and streaming the response in chunks as it arrives\n",
    "    for event in response['stream']:\n",
    "        if 'contentBlockDelta' in event:\n",
    "            # Extract and print each chunk of text in real-time\n",
    "            chunk = event['contentBlockDelta']\n",
    "            sys.stdout.write(chunk['delta']['text'])\n",
    "            sys.stdout.flush()\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Retrieve the error code from the exception response\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    \n",
    "    # Handle AccessDeniedException specifically\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\\x1b[0m\")\n",
    "    else:\n",
    "        # Print a general error message with code and details\n",
    "        print(f\"An error occurred: {error_code} - {error.response['Error'].get('Message', 'No message available')}\")\n",
    "        raise\n",
    "\n",
    "except KeyError as key_error:\n",
    "    # Handle missing keys in the streaming response structure\n",
    "    print(f\"Key error: {key_error}. The response structure may have changed.\")\n",
    "    raise\n",
    "\n",
    "except Exception as general_error:\n",
    "    # Catch-all for any other exceptions\n",
    "    print(f\"An unexpected error occurred: {general_error}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51745d43-360d-42a1-8617-a7a3a2769ab8",
   "metadata": {},
   "source": [
    "# End of NoteBook \n",
    "\n",
    "## Please ensure that you close the kernel after using this notebook to avoid any potential charges to your account.\n",
    "\n",
    "## Process: Go to \"Kernel\" at top option. Choose \"Shut Down Kernel\". \n",
    "##### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/studio-ui.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d46a47-81e8-4ad7-b3c9-65a931230b86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
