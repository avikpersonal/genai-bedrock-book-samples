{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed3a60a-5001-46ff-ae70-61d861a00c53",
   "metadata": {
    "tags": []
   },
   "source": [
    "# File Name: simple_converse_api.ipynb\n",
    "### Location: Chapter 4\n",
    "### Purpose: \n",
    "#####             1. Understanding Amazon Bedrock client and Amazon Bedrock runtime client.\n",
    "#####             2. Example of Amazon Titan LLM foundation model with and without parameters using the Converse API.\n",
    "#####             3. Example of Anthropic LLM foundation model with and without parameters using the Converse API.\n",
    "#####             4. Example of Amazon Titan LLM foundation model with streaming API with and with out parameters using the Converse API.\n",
    "##### Dependency: Not Applicable\n",
    "# <ins>-----------------------------------------------------------------------------------</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99464e3-0679-4a67-bb38-8d96892eefa5",
   "metadata": {},
   "source": [
    "# <ins>Amazon SageMaker Classic</ins>\n",
    "#### Those who are new to Amazon SageMaker Classic. Follow the link for the details. https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91003ca1-0026-46eb-bd09-81861135cb22",
   "metadata": {},
   "source": [
    "# <ins>Environment setup of Kernel</ins>\n",
    "##### Fill \"Image\" as \"Data Science\"\n",
    "##### Fill \"Kernel\" as \"Python 3\"\n",
    "##### Fill \"Instance type\" as \"ml-t3-medium\"\n",
    "##### Fill \"Start-up script\" as \"No Scripts\"\n",
    "##### Click \"Select\"\n",
    "\n",
    "###### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-create-open.html for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865b7f5-755a-498c-bff6-7647281b15b9",
   "metadata": {},
   "source": [
    "# <ins>Mandatory installation on the kernel through pip</ins>\n",
    "\n",
    "##### This lab will work with below software version. But, if you are trying with latest version of boto3, awscli, and botocore. This code may fail. You might need to change the corresponding api. \n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade0112-a4be-4ce8-be92-8cbf28b2acfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall -q \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e3b85-4e64-42ef-9355-ed2a61c68b23",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins>\n",
    "\n",
    "##### You will see pip dependency errors. you can safely ignore these errors and continue executing rest of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ee934-e99c-49ce-9e94-27ffc5baabe1",
   "metadata": {},
   "source": [
    "# <ins>Restart the kernel</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf41bf-d2d2-46fa-b1d4-1dd19c45a069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5af76d-b2a9-4740-b8bf-134932804719",
   "metadata": {},
   "source": [
    "# <ins>Python package import</ins>\n",
    "\n",
    "##### boto3 offers various clients for Amazon Bedrock to execute various actions.\n",
    "##### botocore is a low-level interface to AWS tools, while boto3 is built on top of botocore and provides additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7268959-6ef0-4cbd-bd53-3b31369b4513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import botocore\n",
    "import boto3\n",
    "import warnings\n",
    "import time\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1b877-deab-4961-a249-7663e812fbb8",
   "metadata": {},
   "source": [
    "### Ignore warning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71587271-9f02-46dc-9eaf-5a5291132f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac02fbe-2513-4898-992a-8a5b29ce4ee3",
   "metadata": {},
   "source": [
    "# <ins>Amazon Bedrock Runtime Client</ins>\n",
    "\n",
    "##### Purpose: used for making inference requests for models hosted in Amazon Bedrock. \n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Amazon_Bedrock_Runtime.html for details about Amazon Bedrock runtime client "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b83ea1-9c4e-4ad2-9f83-4dfb0721a166",
   "metadata": {},
   "source": [
    "## Define important environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362bc059-93ce-4812-851e-8e359fb4026b",
   "metadata": {},
   "source": [
    "# <ins>Amazon Bedrock Client</ins>\n",
    "\n",
    "##### Purpose: used for managing, training, and deploying models on Amazon Bedrock\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Amazon_Bedrock.html for details about Amazon Bedrock client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf36a8-87e3-462a-9a13-4a286861c327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Try-except block to handle potential errors\n",
    "try:\n",
    "    # Create a new Boto3 session to interact with AWS services\n",
    "    boto3_session_name = boto3.session.Session()\n",
    "\n",
    "    # Retrieve the current AWS region from the session\n",
    "    aws_region_name = boto3_session_name.region_name\n",
    "    \n",
    "    # Create a new Boto3 bedrock client to interact with AWS services\n",
    "    boto3_bedrock_client = boto3.client('bedrock')\n",
    "    \n",
    "    # Create a new Boto3 bedrock runtime client to interact with AWS services\n",
    "    boto3_bedrock_runtime_client = boto3.client('bedrock-runtime', region_name = aws_region_name,)\n",
    "    \n",
    "    # Generate a random suffix number between 200 and 900\n",
    "    random_suffix = random.randrange(200, 900)\n",
    "    \n",
    "    # Store all variables in a dictionary\n",
    "    variables_store = {\n",
    "        \"boto3_session_name\": boto3_session_name,\n",
    "        \"aws_region_name\": aws_region_name,\n",
    "        \"boto3_bedrock_client\": boto3_bedrock_client,\n",
    "        \"random_suffix\": random_suffix,\n",
    "        \"boto3_bedrock_runtime_client\": boto3_bedrock_runtime_client\n",
    "    }\n",
    "\n",
    "    # Print all variables\n",
    "    for var_name, value in variables_store.items():\n",
    "        print(f\"{var_name}: {value}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04c796-eb62-4883-bb33-17c8301f7cdb",
   "metadata": {},
   "source": [
    "# <ins>Amazon Bedrock Runtime Client</ins>\n",
    "\n",
    "##### Purpose: used for making inference requests for models hosted in Amazon Bedrock. \n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Amazon_Bedrock_Runtime.html for details about Amazon Bedrock runtime client "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e728f5cd-0a3d-4488-8cb6-07e6cd5fd54e",
   "metadata": {},
   "source": [
    "# <ins>Example of Amazon Titan LLM foundation model</ins>\n",
    "\n",
    "##### This example is based on Titan Text G1 - Express v1 foundation model. \n",
    "##### Model ID: amazon.titan-text-express-v1\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-text.html\n",
    "\n",
    "##### API: converse\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d61e1-fea0-43a0-ae8a-32601eceea39",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins> \n",
    "\n",
    "##### Make sure that amazon.titan-text-express-v1 is allowlisted on Amazon Bedrock model access. Refer Section 3.3 of Chapter 3 of the Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd78bc8-cd55-4638-8419-0fc5fda1621f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining model_id, prompt and other variables\n",
    "## You can try out different model id, your own prompt. \n",
    "model_id = \"amazon.titan-text-express-v1\"\n",
    "prompt = \"\"\"User: Generate a story for a kid about beauty of a rainbow within 100 words\n",
    "bot:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f7e2be-92a2-450a-b934-bd805dcc821d",
   "metadata": {},
   "source": [
    "##### <ins>Example with default inferance parameters</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8943f32-97bf-4bcb-83fd-9806b2be1011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    # Constructing the message payload for the converse API request\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Sending the request to the Amazon Bedrock service using the converse API\n",
    "    response = boto3_bedrock_runtime_client.converse(\n",
    "        modelId=model_id, messages=messages\n",
    "    )\n",
    "\n",
    "    # Extracting relevant fields from the response\n",
    "    output_text = response['output']['message']['content'][0]['text']\n",
    "    latency = response['metrics']['latencyMs']\n",
    "    input_tokens = response['usage']['inputTokens']\n",
    "    output_tokens = response['usage']['outputTokens']\n",
    "\n",
    "    # Printing extracted values\n",
    "    print(f\"Output Text: {output_text}\")\n",
    "    print(f\"Latency (ms): {latency}\")\n",
    "    print(f\"Input Tokens: {input_tokens}\")\n",
    "    print(f\"Output Tokens: {output_tokens}\")\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Retrieving the error code from the exception response\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    \n",
    "    # Handling specific error codes for more tailored responses\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\\x1b[0m\")\n",
    "    else:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "        # Raising the exception again to ensure it is visible in the outer scope if necessary\n",
    "        raise\n",
    "except KeyError as key_error:\n",
    "    # Handles missing keys in the response structure\n",
    "    print(f\"Key error: {key_error}. The response format may have changed.\")\n",
    "    raise\n",
    "except Exception as general_error:\n",
    "    # Catch-all for any other exceptions\n",
    "    print(f\"An unexpected error occurred: {general_error}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8bab9-2b05-4747-b1d9-510c2fdede75",
   "metadata": {},
   "source": [
    "##### <ins>Example with  inferance parameters configuration</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef8536b-102a-4873-aa44-740669fd6c64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    # Constructing the message payload and inference configuration for the converse API request\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    inferenceConfig = {\n",
    "        \"temperature\": 1.0,\n",
    "        \"maxTokens\": 2000,\n",
    "        \"topP\": 0.9\n",
    "    }\n",
    "\n",
    "    # Sending the request to the Amazon Bedrock service using the converse API\n",
    "    response = boto3_bedrock_runtime_client.converse(\n",
    "        modelId=model_id, messages=messages, inferenceConfig=inferenceConfig\n",
    "    )\n",
    "\n",
    "    # Extracting and printing relevant fields from the response\n",
    "    output_text = response['output']['message']['content'][0]['text']\n",
    "    latency = response['metrics']['latencyMs']\n",
    "    input_tokens = response['usage']['inputTokens']\n",
    "    output_tokens = response['usage']['outputTokens']\n",
    "\n",
    "    print(f\"Output Text: {output_text}\")\n",
    "    print(f\"Latency (ms): {latency}\")\n",
    "    print(f\"Input Tokens: {input_tokens}\")\n",
    "    print(f\"Output Tokens: {output_tokens}\")\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Retrieve the error code from the exception response\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    \n",
    "    # Specific handling for AccessDeniedException\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\\x1b[0m\")\n",
    "    else:\n",
    "        # General error message with code and message details\n",
    "        print(f\"An error occurred: {error_code} - {error.response['Error'].get('Message', 'No message available')}\")\n",
    "        raise\n",
    "except KeyError as key_error:\n",
    "    # Handles missing keys in the response structure\n",
    "    print(f\"Key error: {key_error}. Check if the response structure has changed.\")\n",
    "    raise\n",
    "except Exception as general_error:\n",
    "    # Catch-all for any other exceptions\n",
    "    print(f\"An unexpected error occurred: {general_error}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb17e0-0bb3-45c5-9fd7-53bf0f9813cd",
   "metadata": {},
   "source": [
    "# <ins>Example of Anthropic LLM foundation model</ins>\n",
    "\n",
    "##### This example is based on Claude 3 Haiku foundation model. \n",
    "##### Model ID: anthropic.claude-3-haiku-20240307-v1:0\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html\n",
    "\n",
    "##### API: converse\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fc9c3c-db0c-4b93-a14f-4bc504747674",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins> \n",
    "\n",
    "##### Make sure that anthropic.claude-3-haiku-20240307-v1:0 is allowlisted on Amazon Bedrock model access. Refer Section 3.3 of Chapter 3 of the Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab0db0-aa7f-4d22-8ac1-b601b3878489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining model_id, prompt and other variables\n",
    "## You can try out different model id, your own prompt. \n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "prompt = \"\"\"Human: Generate a story for a kid about beauty of a rainbow within 100 words\n",
    "\n",
    "Assitance:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7519271f-b78b-4353-a90c-57782eb6e52a",
   "metadata": {},
   "source": [
    "##### <ins>Example with default inferance parameters</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd9c19f-2d7d-41ab-b45f-31ab611171a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    # Constructing the message payload for the converse API request\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Sending the request to the Amazon Bedrock service using the converse API\n",
    "    response = boto3_bedrock_runtime_client.converse(\n",
    "        modelId=model_id, messages=messages\n",
    "    )\n",
    "\n",
    "    # Extracting and printing relevant fields from the response\n",
    "    output_text = response['output']['message']['content'][0]['text']\n",
    "    latency = response['metrics']['latencyMs']\n",
    "    input_tokens = response['usage']['inputTokens']\n",
    "    output_tokens = response['usage']['outputTokens']\n",
    "\n",
    "    print(f\"Output Text: {output_text}\")\n",
    "    print(f\"Latency (ms): {latency}\")\n",
    "    print(f\"Input Tokens: {input_tokens}\")\n",
    "    print(f\"Output Tokens: {output_tokens}\")\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Retrieve the error code from the exception response\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    \n",
    "    # Handle specific error cases\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\\x1b[0m\")\n",
    "    else:\n",
    "        # Print general error message with code and details\n",
    "        print(f\"An error occurred: {error_code} - {error.response['Error'].get('Message', 'No message available')}\")\n",
    "        raise\n",
    "\n",
    "except KeyError as key_error:\n",
    "    # Handle missing keys in the response structure\n",
    "    print(f\"Key error: {key_error}. Check if the response structure has changed.\")\n",
    "    raise\n",
    "\n",
    "except Exception as general_error:\n",
    "    # Catch-all for any other exceptions\n",
    "    print(f\"An unexpected error occurred: {general_error}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1ac6cd-bd39-4e52-9af2-fb2ad6998531",
   "metadata": {},
   "source": [
    "##### <ins>Example with  inferance parameters configuration</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e0b46-2ad3-4c0f-8400-6ec2b78f9f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    # Constructing the message payload and inference configuration for the converse API request\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    inferenceConfig = {\n",
    "        \"temperature\": 1.0,\n",
    "        \"maxTokens\": 2000,\n",
    "        \"topP\": 0.9\n",
    "    }\n",
    "\n",
    "    # Sending the request to the Amazon Bedrock service using the converse API\n",
    "    response = boto3_bedrock_runtime_client.converse(\n",
    "        modelId=model_id, messages=messages, inferenceConfig=inferenceConfig\n",
    "    )\n",
    "\n",
    "    # Extracting and printing relevant fields from the response\n",
    "    output_text = response['output']['message']['content'][0]['text']\n",
    "    latency = response['metrics']['latencyMs']\n",
    "    input_tokens = response['usage']['inputTokens']\n",
    "    output_tokens = response['usage']['outputTokens']\n",
    "\n",
    "    print(f\"Output Text: {output_text}\")\n",
    "    print(f\"Latency (ms): {latency}\")\n",
    "    print(f\"Input Tokens: {input_tokens}\")\n",
    "    print(f\"Output Tokens: {output_tokens}\")\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Retrieve the error code from the exception response\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    \n",
    "    # Handle specific error cases\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\\x1b[0m\")\n",
    "    else:\n",
    "        # Print general error message with code and details\n",
    "        print(f\"An error occurred: {error_code} - {error.response['Error'].get('Message', 'No message available')}\")\n",
    "        raise\n",
    "\n",
    "except KeyError as key_error:\n",
    "    # Handle missing keys in the response structure\n",
    "    print(f\"Key error: {key_error}. The response structure may have changed.\")\n",
    "    raise\n",
    "\n",
    "except Exception as general_error:\n",
    "    # Catch-all for any other exceptions\n",
    "    print(f\"An unexpected error occurred: {general_error}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cae03e7-7586-43ab-8884-ee3a56804f54",
   "metadata": {},
   "source": [
    "# <ins>Example of Amazon Titan LLM foundation model with streaming API</ins>\n",
    "\n",
    "\n",
    "##### This example is based on Titan Text G1 - Express v1 foundation model. \n",
    "##### Model ID: amazon.titan-text-express-v1\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-text.html\n",
    "\n",
    "##### API: converse_stream\n",
    "##### Refer https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ConverseStream.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a302b99-fa5c-43be-983d-22e67d2fd3c7",
   "metadata": {},
   "source": [
    "# <ins>Disclaimer</ins> \n",
    "\n",
    "##### Make sure that amazon.titan-text-express-v1 is allowlisted on Amazon Bedrock model access. Refer Section 3.3 of Chapter 3 of the Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c5b67-8354-4746-bc2a-f697ee27764a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining model_id, prompt and other variables\n",
    "## You can try out different model id, your own prompt. \n",
    "model_id = \"amazon.titan-text-express-v1\"\n",
    "prompt = \"\"\"User: Generate a story for a kid about beauty of a rainbow within 1000 words\n",
    "bot:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060c611b-aea5-4108-beca-b9dd037d6b5c",
   "metadata": {},
   "source": [
    "##### <ins>Example with default inferance parameters</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc043b1d-0367-4ba2-af4e-7f2647aa2059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    # Constructing the message payload for the streaming API request\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Sending the request to the Amazon Bedrock service using the converse streaming API\n",
    "    response = boto3_bedrock_runtime_client.converse_stream(\n",
    "        modelId=model_id, messages=messages\n",
    "    )\n",
    "\n",
    "    # Processing and streaming the response in chunks\n",
    "    for event in response['stream']:\n",
    "        if 'contentBlockDelta' in event:\n",
    "            # Extract and print each chunk of text as it streams in\n",
    "            chunk = event['contentBlockDelta']\n",
    "            sys.stdout.write(chunk['delta']['text'])\n",
    "            sys.stdout.flush()\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Retrieve the error code from the exception response\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    \n",
    "    # Specific handling for AccessDeniedException\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\\x1b[0m\")\n",
    "    else:\n",
    "        # Print general error message with code and details\n",
    "        print(f\"An error occurred: {error_code} - {error.response['Error'].get('Message', 'No message available')}\")\n",
    "        raise\n",
    "\n",
    "except KeyError as key_error:\n",
    "    # Handle missing keys in the streaming response structure\n",
    "    print(f\"Key error: {key_error}. The response structure may have changed.\")\n",
    "    raise\n",
    "\n",
    "except Exception as general_error:\n",
    "    # Catch-all for any other exceptions\n",
    "    print(f\"An unexpected error occurred: {general_error}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814b4e1a-c9df-470f-842d-3e9a82306f40",
   "metadata": {},
   "source": [
    "##### <ins>Example with  inferance parameters configuration</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed73fb0-1026-406a-841f-1dff2886c674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    # Construct the message payload and inference configuration for the streaming API request\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    inferenceConfig = {\n",
    "        \"temperature\": 1.0,\n",
    "        \"maxTokens\": 2000,\n",
    "        \"topP\": 0.9\n",
    "    }\n",
    "\n",
    "    # Sending the request to the Amazon Bedrock service using the converse streaming API\n",
    "    response = boto3_bedrock_runtime_client.converse_stream(\n",
    "        modelId=model_id, messages=messages, inferenceConfig=inferenceConfig\n",
    "    )\n",
    "\n",
    "    # Processing and streaming the response in chunks as it arrives\n",
    "    for event in response['stream']:\n",
    "        if 'contentBlockDelta' in event:\n",
    "            # Extract and print each chunk of text in real-time\n",
    "            chunk = event['contentBlockDelta']\n",
    "            sys.stdout.write(chunk['delta']['text'])\n",
    "            sys.stdout.flush()\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Retrieve the error code from the exception response\n",
    "    error_code = error.response['Error'].get('Code', 'Unknown')\n",
    "    \n",
    "    # Handle AccessDeniedException specifically\n",
    "    if error_code == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41mAccess Denied: {error.response['Error'].get('Message', 'No message available')}\\x1b[0m\")\n",
    "    else:\n",
    "        # Print a general error message with code and details\n",
    "        print(f\"An error occurred: {error_code} - {error.response['Error'].get('Message', 'No message available')}\")\n",
    "        raise\n",
    "\n",
    "except KeyError as key_error:\n",
    "    # Handle missing keys in the streaming response structure\n",
    "    print(f\"Key error: {key_error}. The response structure may have changed.\")\n",
    "    raise\n",
    "\n",
    "except Exception as general_error:\n",
    "    # Catch-all for any other exceptions\n",
    "    print(f\"An unexpected error occurred: {general_error}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51745d43-360d-42a1-8617-a7a3a2769ab8",
   "metadata": {},
   "source": [
    "# End of NoteBook \n",
    "\n",
    "## Please ensure that you close the kernel after using this notebook to avoid any potential charges to your account.\n",
    "\n",
    "## Process: Go to \"Kernel\" at top option. Choose \"Shut Down Kernel\". \n",
    "##### Refer https://docs.aws.amazon.com/sagemaker/latest/dg/studio-ui.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d46a47-81e8-4ad7-b3c9-65a931230b86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
